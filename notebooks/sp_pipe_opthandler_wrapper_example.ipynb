{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ade29e-a77d-44e2-96f1-fc0f6a975780",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4baf8671-4258-44c4-be4f-161728a53c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from functools import partial\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import decision_learning.modeling.pipeline\n",
    "import decision_learning.data.shortest_path_grid\n",
    "\n",
    "from decision_learning.utils import handle_solver\n",
    "from decision_learning.modeling.models import LinearRegression\n",
    "from decision_learning.modeling.pipeline import lossfn_experiment_pipeline, lossfn_hyperparam_grid\n",
    "from decision_learning.data.shortest_path_grid import genData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf793762-b0e9-4994-a0e4-b8a68dbedb1f",
   "metadata": {},
   "source": [
    "# Pipeline Setup Overview\n",
    "To run pipeline function in `decision_learning.modeling.pipeline`, we need these components:\n",
    "- data (features, true costs), and appropriate train-test splits\n",
    "- prediction model: predicting true costs\n",
    "- optimization model: linear optimization model parameterized by cost/coefficient vector for objective function, and returns the corresponding solution, objective value.\n",
    "- existing loss functions (hyperparameter configs):loss functions to train the prediction model against true costs as labels and already implemented within `decision_learning.modeling.loss` specified with the loss name as string, and hyperparameters to search over as a dictionary.\n",
    "- custom loss functions: user provided loss function as a callable, and data dictionary with appropriate features, and labels required by loss function\n",
    "- misc params: other parameters to set for pipeline experiment function\n",
    "    - val_split_params={'test_size':200, 'random_state':42},\n",
    "    - training configuration: ex: {'num_epochs':100, 'dataloader_params': {'batch_size':200, 'shuffle':True}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bb933f-d853-419a-affe-33eb5ca30c39",
   "metadata": {},
   "source": [
    "## Optimization Model (linear)\n",
    "\n",
    "Decision-aware/focused problems require an optimization model to actually solve the decision problem. Since each decision problem is unique in terms of the modeling and solving, the user is expected to provide the optimization model function/object, which is treated like a black-box by the `pipeline`,`train`, and loss/regret functions in the code base. It could be Gurobi, Pyomo, or any user custom solver. However, to play nicely with the rest of the package, it must do the following:\n",
    "\n",
    "- Input Argument when called:\n",
    "    - costs: vector of objective function coefficients. Expected to be numpy np.ndarray or torch.tensor\n",
    "- Returns 2 objects:\n",
    "    - sols: solution to optimization model given the input costs. Expected to be numpy np.ndarray or torch.tensor\n",
    "    - obj: objective value to optimization model given the input costs. Expected to be numpy np.ndarray or torch.tensor\n",
    " \n",
    "The return objects of optimal solution and objective are generally returned as any solver, and any linear program needs its objective function to be parameterized by a vector of cost/coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a075f24e-0ba4-46a4-a761-78e4ce2fb0ed",
   "metadata": {},
   "source": [
    "### Example Solver/Optimization Model\n",
    "- Below, `shortest_path_solver` is a custom user optimization model specified in the form of a callable function, and its first input argument is the vector of costs. The rest of the input arguments size, sens, need to be pre-set before being passed to `pipeline`, `train`, or any loss function. This can be accomplished using the `partial` python function (see example below). The exact implementation is not important but mainly that it:\n",
    "    - accepts a costs vector input\n",
    "    - returns solution (sol) and objective value (obj) for the input cost vector\n",
    "- Note that `shortest_path_solver` also has two returns: sol, obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0cc057-2b90-488e-b434-88d16ca8cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path_solver(costs, size, sens = 1e-4):\n",
    "    if isinstance(size, np.ndarray):\n",
    "        size = int(size[0])   \n",
    "    elif isinstance(size, torch.Tensor):     \n",
    "        size = int(size[0].item())\n",
    "    \n",
    "    if type(size) != int:\n",
    "        size = int(size)\n",
    "        \n",
    "    # Forward Pass\n",
    "    starting_ind = 0\n",
    "    starting_ind_c = 0\n",
    "    samples = costs.shape[0]\n",
    "    V_arr = torch.zeros(samples, size ** 2)\n",
    "    for i in range(0, 2 * (size - 1)):\n",
    "        num_nodes = min(i + 1, 9 - i)\n",
    "        num_nodes_next = min(i + 2, 9 - i - 1)\n",
    "        num_arcs = 2 * (max(num_nodes, num_nodes_next) - 1)\n",
    "        V_1 = V_arr[:, starting_ind:starting_ind + num_nodes]\n",
    "        layer_costs = costs[:, starting_ind_c:starting_ind_c + num_arcs]\n",
    "        l_costs = layer_costs[:, 0::2]\n",
    "        r_costs = layer_costs[:, 1::2]\n",
    "        next_V_val_l = torch.ones(samples, num_nodes_next) * float('inf')\n",
    "        next_V_val_r = torch.ones(samples, num_nodes_next) * float('inf')\n",
    "        if num_nodes_next > num_nodes:\n",
    "            next_V_val_l[:, :num_nodes_next - 1] = V_1 + l_costs\n",
    "            next_V_val_r[:, 1:num_nodes_next] = V_1 + r_costs\n",
    "        else:\n",
    "            next_V_val_l = V_1[:, :num_nodes_next] + l_costs\n",
    "            next_V_val_r = V_1[:, 1:num_nodes_next + 1] + r_costs\n",
    "        next_V_val = torch.minimum(next_V_val_l, next_V_val_r)\n",
    "        V_arr[:, starting_ind + num_nodes:starting_ind + num_nodes + num_nodes_next] = next_V_val\n",
    "\n",
    "        starting_ind += num_nodes\n",
    "        starting_ind_c += num_arcs\n",
    "\n",
    "    # Backward Pass\n",
    "    starting_ind = size ** 2\n",
    "    starting_ind_c = costs.shape[1]\n",
    "    prev_act = torch.ones(samples, 1)\n",
    "    sol = torch.zeros(costs.shape)\n",
    "    for i in range(2 * (size - 1), 0, -1):\n",
    "        num_nodes = min(i + 1, 9 - i)\n",
    "        num_nodes_next = min(i, 9 - i + 1)\n",
    "        V_1 = V_arr[:, starting_ind - num_nodes:starting_ind]\n",
    "        V_2 = V_arr[:, starting_ind - num_nodes - num_nodes_next:starting_ind - num_nodes]\n",
    "\n",
    "        num_arcs = 2 * (max(num_nodes, num_nodes_next) - 1)\n",
    "        layer_costs = costs[:, starting_ind_c - num_arcs: starting_ind_c]\n",
    "\n",
    "        if num_nodes < num_nodes_next:\n",
    "            l_cs_res = ((V_2[:, :num_nodes_next - 1] - V_1 + layer_costs[:, ::2]) < sens) * prev_act\n",
    "            r_cs_res = ((V_2[:, 1:num_nodes_next] - V_1 + layer_costs[:, 1::2]) < sens) * prev_act\n",
    "            prev_act = torch.zeros(V_2.shape)\n",
    "            prev_act[:, :num_nodes_next - 1] += l_cs_res\n",
    "            prev_act[:, 1:num_nodes_next] += r_cs_res\n",
    "        else:\n",
    "            l_cs_res = ((V_2 - V_1[:, :num_nodes - 1] + layer_costs[:, ::2]) < sens) * prev_act[:, :num_nodes - 1]\n",
    "            r_cs_res = ((V_2 - V_1[:, 1:num_nodes] + layer_costs[:, 1::2]) < sens) * prev_act[:, 1:num_nodes]\n",
    "            prev_act = torch.zeros(V_2.shape)\n",
    "            prev_act += l_cs_res\n",
    "            prev_act += r_cs_res\n",
    "        cs = torch.zeros(layer_costs.shape)\n",
    "        cs[:, ::2] = l_cs_res\n",
    "        cs[:, 1::2] = r_cs_res\n",
    "        sol[:, starting_ind_c - num_arcs: starting_ind_c] = cs\n",
    "\n",
    "        starting_ind = starting_ind - num_nodes\n",
    "        starting_ind_c = starting_ind_c - num_arcs\n",
    "    # Dimension (samples, num edges)\n",
    "    obj = torch.sum(sol * costs, axis=1)\n",
    "    # Dimension (samples, 1)\n",
    "    sol = sol.to(torch.float32)\n",
    "    obj = obj.reshape(-1,1).to(torch.float32)\n",
    "    return sol, obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e59d0-469f-4d64-b6f4-1b51a624d1bd",
   "metadata": {},
   "source": [
    "### Presetting non-cost input arguments of `shortest_path_solver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae92ee9-ce23-424c-8fa6-f47d06892522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------optimization model------------\n",
    "optmodel = partial(handle_solver, optmodel=shortest_path_solver, detach_tensor=False, solver_batch_solve=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571bb3c5-d810-4bd4-981a-6972f4d2d475",
   "metadata": {},
   "source": [
    "## Data Generation Setup\n",
    "Any decision-aware/focused problem will of course need data inputs. The example below uses a pre-implemented synthetic data generator provided in the package found within \n",
    "`decision_learning.data.shortest_path_grid` to generate shortest path problem and can be generated by calling the `genData` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb52ecaf-e5ef-4125-9431-79d1893b9a08",
   "metadata": {},
   "source": [
    "### Specific parameters to set up data generation\n",
    "This data setup, and the synthetic data generation is in line with the paper https://arxiv.org/pdf/2402.03256 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a53af7-bf10-486f-ad90-159897baba3b",
   "metadata": {},
   "source": [
    "### Create Experiments Grid\n",
    "This shortest path experiment has two important settings:\n",
    "- number of samples: less samples means higher error/more noise, more samples means lower error/less noise\n",
    "- epsilon: noise level on edge costs, can be uniformly distributed multiplicative noise, or normally distributed additive noise\n",
    "- This example below creates 100 trials for 8 different settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c950eb8-0d00-48c8-b211-f0afec1d0007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control the randomization seeding for pytorch\n",
    "torch.manual_seed(105)\n",
    "indices_arr = torch.randperm(100000)\n",
    "indices_arr_test = torch.randperm(100000)\n",
    "\n",
    "n_arr = [200, 400, 800, 1600] # array of number of samples for an experiment\n",
    "ep_arr = ['unif', 'normal'] # noise type\n",
    "trials = 100 # number of trials per setting\n",
    "\n",
    "# create an array where each item is [number of samples, noise type, trial number] representing an experiment run\n",
    "exp_arr = []\n",
    "for n in n_arr:\n",
    "    for ep in ep_arr:\n",
    "        for t in range(trials):\n",
    "            exp_arr.append([n, ep, t]) # add current [number of samples, noise type, trial number] experiment run setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21a3dd33-3f0a-4966-8649-63c76ff46e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current experiment setting: number of data points 200, epsilon type unif, trial number 0\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "sim = 0 # simulation trial number, only show one experiment run for demonstration purposes\n",
    "exp = exp_arr[sim] # current experiment\n",
    "num_data = exp[0]  # number of training data\n",
    "ep_type = exp[1] # noise type of current experiment\n",
    "trial = exp[2] # trial number of current experiment\n",
    "\n",
    "# shortest path problem data generation parameters - https://arxiv.org/pdf/2402.03256\n",
    "grid = (5, 5)  # grid size\n",
    "num_feat = 5  # size of feature\n",
    "deg = 6  # polynomial degree in edge cost function\n",
    "e = .3  # noise width/amount of noise\n",
    "\n",
    "# path planting for shortest path example - see page 9, subsection \"Harder Example with Planted Arcs\" in section 4.2 of paper https://arxiv.org/pdf/2402.03256\n",
    "planted_good_pwl_params = {'slope0':0, # slope of first segment of piecewise linear cost function for \"good\" edge cost planted\n",
    "                    'int0':2, # intercept of first segment of piecewise linear cost function for \"good\" edge cost planted\n",
    "                    'slope1':0, # slope of second segment of piecewise linear cost function for \"good\" edge cost planted\n",
    "                    'int1':2} # intercept of second segment of piecewise linear cost function for \"good\" edge cost planted\n",
    "planted_bad_pwl_params = {'slope0':4, # slope of first segment of piecewise linear cost function for \"bad\" edge cost planted\n",
    "                    'int0':0, # intercept of first segment of piecewise linear cost function for \"bad\" edge cost planted\n",
    "                    'slope1':0, # slope of second segment of piecewise linear cost function for \"bad\" edge cost planted\n",
    "                    'int1':2.2} # intercept of second segment of piecewise linear cost function for \"bad\" edge cost planted\n",
    "plant_edge = True # to plant edges in shortest path experiment or not\n",
    "\n",
    "print(f'current experiment setting: number of data points {num_data}, epsilon type {ep_type}, trial number {trial}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59553dc-1f26-4848-97f1-5de3006afd70",
   "metadata": {},
   "source": [
    "### Calling `genData` from `decision_learning.data.shortest_path_grid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6208b7ef-6b2e-4a04-b68f-323ca537e207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------DATA------------\n",
    "# training data\n",
    "generated_data = genData(num_data=num_data+200, # number of data points to generate for training set\n",
    "        num_features=num_feat, # number of features \n",
    "        grid=grid, # grid shape\n",
    "        deg=deg, # polynomial degree\n",
    "        noise_type=ep_type, # epsilon noise type\n",
    "        noise_width=e, # amount of noise\n",
    "        seed=indices_arr[trial], # seed the randomness\n",
    "        plant_edges=plant_edge, # to plant edges or not\n",
    "        planted_good_pwl_params=planted_good_pwl_params, # cost function for good edges\n",
    "        planted_bad_pwl_params=planted_bad_pwl_params) # cost function for bad edges\n",
    "\n",
    "# testing data\n",
    "generated_data_test = genData(num_data=10000, # number of data points to generate for test set\n",
    "        num_features=num_feat, # number of features \n",
    "        grid=grid,  # grid shape\n",
    "        deg=deg,  # polynomial degree\n",
    "        noise_type=ep_type,  # epsilon noise type\n",
    "        noise_width=e, # amount of noise\n",
    "        seed=indices_arr_test[trial],      # seed the randomness\n",
    "        plant_edges=plant_edge, # to plant edges or not\n",
    "        planted_good_pwl_params=planted_good_pwl_params, # cost function for good edges\n",
    "        planted_bad_pwl_params=planted_bad_pwl_params) # cost function for bad edges\n",
    "\n",
    "\n",
    "train_solver_kwargs = {'size': np.zeros(len(generated_data['cost'])) + 5}\n",
    "test_solver_kwargs = {'size': np.zeros(len(generated_data_test['cost'])) + 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b2dd8-1cbb-48a2-b648-d50e2255ada6",
   "metadata": {},
   "source": [
    "## Prediction Model\n",
    "- Any decision-aware/focused problem will of course need prediction model to predict the cost/coefficient vector given contextual input/features. This example uses a simple `LinearRegression` object implemented within `decision_learning.modeling.models`. \n",
    "- The package expects the prediction model to be a PyTorch model since PyTorch offers convenient autograd functionality/allows user to specify custom losses/backwards passes that are found within many decision-aware/focused works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b32a03bb-f163-4f38-b81b-8574a61253a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------prediction model------------\n",
    "pred_model = LinearRegression(input_dim=generated_data['feat'].shape[1],\n",
    "                 output_dim=generated_data['cost'].shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972c6cc7-c367-4294-960c-2149d0e5ba08",
   "metadata": {},
   "source": [
    "# Pipeline Function Overview\n",
    "Pipeline function `lossfn_experiment_pipeline` from `decision_learning.modeling.pipeline` takes in the following arguments:\n",
    "- X_train: training set features\n",
    "- true_cost_train: training set true costs\n",
    "- X_test: test set features\n",
    "- true_cost_test: test set true costs\n",
    "- predmodel: pytorch prediction model\n",
    "- optmodel: optimization model\n",
    "- val_split_params: how to split training data into train/val splits. Defaults to {'test_size':0.2, 'random_state':42}.\n",
    "- loss_names: list of loss functions to run experiment pipeline on that are implemented already in the codebase in decision_learning.modeling.loss\n",
    "- loss_configs: dictionary mapping from loss_name (key) to a dictionary of different hyperparameters that are then grid searched over.\n",
    "- custom_loss_inputs:list of custom loss function configurations to run through the train function as part of experient pipeline\n",
    "- minimize: minimization problem?\n",
    "- training_configs: parameters to be passed into train function for pytorch training loop. \n",
    "- save_models: flag to save models or not.\n",
    "\n",
    "Note when running pipeline function, we turn off training loop logging/console output for each experiment setting, however, this can still be turned on by setting `training_loop_verbose=True` flag on when calling pipeline function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb45d5-9dcc-4351-bad3-1dcaf469cefa",
   "metadata": {},
   "source": [
    "# Example: Off-the-Shelf Preimplemented Loss Functions\n",
    "Here since we only use off-the-shelf preimplemented loss functions, without any hyperparameter searching, we only need to specify the individual loss names `['SPO+', 'MSE']` to `loss_name` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9c49167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import decision_learning.modeling.pipeline\n",
    "importlib.reload(decision_learning.modeling.pipeline)\n",
    "from decision_learning.modeling.pipeline import lossfn_experiment_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad0dd576-08a8-4205-bd5f-96603ce39242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:26: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_l[:, :num_nodes_next - 1] = V_1 + l_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:27: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_r[:, 1:num_nodes_next] = V_1 + r_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:29: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_l = V_1[:, :num_nodes_next] + l_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:30: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_r = V_1[:, 1:num_nodes_next + 1] + r_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:52: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  l_cs_res = ((V_2[:, :num_nodes_next - 1] - V_1 + layer_costs[:, ::2]) < sens) * prev_act\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:53: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  r_cs_res = ((V_2[:, 1:num_nodes_next] - V_1 + layer_costs[:, 1::2]) < sens) * prev_act\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:58: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  l_cs_res = ((V_2 - V_1[:, :num_nodes - 1] + layer_costs[:, ::2]) < sens) * prev_act[:, :num_nodes - 1]\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:59: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  r_cs_res = ((V_2 - V_1[:, 1:num_nodes] + layer_costs[:, 1::2]) < sens) * prev_act[:, 1:num_nodes]\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:71: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  obj = torch.sum(sol * costs, axis=1)\n",
      "2025-02-04 11:33:51,408 - decision_learning.modeling.pipeline - INFO - Loss number 1/3, on loss function SPO+\n",
      "2025-02-04 11:33:51,408 - decision_learning.modeling.pipeline - INFO - Trial 1/1 for running loss function SPO+, current hyperparameters: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 11:33:51,410 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:33:52,305 - decision_learning.utils - INFO - Function 'train' took 0.8952939510345459 seconds to run.\n",
      "2025-02-04 11:33:52,306 - decision_learning.modeling.pipeline - INFO - Loss number 2/3, on loss function MSE\n",
      "2025-02-04 11:33:52,306 - decision_learning.modeling.pipeline - INFO - Trial 1/1 for running loss function MSE, current hyperparameters: {}\n",
      "2025-02-04 11:33:52,307 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:33:52,857 - decision_learning.utils - INFO - Function 'train' took 0.5508370399475098 seconds to run.\n",
      "2025-02-04 11:33:52,858 - decision_learning.modeling.pipeline - INFO - Loss number 3/3, on loss function Cosine\n",
      "2025-02-04 11:33:52,858 - decision_learning.modeling.pipeline - INFO - Trial 1/1 for running loss function Cosine, current hyperparameters: {}\n",
      "2025-02-04 11:33:52,859 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:33:53,485 - decision_learning.utils - INFO - Function 'train' took 0.6265180110931396 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "preimplement_loss_results, preimplement_loss_models = lossfn_experiment_pipeline(X_train=generated_data['feat'],\n",
    "                true_cost_train=generated_data['cost'],\n",
    "                X_test=generated_data_test['feat'],\n",
    "                true_cost_test=generated_data_test['cost_true'], \n",
    "                predmodel=pred_model,\n",
    "                optmodel=optmodel,\n",
    "                train_solver_kwargs=train_solver_kwargs,\n",
    "                test_solver_kwargs=test_solver_kwargs,\n",
    "                val_split_params={'test_size':200, 'random_state':42},\n",
    "                loss_names=['SPO+', 'MSE', 'Cosine'],                            \n",
    "                training_configs={'num_epochs':100,\n",
    "                                 'dataloader_params': {'batch_size':32, 'shuffle':True}},\n",
    "                save_models=True                                                                              \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6930de09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_regret</th>\n",
       "      <th>loss_name</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>3.818337</td>\n",
       "      <td>0.068014</td>\n",
       "      <td>0.032343</td>\n",
       "      <td>SPO+</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>99</td>\n",
       "      <td>0.866071</td>\n",
       "      <td>0.106883</td>\n",
       "      <td>0.063263</td>\n",
       "      <td>MSE</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>99</td>\n",
       "      <td>0.021646</td>\n",
       "      <td>0.061828</td>\n",
       "      <td>0.029458</td>\n",
       "      <td>Cosine</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  train_loss  val_metric  test_regret loss_name hyperparameters\n",
       "99      99    3.818337    0.068014     0.032343      SPO+              {}\n",
       "199     99    0.866071    0.106883     0.063263       MSE              {}\n",
       "299     99    0.021646    0.061828     0.029458    Cosine              {}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preimplement_loss_results[preimplement_loss_results.epoch == 99]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92815220-580f-4857-acf5-bbea917bdd06",
   "metadata": {},
   "source": [
    "### Saved Down Models\n",
    "models saved as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e77adca-8885-49ea-938f-02d2cd196bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SPO+_{}': LinearRegression(\n",
       "   (linear): Linear(in_features=6, out_features=40, bias=True)\n",
       " ),\n",
       " 'MSE_{}': LinearRegression(\n",
       "   (linear): Linear(in_features=6, out_features=40, bias=True)\n",
       " ),\n",
       " 'Cosine_{}': LinearRegression(\n",
       "   (linear): Linear(in_features=6, out_features=40, bias=True)\n",
       " )}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preimplement_loss_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1b038b-0739-4224-9335-130c5722ed13",
   "metadata": {},
   "source": [
    "# Providing Hyperparameter Search Example\n",
    "Here since we we are still using off-the-shelf preimplemented loss functions, but now since PG loss accepts two arguments ('h': width size, and 'finite_diff_type': finite different scheme}, we can search over the hyperparameters by inputting them into the `loss_configs` argument in the exaxmple below as: `{'PG': {'h':[num_data**-.125, num_data**-.25, num_data**-.5, num_data**-1], 'finite_diff_type': ['B', 'C', 'F']}}`. The pipeline function will use a helper function `lossfn_hyperparam_grid` to take the cartesian product of the `h` and `finite_diff_type` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c32ab754-a6e5-4b09-a55b-2d74054656f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:26: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_l[:, :num_nodes_next - 1] = V_1 + l_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:27: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_r[:, 1:num_nodes_next] = V_1 + r_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:29: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_l = V_1[:, :num_nodes_next] + l_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:30: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_r = V_1[:, 1:num_nodes_next + 1] + r_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:52: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  l_cs_res = ((V_2[:, :num_nodes_next - 1] - V_1 + layer_costs[:, ::2]) < sens) * prev_act\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:53: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  r_cs_res = ((V_2[:, 1:num_nodes_next] - V_1 + layer_costs[:, 1::2]) < sens) * prev_act\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:58: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  l_cs_res = ((V_2 - V_1[:, :num_nodes - 1] + layer_costs[:, ::2]) < sens) * prev_act[:, :num_nodes - 1]\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:59: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  r_cs_res = ((V_2 - V_1[:, 1:num_nodes] + layer_costs[:, 1::2]) < sens) * prev_act[:, 1:num_nodes]\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:71: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  obj = torch.sum(sol * costs, axis=1)\n",
      "2025-02-04 11:34:09,449 - decision_learning.modeling.pipeline - INFO - Loss number 1/1, on loss function PG\n",
      "2025-02-04 11:34:09,451 - decision_learning.modeling.pipeline - INFO - Trial 1/12 for running loss function PG, current hyperparameters: {'h': 0.5156692688606229, 'finite_diff_type': 'B'}\n",
      "2025-02-04 11:34:09,453 - decision_learning.modeling.train - INFO - Training on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 11:34:10,665 - decision_learning.utils - INFO - Function 'train' took 1.2116978168487549 seconds to run.\n",
      "2025-02-04 11:34:10,665 - decision_learning.modeling.pipeline - INFO - Trial 2/12 for running loss function PG, current hyperparameters: {'h': 0.5156692688606229, 'finite_diff_type': 'C'}\n",
      "2025-02-04 11:34:10,666 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:34:11,813 - decision_learning.utils - INFO - Function 'train' took 1.1472151279449463 seconds to run.\n",
      "2025-02-04 11:34:11,813 - decision_learning.modeling.pipeline - INFO - Trial 3/12 for running loss function PG, current hyperparameters: {'h': 0.5156692688606229, 'finite_diff_type': 'F'}\n",
      "2025-02-04 11:34:11,814 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:34:12,957 - decision_learning.utils - INFO - Function 'train' took 1.143430233001709 seconds to run.\n",
      "2025-02-04 11:34:12,958 - decision_learning.modeling.pipeline - INFO - Trial 4/12 for running loss function PG, current hyperparameters: {'h': 0.26591479484724945, 'finite_diff_type': 'B'}\n",
      "2025-02-04 11:34:12,959 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:34:14,130 - decision_learning.utils - INFO - Function 'train' took 1.1717660427093506 seconds to run.\n",
      "2025-02-04 11:34:14,131 - decision_learning.modeling.pipeline - INFO - Trial 5/12 for running loss function PG, current hyperparameters: {'h': 0.26591479484724945, 'finite_diff_type': 'C'}\n",
      "2025-02-04 11:34:14,132 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:34:15,364 - decision_learning.utils - INFO - Function 'train' took 1.2320950031280518 seconds to run.\n",
      "2025-02-04 11:34:15,364 - decision_learning.modeling.pipeline - INFO - Trial 6/12 for running loss function PG, current hyperparameters: {'h': 0.26591479484724945, 'finite_diff_type': 'F'}\n",
      "2025-02-04 11:34:15,365 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:34:16,529 - decision_learning.utils - INFO - Function 'train' took 1.1646790504455566 seconds to run.\n",
      "2025-02-04 11:34:16,530 - decision_learning.modeling.pipeline - INFO - Trial 7/12 for running loss function PG, current hyperparameters: {'h': 0.07071067811865475, 'finite_diff_type': 'B'}\n",
      "2025-02-04 11:34:16,531 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:34:17,686 - decision_learning.utils - INFO - Function 'train' took 1.1552488803863525 seconds to run.\n",
      "2025-02-04 11:34:17,686 - decision_learning.modeling.pipeline - INFO - Trial 8/12 for running loss function PG, current hyperparameters: {'h': 0.07071067811865475, 'finite_diff_type': 'C'}\n",
      "2025-02-04 11:34:17,687 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:34:18,893 - decision_learning.utils - INFO - Function 'train' took 1.2053821086883545 seconds to run.\n",
      "2025-02-04 11:34:18,893 - decision_learning.modeling.pipeline - INFO - Trial 9/12 for running loss function PG, current hyperparameters: {'h': 0.07071067811865475, 'finite_diff_type': 'F'}\n",
      "2025-02-04 11:34:18,894 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:34:20,091 - decision_learning.utils - INFO - Function 'train' took 1.1970281600952148 seconds to run.\n",
      "2025-02-04 11:34:20,091 - decision_learning.modeling.pipeline - INFO - Trial 10/12 for running loss function PG, current hyperparameters: {'h': 0.005, 'finite_diff_type': 'B'}\n",
      "2025-02-04 11:34:20,092 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:34:21,252 - decision_learning.utils - INFO - Function 'train' took 1.1603829860687256 seconds to run.\n",
      "2025-02-04 11:34:21,253 - decision_learning.modeling.pipeline - INFO - Trial 11/12 for running loss function PG, current hyperparameters: {'h': 0.005, 'finite_diff_type': 'C'}\n",
      "2025-02-04 11:34:21,253 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:34:22,423 - decision_learning.utils - INFO - Function 'train' took 1.1693780422210693 seconds to run.\n",
      "2025-02-04 11:34:22,423 - decision_learning.modeling.pipeline - INFO - Trial 12/12 for running loss function PG, current hyperparameters: {'h': 0.005, 'finite_diff_type': 'F'}\n",
      "2025-02-04 11:34:22,424 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:34:23,580 - decision_learning.utils - INFO - Function 'train' took 1.1557528972625732 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "PG_results, PG_models = lossfn_experiment_pipeline(X_train=generated_data['feat'],\n",
    "                true_cost_train=generated_data['cost'],\n",
    "                X_test=generated_data_test['feat'],\n",
    "                true_cost_test=generated_data_test['cost_true'], \n",
    "                predmodel=pred_model,\n",
    "                optmodel=optmodel,\n",
    "                train_solver_kwargs=train_solver_kwargs,\n",
    "                test_solver_kwargs=test_solver_kwargs,\n",
    "                val_split_params={'test_size':200, 'random_state':42},\n",
    "                loss_names=['PG'],\n",
    "                loss_configs={'PG': {'h':[num_data**-.125, num_data**-.25, num_data**-.5, num_data**-1], 'finite_diff_type': ['B', 'C', 'F']}},\n",
    "                training_configs={'num_epochs':100,\n",
    "                                 'dataloader_params': {'batch_size':32, 'shuffle':True}},\n",
    "                save_models=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e0085ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_regret</th>\n",
       "      <th>loss_name</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>14.842943</td>\n",
       "      <td>0.115022</td>\n",
       "      <td>0.069211</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.5156692688606229, 'finite_diff_type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>99</td>\n",
       "      <td>14.161848</td>\n",
       "      <td>0.044928</td>\n",
       "      <td>0.007162</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.5156692688606229, 'finite_diff_type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>99</td>\n",
       "      <td>13.717442</td>\n",
       "      <td>0.088385</td>\n",
       "      <td>0.046018</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.5156692688606229, 'finite_diff_type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>99</td>\n",
       "      <td>14.951248</td>\n",
       "      <td>0.116574</td>\n",
       "      <td>0.070373</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.26591479484724945, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>99</td>\n",
       "      <td>14.634395</td>\n",
       "      <td>0.088882</td>\n",
       "      <td>0.042735</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.26591479484724945, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>99</td>\n",
       "      <td>14.012732</td>\n",
       "      <td>0.053794</td>\n",
       "      <td>0.012122</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.26591479484724945, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>99</td>\n",
       "      <td>15.130478</td>\n",
       "      <td>0.147690</td>\n",
       "      <td>0.098016</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.07071067811865475, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>99</td>\n",
       "      <td>14.809798</td>\n",
       "      <td>0.125931</td>\n",
       "      <td>0.075467</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.07071067811865475, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>99</td>\n",
       "      <td>15.013995</td>\n",
       "      <td>0.144541</td>\n",
       "      <td>0.091452</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.07071067811865475, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>99</td>\n",
       "      <td>16.233646</td>\n",
       "      <td>0.246358</td>\n",
       "      <td>0.191938</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.005, 'finite_diff_type': 'B'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>99</td>\n",
       "      <td>15.409228</td>\n",
       "      <td>0.171873</td>\n",
       "      <td>0.116332</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.005, 'finite_diff_type': 'C'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>99</td>\n",
       "      <td>16.077142</td>\n",
       "      <td>0.224672</td>\n",
       "      <td>0.165651</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.005, 'finite_diff_type': 'F'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      epoch  train_loss  val_metric  test_regret loss_name  \\\n",
       "99       99   14.842943    0.115022     0.069211        PG   \n",
       "199      99   14.161848    0.044928     0.007162        PG   \n",
       "299      99   13.717442    0.088385     0.046018        PG   \n",
       "399      99   14.951248    0.116574     0.070373        PG   \n",
       "499      99   14.634395    0.088882     0.042735        PG   \n",
       "599      99   14.012732    0.053794     0.012122        PG   \n",
       "699      99   15.130478    0.147690     0.098016        PG   \n",
       "799      99   14.809798    0.125931     0.075467        PG   \n",
       "899      99   15.013995    0.144541     0.091452        PG   \n",
       "999      99   16.233646    0.246358     0.191938        PG   \n",
       "1099     99   15.409228    0.171873     0.116332        PG   \n",
       "1199     99   16.077142    0.224672     0.165651        PG   \n",
       "\n",
       "                                        hyperparameters  \n",
       "99    {'h': 0.5156692688606229, 'finite_diff_type': ...  \n",
       "199   {'h': 0.5156692688606229, 'finite_diff_type': ...  \n",
       "299   {'h': 0.5156692688606229, 'finite_diff_type': ...  \n",
       "399   {'h': 0.26591479484724945, 'finite_diff_type':...  \n",
       "499   {'h': 0.26591479484724945, 'finite_diff_type':...  \n",
       "599   {'h': 0.26591479484724945, 'finite_diff_type':...  \n",
       "699   {'h': 0.07071067811865475, 'finite_diff_type':...  \n",
       "799   {'h': 0.07071067811865475, 'finite_diff_type':...  \n",
       "899   {'h': 0.07071067811865475, 'finite_diff_type':...  \n",
       "999               {'h': 0.005, 'finite_diff_type': 'B'}  \n",
       "1099              {'h': 0.005, 'finite_diff_type': 'C'}  \n",
       "1199              {'h': 0.005, 'finite_diff_type': 'F'}  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PG_results[PG_results.epoch == 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f1246a9-41ef-4867-866b-707b61c9ad07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PG_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28068f",
   "metadata": {},
   "source": [
    "### Additional Parameter Tuning Example - CosineSurrogateDotProdVecMag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e535be13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:26: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_l[:, :num_nodes_next - 1] = V_1 + l_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:27: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_r[:, 1:num_nodes_next] = V_1 + r_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:29: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_l = V_1[:, :num_nodes_next] + l_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:30: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_r = V_1[:, 1:num_nodes_next + 1] + r_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:52: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  l_cs_res = ((V_2[:, :num_nodes_next - 1] - V_1 + layer_costs[:, ::2]) < sens) * prev_act\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:53: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  r_cs_res = ((V_2[:, 1:num_nodes_next] - V_1 + layer_costs[:, 1::2]) < sens) * prev_act\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:58: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  l_cs_res = ((V_2 - V_1[:, :num_nodes - 1] + layer_costs[:, ::2]) < sens) * prev_act[:, :num_nodes - 1]\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:59: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  r_cs_res = ((V_2 - V_1[:, 1:num_nodes] + layer_costs[:, 1::2]) < sens) * prev_act[:, 1:num_nodes]\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:71: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  obj = torch.sum(sol * costs, axis=1)\n",
      "2025-02-04 11:34:48,352 - decision_learning.modeling.pipeline - INFO - Loss number 1/1, on loss function CosineSurrogateDotProdVecMag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 11:34:48,353 - decision_learning.modeling.pipeline - INFO - Trial 1/7 for running loss function CosineSurrogateDotProdVecMag, current hyperparameters: {'alpha': 0.01}\n",
      "2025-02-04 11:34:48,354 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:34:48,918 - decision_learning.utils - INFO - Function 'train' took 0.5641160011291504 seconds to run.\n",
      "2025-02-04 11:34:48,919 - decision_learning.modeling.pipeline - INFO - Trial 2/7 for running loss function CosineSurrogateDotProdVecMag, current hyperparameters: {'alpha': 0.1}\n",
      "2025-02-04 11:34:48,919 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:34:49,421 - decision_learning.utils - INFO - Function 'train' took 0.5013349056243896 seconds to run.\n",
      "2025-02-04 11:34:49,421 - decision_learning.modeling.pipeline - INFO - Trial 3/7 for running loss function CosineSurrogateDotProdVecMag, current hyperparameters: {'alpha': 1}\n",
      "2025-02-04 11:34:49,422 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:34:49,984 - decision_learning.utils - INFO - Function 'train' took 0.5624051094055176 seconds to run.\n",
      "2025-02-04 11:34:49,985 - decision_learning.modeling.pipeline - INFO - Trial 4/7 for running loss function CosineSurrogateDotProdVecMag, current hyperparameters: {'alpha': 2.5}\n",
      "2025-02-04 11:34:49,986 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:34:50,522 - decision_learning.utils - INFO - Function 'train' took 0.535944938659668 seconds to run.\n",
      "2025-02-04 11:34:50,522 - decision_learning.modeling.pipeline - INFO - Trial 5/7 for running loss function CosineSurrogateDotProdVecMag, current hyperparameters: {'alpha': 5}\n",
      "2025-02-04 11:34:50,523 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:34:51,080 - decision_learning.utils - INFO - Function 'train' took 0.5579898357391357 seconds to run.\n",
      "2025-02-04 11:34:51,081 - decision_learning.modeling.pipeline - INFO - Trial 6/7 for running loss function CosineSurrogateDotProdVecMag, current hyperparameters: {'alpha': 7.5}\n",
      "2025-02-04 11:34:51,082 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:34:51,672 - decision_learning.utils - INFO - Function 'train' took 0.5902018547058105 seconds to run.\n",
      "2025-02-04 11:34:51,672 - decision_learning.modeling.pipeline - INFO - Trial 7/7 for running loss function CosineSurrogateDotProdVecMag, current hyperparameters: {'alpha': 10}\n",
      "2025-02-04 11:34:51,673 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:34:52,219 - decision_learning.utils - INFO - Function 'train' took 0.5464479923248291 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "cos_surr_results, cos_surr_models = lossfn_experiment_pipeline(X_train=generated_data['feat'],\n",
    "                true_cost_train=generated_data['cost'],\n",
    "                X_test=generated_data_test['feat'],\n",
    "                true_cost_test=generated_data_test['cost_true'], \n",
    "                predmodel=pred_model,\n",
    "                optmodel=optmodel,\n",
    "                train_solver_kwargs=train_solver_kwargs,\n",
    "                test_solver_kwargs=test_solver_kwargs,\n",
    "                val_split_params={'test_size':200, 'random_state':42},\n",
    "                loss_names=['CosineSurrogateDotProdVecMag'],\n",
    "                loss_configs={'CosineSurrogateDotProdVecMag': {'alpha':[0.01, 0.1, 1, 2.5, 5, 7.5, 10]}},\n",
    "                training_configs={'num_epochs':100,\n",
    "                                 'dataloader_params': {'batch_size':32, 'shuffle':True}},\n",
    "                save_models=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d44eee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_regret</th>\n",
       "      <th>loss_name</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>-1590.639073</td>\n",
       "      <td>0.318395</td>\n",
       "      <td>0.230097</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>99</td>\n",
       "      <td>-690.318542</td>\n",
       "      <td>0.241694</td>\n",
       "      <td>0.173082</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>99</td>\n",
       "      <td>-76.590943</td>\n",
       "      <td>0.091228</td>\n",
       "      <td>0.049137</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>99</td>\n",
       "      <td>-32.698903</td>\n",
       "      <td>0.088055</td>\n",
       "      <td>0.050155</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 2.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>99</td>\n",
       "      <td>-15.149711</td>\n",
       "      <td>0.091727</td>\n",
       "      <td>0.054726</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>99</td>\n",
       "      <td>-10.697785</td>\n",
       "      <td>0.081171</td>\n",
       "      <td>0.049264</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 7.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>99</td>\n",
       "      <td>-7.673268</td>\n",
       "      <td>0.091153</td>\n",
       "      <td>0.055243</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 10}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch   train_loss  val_metric  test_regret  \\\n",
       "99      99 -1590.639073    0.318395     0.230097   \n",
       "199     99  -690.318542    0.241694     0.173082   \n",
       "299     99   -76.590943    0.091228     0.049137   \n",
       "399     99   -32.698903    0.088055     0.050155   \n",
       "499     99   -15.149711    0.091727     0.054726   \n",
       "599     99   -10.697785    0.081171     0.049264   \n",
       "699     99    -7.673268    0.091153     0.055243   \n",
       "\n",
       "                        loss_name  hyperparameters  \n",
       "99   CosineSurrogateDotProdVecMag  {'alpha': 0.01}  \n",
       "199  CosineSurrogateDotProdVecMag   {'alpha': 0.1}  \n",
       "299  CosineSurrogateDotProdVecMag     {'alpha': 1}  \n",
       "399  CosineSurrogateDotProdVecMag   {'alpha': 2.5}  \n",
       "499  CosineSurrogateDotProdVecMag     {'alpha': 5}  \n",
       "599  CosineSurrogateDotProdVecMag   {'alpha': 7.5}  \n",
       "699  CosineSurrogateDotProdVecMag    {'alpha': 10}  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_surr_results[cos_surr_results.epoch == 99]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd831d",
   "metadata": {},
   "source": [
    "Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a071afc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_regret</th>\n",
       "      <th>loss_name</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>-262.287231</td>\n",
       "      <td>0.391855</td>\n",
       "      <td>0.250060</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>99</td>\n",
       "      <td>-227.810394</td>\n",
       "      <td>0.478831</td>\n",
       "      <td>0.267152</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>99</td>\n",
       "      <td>-73.140350</td>\n",
       "      <td>0.299098</td>\n",
       "      <td>0.129629</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>99</td>\n",
       "      <td>-30.240700</td>\n",
       "      <td>0.258759</td>\n",
       "      <td>0.075556</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 2.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>99</td>\n",
       "      <td>-15.257957</td>\n",
       "      <td>0.204101</td>\n",
       "      <td>0.068355</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>99</td>\n",
       "      <td>-10.171493</td>\n",
       "      <td>0.242172</td>\n",
       "      <td>0.106509</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 7.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>99</td>\n",
       "      <td>-7.603749</td>\n",
       "      <td>0.249935</td>\n",
       "      <td>0.125792</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 10}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  train_loss  val_metric  test_regret                     loss_name  \\\n",
       "99      99 -262.287231    0.391855     0.250060  CosineSurrogateDotProdVecMag   \n",
       "199     99 -227.810394    0.478831     0.267152  CosineSurrogateDotProdVecMag   \n",
       "299     99  -73.140350    0.299098     0.129629  CosineSurrogateDotProdVecMag   \n",
       "399     99  -30.240700    0.258759     0.075556  CosineSurrogateDotProdVecMag   \n",
       "499     99  -15.257957    0.204101     0.068355  CosineSurrogateDotProdVecMag   \n",
       "599     99  -10.171493    0.242172     0.106509  CosineSurrogateDotProdVecMag   \n",
       "699     99   -7.603749    0.249935     0.125792  CosineSurrogateDotProdVecMag   \n",
       "\n",
       "     hyperparameters  \n",
       "99   {'alpha': 0.01}  \n",
       "199   {'alpha': 0.1}  \n",
       "299     {'alpha': 1}  \n",
       "399   {'alpha': 2.5}  \n",
       "499     {'alpha': 5}  \n",
       "599   {'alpha': 7.5}  \n",
       "699    {'alpha': 10}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cos_surr_results[cos_surr_results.epoch == 99]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc086f-aef8-461b-b555-9e95143f6dc3",
   "metadata": {},
   "source": [
    "# Specific Model Initialization Example\n",
    "Here we will use the pre-trained `SPO+` model as initialization point for PG loss example from above and observe the improvement in test_regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2440fac9-24fd-47e2-bf20-97599580c78c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:26: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_l[:, :num_nodes_next - 1] = V_1 + l_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:27: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_r[:, 1:num_nodes_next] = V_1 + r_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:29: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_l = V_1[:, :num_nodes_next] + l_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:30: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_r = V_1[:, 1:num_nodes_next + 1] + r_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:52: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  l_cs_res = ((V_2[:, :num_nodes_next - 1] - V_1 + layer_costs[:, ::2]) < sens) * prev_act\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:53: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  r_cs_res = ((V_2[:, 1:num_nodes_next] - V_1 + layer_costs[:, 1::2]) < sens) * prev_act\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:58: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  l_cs_res = ((V_2 - V_1[:, :num_nodes - 1] + layer_costs[:, ::2]) < sens) * prev_act[:, :num_nodes - 1]\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:59: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  r_cs_res = ((V_2 - V_1[:, 1:num_nodes] + layer_costs[:, 1::2]) < sens) * prev_act[:, 1:num_nodes]\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:71: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  obj = torch.sum(sol * costs, axis=1)\n",
      "2025-02-04 11:35:33,321 - decision_learning.modeling.pipeline - INFO - Loss number 1/1, on loss function PG\n",
      "2025-02-04 11:35:33,322 - decision_learning.modeling.pipeline - INFO - Trial 1/12 for running loss function PG, current hyperparameters: {'h': 0.5156692688606229, 'finite_diff_type': 'B'}\n",
      "2025-02-04 11:35:33,324 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:35:34,599 - decision_learning.utils - INFO - Function 'train' took 1.2755701541900635 seconds to run.\n",
      "2025-02-04 11:35:34,600 - decision_learning.modeling.pipeline - INFO - Trial 2/12 for running loss function PG, current hyperparameters: {'h': 0.5156692688606229, 'finite_diff_type': 'C'}\n",
      "2025-02-04 11:35:34,600 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:35:35,780 - decision_learning.utils - INFO - Function 'train' took 1.1796238422393799 seconds to run.\n",
      "2025-02-04 11:35:35,780 - decision_learning.modeling.pipeline - INFO - Trial 3/12 for running loss function PG, current hyperparameters: {'h': 0.5156692688606229, 'finite_diff_type': 'F'}\n",
      "2025-02-04 11:35:35,781 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:35:36,971 - decision_learning.utils - INFO - Function 'train' took 1.1901829242706299 seconds to run.\n",
      "2025-02-04 11:35:36,972 - decision_learning.modeling.pipeline - INFO - Trial 4/12 for running loss function PG, current hyperparameters: {'h': 0.26591479484724945, 'finite_diff_type': 'B'}\n",
      "2025-02-04 11:35:36,972 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:35:38,127 - decision_learning.utils - INFO - Function 'train' took 1.1551589965820312 seconds to run.\n",
      "2025-02-04 11:35:38,128 - decision_learning.modeling.pipeline - INFO - Trial 5/12 for running loss function PG, current hyperparameters: {'h': 0.26591479484724945, 'finite_diff_type': 'C'}\n",
      "2025-02-04 11:35:38,128 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:35:39,327 - decision_learning.utils - INFO - Function 'train' took 1.198807954788208 seconds to run.\n",
      "2025-02-04 11:35:39,328 - decision_learning.modeling.pipeline - INFO - Trial 6/12 for running loss function PG, current hyperparameters: {'h': 0.26591479484724945, 'finite_diff_type': 'F'}\n",
      "2025-02-04 11:35:39,328 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:35:40,480 - decision_learning.utils - INFO - Function 'train' took 1.1519029140472412 seconds to run.\n",
      "2025-02-04 11:35:40,481 - decision_learning.modeling.pipeline - INFO - Trial 7/12 for running loss function PG, current hyperparameters: {'h': 0.07071067811865475, 'finite_diff_type': 'B'}\n",
      "2025-02-04 11:35:40,481 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:35:41,639 - decision_learning.utils - INFO - Function 'train' took 1.1577460765838623 seconds to run.\n",
      "2025-02-04 11:35:41,640 - decision_learning.modeling.pipeline - INFO - Trial 8/12 for running loss function PG, current hyperparameters: {'h': 0.07071067811865475, 'finite_diff_type': 'C'}\n",
      "2025-02-04 11:35:41,640 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:35:42,799 - decision_learning.utils - INFO - Function 'train' took 1.1593129634857178 seconds to run.\n",
      "2025-02-04 11:35:42,800 - decision_learning.modeling.pipeline - INFO - Trial 9/12 for running loss function PG, current hyperparameters: {'h': 0.07071067811865475, 'finite_diff_type': 'F'}\n",
      "2025-02-04 11:35:42,801 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:35:43,945 - decision_learning.utils - INFO - Function 'train' took 1.1449227333068848 seconds to run.\n",
      "2025-02-04 11:35:43,946 - decision_learning.modeling.pipeline - INFO - Trial 10/12 for running loss function PG, current hyperparameters: {'h': 0.005, 'finite_diff_type': 'B'}\n",
      "2025-02-04 11:35:43,947 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:35:45,100 - decision_learning.utils - INFO - Function 'train' took 1.1534228324890137 seconds to run.\n",
      "2025-02-04 11:35:45,100 - decision_learning.modeling.pipeline - INFO - Trial 11/12 for running loss function PG, current hyperparameters: {'h': 0.005, 'finite_diff_type': 'C'}\n",
      "2025-02-04 11:35:45,101 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:35:46,246 - decision_learning.utils - INFO - Function 'train' took 1.1448147296905518 seconds to run.\n",
      "2025-02-04 11:35:46,246 - decision_learning.modeling.pipeline - INFO - Trial 12/12 for running loss function PG, current hyperparameters: {'h': 0.005, 'finite_diff_type': 'F'}\n",
      "2025-02-04 11:35:46,247 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-02-04 11:35:47,468 - decision_learning.utils - INFO - Function 'train' took 1.221479892730713 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "PG_init_results, PG_init_models = lossfn_experiment_pipeline(X_train=generated_data['feat'],\n",
    "                true_cost_train=generated_data['cost'],\n",
    "                X_test=generated_data_test['feat'],\n",
    "                true_cost_test=generated_data_test['cost_true'], \n",
    "                predmodel=preimplement_loss_models['SPO+_{}'],\n",
    "                optmodel=optmodel,                \n",
    "                train_solver_kwargs=train_solver_kwargs,\n",
    "                test_solver_kwargs=test_solver_kwargs,\n",
    "                val_split_params={'test_size':200, 'random_state':42},\n",
    "                loss_names=['PG'],\n",
    "                loss_configs={'PG': {'h':[num_data**-.125, num_data**-.25, num_data**-.5, num_data**-1], 'finite_diff_type': ['B', 'C', 'F']}},\n",
    "                training_configs={'num_epochs':100,\n",
    "                                 'dataloader_params': {'batch_size':32, 'shuffle':True}},\n",
    "                save_models=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03ddad35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_regret</th>\n",
       "      <th>loss_name</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>14.293769</td>\n",
       "      <td>0.043477</td>\n",
       "      <td>0.009471</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.5156692688606229, 'finite_diff_type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>99</td>\n",
       "      <td>13.995255</td>\n",
       "      <td>0.044939</td>\n",
       "      <td>0.008897</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.5156692688606229, 'finite_diff_type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>99</td>\n",
       "      <td>13.945991</td>\n",
       "      <td>0.107598</td>\n",
       "      <td>0.057595</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.5156692688606229, 'finite_diff_type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>99</td>\n",
       "      <td>13.933623</td>\n",
       "      <td>0.046911</td>\n",
       "      <td>0.007061</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.26591479484724945, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>99</td>\n",
       "      <td>14.274433</td>\n",
       "      <td>0.046529</td>\n",
       "      <td>0.008756</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.26591479484724945, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>99</td>\n",
       "      <td>14.178854</td>\n",
       "      <td>0.051247</td>\n",
       "      <td>0.008525</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.26591479484724945, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>99</td>\n",
       "      <td>14.361191</td>\n",
       "      <td>0.050816</td>\n",
       "      <td>0.010623</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.07071067811865475, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>99</td>\n",
       "      <td>13.986787</td>\n",
       "      <td>0.045326</td>\n",
       "      <td>0.007424</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.07071067811865475, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>99</td>\n",
       "      <td>14.519786</td>\n",
       "      <td>0.049164</td>\n",
       "      <td>0.015538</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.07071067811865475, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>99</td>\n",
       "      <td>14.314454</td>\n",
       "      <td>0.068014</td>\n",
       "      <td>0.032343</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.005, 'finite_diff_type': 'B'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>99</td>\n",
       "      <td>14.246981</td>\n",
       "      <td>0.078326</td>\n",
       "      <td>0.037596</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.005, 'finite_diff_type': 'C'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>99</td>\n",
       "      <td>14.406777</td>\n",
       "      <td>0.076593</td>\n",
       "      <td>0.031360</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.005, 'finite_diff_type': 'F'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      epoch  train_loss  val_metric  test_regret loss_name  \\\n",
       "99       99   14.293769    0.043477     0.009471        PG   \n",
       "199      99   13.995255    0.044939     0.008897        PG   \n",
       "299      99   13.945991    0.107598     0.057595        PG   \n",
       "399      99   13.933623    0.046911     0.007061        PG   \n",
       "499      99   14.274433    0.046529     0.008756        PG   \n",
       "599      99   14.178854    0.051247     0.008525        PG   \n",
       "699      99   14.361191    0.050816     0.010623        PG   \n",
       "799      99   13.986787    0.045326     0.007424        PG   \n",
       "899      99   14.519786    0.049164     0.015538        PG   \n",
       "999      99   14.314454    0.068014     0.032343        PG   \n",
       "1099     99   14.246981    0.078326     0.037596        PG   \n",
       "1199     99   14.406777    0.076593     0.031360        PG   \n",
       "\n",
       "                                        hyperparameters  \n",
       "99    {'h': 0.5156692688606229, 'finite_diff_type': ...  \n",
       "199   {'h': 0.5156692688606229, 'finite_diff_type': ...  \n",
       "299   {'h': 0.5156692688606229, 'finite_diff_type': ...  \n",
       "399   {'h': 0.26591479484724945, 'finite_diff_type':...  \n",
       "499   {'h': 0.26591479484724945, 'finite_diff_type':...  \n",
       "599   {'h': 0.26591479484724945, 'finite_diff_type':...  \n",
       "699   {'h': 0.07071067811865475, 'finite_diff_type':...  \n",
       "799   {'h': 0.07071067811865475, 'finite_diff_type':...  \n",
       "899   {'h': 0.07071067811865475, 'finite_diff_type':...  \n",
       "999               {'h': 0.005, 'finite_diff_type': 'B'}  \n",
       "1099              {'h': 0.005, 'finite_diff_type': 'C'}  \n",
       "1199              {'h': 0.005, 'finite_diff_type': 'F'}  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PG_init_results[PG_init_results.epoch == 99]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d75eb3-683d-4d11-b355-40333a48cd5d",
   "metadata": {},
   "source": [
    "# Custom Loss Function Example\n",
    "Simple example using `nn.CosineEmbeddingLoss`, which takes different input arguments then our existing decision-aware loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c95d9777-39f3-405d-970c-a4bb116cbd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------custom loss function inputs------------\n",
    "# every dictionary key is necessary for custom loss inputs \n",
    "custom_loss_inputs = [{'loss_name':'cosine', # name of loss function - used just for final metric logging purposes\n",
    "                      'loss':nn.CosineEmbeddingLoss, # callable function \n",
    "                      'data': {'X': generated_data['feat'], # data input for loss function, X is the feature name, it must be X for pipeline function\n",
    "                               'input2':generated_data['cost'], # remaining is whatever labels, arguments, the loss function needs. input2, target are arguments used by nn.CosineEmbeddingLoss\n",
    "                               'target':torch.ones(generated_data['cost'].shape[0])}\n",
    "                      }\n",
    "                     ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e248af6-acf6-4673-a3e1-abdffe4c4d1d",
   "metadata": {},
   "source": [
    "Here we also set `training_loop_verbose=True` to showcase console/logging output when we allow for training loop outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec03507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import decision_learning.modeling.pipeline\n",
    "importlib.reload(decision_learning.modeling.pipeline)\n",
    "from decision_learning.modeling.pipeline import lossfn_experiment_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "976df299-bfe3-46fe-ad60-a5112d269c4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:26: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_l[:, :num_nodes_next - 1] = V_1 + l_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:27: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_r[:, 1:num_nodes_next] = V_1 + r_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:29: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_l = V_1[:, :num_nodes_next] + l_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:30: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_r = V_1[:, 1:num_nodes_next + 1] + r_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:52: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  l_cs_res = ((V_2[:, :num_nodes_next - 1] - V_1 + layer_costs[:, ::2]) < sens) * prev_act\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:53: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  r_cs_res = ((V_2[:, 1:num_nodes_next] - V_1 + layer_costs[:, 1::2]) < sens) * prev_act\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:58: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  l_cs_res = ((V_2 - V_1[:, :num_nodes - 1] + layer_costs[:, ::2]) < sens) * prev_act[:, :num_nodes - 1]\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:59: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  r_cs_res = ((V_2 - V_1[:, 1:num_nodes] + layer_costs[:, 1::2]) < sens) * prev_act[:, 1:num_nodes]\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_9939/1802634363.py:71: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  obj = torch.sum(sol * costs, axis=1)\n",
      "2025-02-04 11:35:57,724 - decision_learning.modeling.pipeline - INFO - Trial 1/1 for custom loss functions, current loss function: cosine\n",
      "2025-02-04 11:35:57,730 - decision_learning.modeling.train - INFO - Training on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loader: Epoch 1/100: 100%|██████████| 7/7 [00:00<00:00, 1119.25it/s]\n",
      "2025-02-04 11:35:57,748 - decision_learning.modeling.train - INFO - epoch: 1, train_loss: 0.7574476259095329, val_metric: 0.5229720056566004, test_regret: 0.47177226737189426\n",
      "Training Loader: Epoch 2/100: 100%|██████████| 7/7 [00:00<00:00, 1422.21it/s]\n",
      "2025-02-04 11:35:57,764 - decision_learning.modeling.train - INFO - epoch: 2, train_loss: 0.5515320684228625, val_metric: 0.5253658028558674, test_regret: 0.4616907710520226\n",
      "Training Loader: Epoch 3/100: 100%|██████████| 7/7 [00:00<00:00, 1426.22it/s]\n",
      "2025-02-04 11:35:57,778 - decision_learning.modeling.train - INFO - epoch: 3, train_loss: 0.393504981483732, val_metric: 0.5197530947844433, test_regret: 0.4482718987633784\n",
      "Training Loader: Epoch 4/100: 100%|██████████| 7/7 [00:00<00:00, 1796.17it/s]\n",
      "2025-02-04 11:35:57,791 - decision_learning.modeling.train - INFO - epoch: 4, train_loss: 0.2704984779868807, val_metric: 0.4983010696238059, test_regret: 0.4272184040947559\n",
      "Training Loader: Epoch 5/100: 100%|██████████| 7/7 [00:00<00:00, 1717.47it/s]\n",
      "2025-02-04 11:35:57,818 - decision_learning.modeling.train - INFO - epoch: 5, train_loss: 0.18596553589616502, val_metric: 0.4635765202745119, test_regret: 0.3829588758178365\n",
      "Training Loader: Epoch 6/100: 100%|██████████| 7/7 [00:00<00:00, 758.88it/s]\n",
      "2025-02-04 11:35:57,836 - decision_learning.modeling.train - INFO - epoch: 6, train_loss: 0.12365614622831345, val_metric: 0.3681342476272481, test_regret: 0.29878012481499483\n",
      "Training Loader: Epoch 7/100: 100%|██████████| 7/7 [00:00<00:00, 2053.87it/s]\n",
      "2025-02-04 11:35:57,852 - decision_learning.modeling.train - INFO - epoch: 7, train_loss: 0.08530885194029127, val_metric: 0.2974865059518289, test_regret: 0.22569084418305363\n",
      "Training Loader: Epoch 8/100: 100%|██████████| 7/7 [00:00<00:00, 1233.93it/s]\n",
      "2025-02-04 11:35:57,866 - decision_learning.modeling.train - INFO - epoch: 8, train_loss: 0.06996233229126249, val_metric: 0.2652549274600837, test_regret: 0.18016017944249613\n",
      "Training Loader: Epoch 9/100: 100%|██████████| 7/7 [00:00<00:00, 1442.69it/s]\n",
      "2025-02-04 11:35:57,877 - decision_learning.modeling.train - INFO - epoch: 9, train_loss: 0.051646595022508075, val_metric: 0.21668155821272764, test_regret: 0.15627318435064197\n",
      "Training Loader: Epoch 10/100: 100%|██████████| 7/7 [00:00<00:00, 1579.78it/s]\n",
      "2025-02-04 11:35:57,891 - decision_learning.modeling.train - INFO - epoch: 10, train_loss: 0.04400376124041421, val_metric: 0.20624380977892487, test_regret: 0.1385174066501944\n",
      "Training Loader: Epoch 11/100: 100%|██████████| 7/7 [00:00<00:00, 1918.84it/s]\n",
      "2025-02-04 11:35:57,901 - decision_learning.modeling.train - INFO - epoch: 11, train_loss: 0.04105703053729875, val_metric: 0.18493255585814444, test_regret: 0.12440239766659081\n",
      "Training Loader: Epoch 12/100: 100%|██████████| 7/7 [00:00<00:00, 1860.24it/s]\n",
      "2025-02-04 11:35:57,912 - decision_learning.modeling.train - INFO - epoch: 12, train_loss: 0.040835173800587654, val_metric: 0.17599584400299048, test_regret: 0.11149843047337255\n",
      "Training Loader: Epoch 13/100: 100%|██████████| 7/7 [00:00<00:00, 1925.63it/s]\n",
      "2025-02-04 11:35:57,922 - decision_learning.modeling.train - INFO - epoch: 13, train_loss: 0.03455697798303196, val_metric: 0.15950829308873712, test_regret: 0.09818884339631895\n",
      "Training Loader: Epoch 14/100: 100%|██████████| 7/7 [00:00<00:00, 1493.55it/s]\n",
      "2025-02-04 11:35:57,933 - decision_learning.modeling.train - INFO - epoch: 14, train_loss: 0.035191387736371586, val_metric: 0.1270752390306538, test_regret: 0.08572731931570597\n",
      "Training Loader: Epoch 15/100: 100%|██████████| 7/7 [00:00<00:00, 1359.71it/s]\n",
      "2025-02-04 11:35:57,945 - decision_learning.modeling.train - INFO - epoch: 15, train_loss: 0.031025622306125506, val_metric: 0.11469096334026395, test_regret: 0.07686054362534749\n",
      "Training Loader: Epoch 16/100: 100%|██████████| 7/7 [00:00<00:00, 1530.45it/s]\n",
      "2025-02-04 11:35:57,956 - decision_learning.modeling.train - INFO - epoch: 16, train_loss: 0.0302837182368551, val_metric: 0.10579840748657528, test_regret: 0.06865195963394442\n",
      "Training Loader: Epoch 17/100: 100%|██████████| 7/7 [00:00<00:00, 1814.93it/s]\n",
      "2025-02-04 11:35:57,976 - decision_learning.modeling.train - INFO - epoch: 17, train_loss: 0.02924112948988165, val_metric: 0.09431616087431424, test_regret: 0.05952262073583461\n",
      "Training Loader: Epoch 18/100: 100%|██████████| 7/7 [00:00<00:00, 921.77it/s]\n",
      "2025-02-04 11:35:58,050 - decision_learning.modeling.train - INFO - epoch: 18, train_loss: 0.029279058799147606, val_metric: 0.08803755425142773, test_regret: 0.04866877722407157\n",
      "Training Loader: Epoch 19/100: 100%|██████████| 7/7 [00:00<00:00, 1701.05it/s]\n",
      "2025-02-04 11:35:58,068 - decision_learning.modeling.train - INFO - epoch: 19, train_loss: 0.028169629829270498, val_metric: 0.08356072065066977, test_regret: 0.04328284526097216\n",
      "Training Loader: Epoch 20/100: 100%|██████████| 7/7 [00:00<00:00, 1416.11it/s]\n",
      "2025-02-04 11:35:58,080 - decision_learning.modeling.train - INFO - epoch: 20, train_loss: 0.02749190027160304, val_metric: 0.07826086081506833, test_regret: 0.039027184363657284\n",
      "Training Loader: Epoch 21/100: 100%|██████████| 7/7 [00:00<00:00, 1792.98it/s]\n",
      "2025-02-04 11:35:58,091 - decision_learning.modeling.train - INFO - epoch: 21, train_loss: 0.026605008968285153, val_metric: 0.0743402916588953, test_regret: 0.037914224047518714\n",
      "Training Loader: Epoch 22/100: 100%|██████████| 7/7 [00:00<00:00, 1935.79it/s]\n",
      "2025-02-04 11:35:58,103 - decision_learning.modeling.train - INFO - epoch: 22, train_loss: 0.02580715369965349, val_metric: 0.08071575773298124, test_regret: 0.03866274587880862\n",
      "Training Loader: Epoch 23/100: 100%|██████████| 7/7 [00:00<00:00, 1608.86it/s]\n",
      "2025-02-04 11:35:58,118 - decision_learning.modeling.train - INFO - epoch: 23, train_loss: 0.025523808651736805, val_metric: 0.07228422914347195, test_regret: 0.03641649757010077\n",
      "Training Loader: Epoch 24/100: 100%|██████████| 7/7 [00:00<00:00, 1508.59it/s]\n",
      "2025-02-04 11:35:58,131 - decision_learning.modeling.train - INFO - epoch: 24, train_loss: 0.025701460827674185, val_metric: 0.07070073585468631, test_regret: 0.035985773789717075\n",
      "Training Loader: Epoch 25/100: 100%|██████████| 7/7 [00:00<00:00, 1940.14it/s]\n",
      "2025-02-04 11:35:58,140 - decision_learning.modeling.train - INFO - epoch: 25, train_loss: 0.02477321321410792, val_metric: 0.0694249914057153, test_regret: 0.03397172433095936\n",
      "Training Loader: Epoch 26/100: 100%|██████████| 7/7 [00:00<00:00, 1935.28it/s]\n",
      "2025-02-04 11:35:58,149 - decision_learning.modeling.train - INFO - epoch: 26, train_loss: 0.024701614997216632, val_metric: 0.07297058923946376, test_regret: 0.032967181772867026\n",
      "Training Loader: Epoch 27/100: 100%|██████████| 7/7 [00:00<00:00, 1744.41it/s]\n",
      "2025-02-04 11:35:58,160 - decision_learning.modeling.train - INFO - epoch: 27, train_loss: 0.024244826020939008, val_metric: 0.06317424830126724, test_regret: 0.030784132157925984\n",
      "Training Loader: Epoch 28/100: 100%|██████████| 7/7 [00:00<00:00, 1854.83it/s]\n",
      "2025-02-04 11:35:58,169 - decision_learning.modeling.train - INFO - epoch: 28, train_loss: 0.024129622748919895, val_metric: 0.06286053369143636, test_regret: 0.02946023431304663\n",
      "Training Loader: Epoch 29/100: 100%|██████████| 7/7 [00:00<00:00, 2230.84it/s]\n",
      "2025-02-04 11:35:58,180 - decision_learning.modeling.train - INFO - epoch: 29, train_loss: 0.0239919690149171, val_metric: 0.06087851998805675, test_regret: 0.028649882901415322\n",
      "Training Loader: Epoch 30/100: 100%|██████████| 7/7 [00:00<00:00, 2026.65it/s]\n",
      "2025-02-04 11:35:58,188 - decision_learning.modeling.train - INFO - epoch: 30, train_loss: 0.023011983771409308, val_metric: 0.07236361574974075, test_regret: 0.028377619194088793\n",
      "Training Loader: Epoch 31/100: 100%|██████████| 7/7 [00:00<00:00, 2015.52it/s]\n",
      "2025-02-04 11:35:58,197 - decision_learning.modeling.train - INFO - epoch: 31, train_loss: 0.02306528575718403, val_metric: 0.060305904914538425, test_regret: 0.027135112181163244\n",
      "Training Loader: Epoch 32/100: 100%|██████████| 7/7 [00:00<00:00, 2171.29it/s]\n",
      "2025-02-04 11:35:58,207 - decision_learning.modeling.train - INFO - epoch: 32, train_loss: 0.024475352306451117, val_metric: 0.06004983615650195, test_regret: 0.027511444185692183\n",
      "Training Loader: Epoch 33/100: 100%|██████████| 7/7 [00:00<00:00, 1998.24it/s]\n",
      "2025-02-04 11:35:58,216 - decision_learning.modeling.train - INFO - epoch: 33, train_loss: 0.023451946143593107, val_metric: 0.06249938365100848, test_regret: 0.0278626688099236\n",
      "Training Loader: Epoch 34/100: 100%|██████████| 7/7 [00:00<00:00, 2018.02it/s]\n",
      "2025-02-04 11:35:58,225 - decision_learning.modeling.train - INFO - epoch: 34, train_loss: 0.02284313871392182, val_metric: 0.061302508309737616, test_regret: 0.02766191079626945\n",
      "Training Loader: Epoch 35/100: 100%|██████████| 7/7 [00:00<00:00, 2164.56it/s]\n",
      "2025-02-04 11:35:58,234 - decision_learning.modeling.train - INFO - epoch: 35, train_loss: 0.022662203492862836, val_metric: 0.0642392370972082, test_regret: 0.02858395294829604\n",
      "Training Loader: Epoch 36/100: 100%|██████████| 7/7 [00:00<00:00, 2103.31it/s]\n",
      "2025-02-04 11:35:58,244 - decision_learning.modeling.train - INFO - epoch: 36, train_loss: 0.023313467523881366, val_metric: 0.06275290561833627, test_regret: 0.0283174008477404\n",
      "Training Loader: Epoch 37/100: 100%|██████████| 7/7 [00:00<00:00, 2084.05it/s]\n",
      "2025-02-04 11:35:58,253 - decision_learning.modeling.train - INFO - epoch: 37, train_loss: 0.023224666182483946, val_metric: 0.06364065987639696, test_regret: 0.029013798823887875\n",
      "Training Loader: Epoch 38/100: 100%|██████████| 7/7 [00:00<00:00, 1832.03it/s]\n",
      "2025-02-04 11:35:58,263 - decision_learning.modeling.train - INFO - epoch: 38, train_loss: 0.02249340500150408, val_metric: 0.06816831881410616, test_regret: 0.02849386493652917\n",
      "Training Loader: Epoch 39/100: 100%|██████████| 7/7 [00:00<00:00, 2194.33it/s]\n",
      "2025-02-04 11:35:58,272 - decision_learning.modeling.train - INFO - epoch: 39, train_loss: 0.021603052637406757, val_metric: 0.06241345563024818, test_regret: 0.029050818884486133\n",
      "Training Loader: Epoch 40/100: 100%|██████████| 7/7 [00:00<00:00, 1886.29it/s]\n",
      "2025-02-04 11:35:58,281 - decision_learning.modeling.train - INFO - epoch: 40, train_loss: 0.022024712126169885, val_metric: 0.06546798220988163, test_regret: 0.02822834228385447\n",
      "Training Loader: Epoch 41/100: 100%|██████████| 7/7 [00:00<00:00, 2060.65it/s]\n",
      "2025-02-04 11:35:58,289 - decision_learning.modeling.train - INFO - epoch: 41, train_loss: 0.02207508177629539, val_metric: 0.06252923575945235, test_regret: 0.028064127057161735\n",
      "Training Loader: Epoch 42/100: 100%|██████████| 7/7 [00:00<00:00, 2067.47it/s]\n",
      "2025-02-04 11:35:58,298 - decision_learning.modeling.train - INFO - epoch: 42, train_loss: 0.021499134866254672, val_metric: 0.061402559971208893, test_regret: 0.027688439546229997\n",
      "Training Loader: Epoch 43/100: 100%|██████████| 7/7 [00:00<00:00, 2240.89it/s]\n",
      "2025-02-04 11:35:58,307 - decision_learning.modeling.train - INFO - epoch: 43, train_loss: 0.022042789097343172, val_metric: 0.06037351116013188, test_regret: 0.027429410602015765\n",
      "Training Loader: Epoch 44/100: 100%|██████████| 7/7 [00:00<00:00, 1838.22it/s]\n",
      "2025-02-04 11:35:58,318 - decision_learning.modeling.train - INFO - epoch: 44, train_loss: 0.021615006029605865, val_metric: 0.06212001649804562, test_regret: 0.026876175556302125\n",
      "Training Loader: Epoch 45/100: 100%|██████████| 7/7 [00:00<00:00, 2105.88it/s]\n",
      "2025-02-04 11:35:58,328 - decision_learning.modeling.train - INFO - epoch: 45, train_loss: 0.02165058707552297, val_metric: 0.0604714521251939, test_regret: 0.02608702972589338\n",
      "Training Loader: Epoch 46/100: 100%|██████████| 7/7 [00:00<00:00, 2016.08it/s]\n",
      "2025-02-04 11:35:58,337 - decision_learning.modeling.train - INFO - epoch: 46, train_loss: 0.02157462202012539, val_metric: 0.060680399440528957, test_regret: 0.026012824126611097\n",
      "Training Loader: Epoch 47/100: 100%|██████████| 7/7 [00:00<00:00, 1608.51it/s]\n",
      "2025-02-04 11:35:58,347 - decision_learning.modeling.train - INFO - epoch: 47, train_loss: 0.021158180864793912, val_metric: 0.061316317962553, test_regret: 0.027352031307235902\n",
      "Training Loader: Epoch 48/100: 100%|██████████| 7/7 [00:00<00:00, 1939.63it/s]\n",
      "2025-02-04 11:35:58,357 - decision_learning.modeling.train - INFO - epoch: 48, train_loss: 0.021379711904696057, val_metric: 0.06028437929991841, test_regret: 0.025256594500273555\n",
      "Training Loader: Epoch 49/100: 100%|██████████| 7/7 [00:00<00:00, 1896.53it/s]\n",
      "2025-02-04 11:35:58,376 - decision_learning.modeling.train - INFO - epoch: 49, train_loss: 0.021541639364191463, val_metric: 0.06082306042235018, test_regret: 0.02632335333486561\n",
      "Training Loader: Epoch 50/100: 100%|██████████| 7/7 [00:00<00:00, 1878.81it/s]\n",
      "2025-02-04 11:35:58,387 - decision_learning.modeling.train - INFO - epoch: 50, train_loss: 0.02132202684879303, val_metric: 0.06168513744806024, test_regret: 0.027250264026362964\n",
      "Training Loader: Epoch 51/100: 100%|██████████| 7/7 [00:00<00:00, 2005.61it/s]\n",
      "2025-02-04 11:35:58,396 - decision_learning.modeling.train - INFO - epoch: 51, train_loss: 0.021586488134094646, val_metric: 0.060808116924356274, test_regret: 0.028120508053794888\n",
      "Training Loader: Epoch 52/100: 100%|██████████| 7/7 [00:00<00:00, 2041.88it/s]\n",
      "2025-02-04 11:35:58,406 - decision_learning.modeling.train - INFO - epoch: 52, train_loss: 0.02155765491936888, val_metric: 0.061079012888573474, test_regret: 0.027163630151902183\n",
      "Training Loader: Epoch 53/100: 100%|██████████| 7/7 [00:00<00:00, 1865.68it/s]\n",
      "2025-02-04 11:35:58,415 - decision_learning.modeling.train - INFO - epoch: 53, train_loss: 0.021181847900152206, val_metric: 0.06361182532131843, test_regret: 0.026690030128558615\n",
      "Training Loader: Epoch 54/100: 100%|██████████| 7/7 [00:00<00:00, 1795.73it/s]\n",
      "2025-02-04 11:35:58,424 - decision_learning.modeling.train - INFO - epoch: 54, train_loss: 0.021160322108439038, val_metric: 0.06228270874468747, test_regret: 0.027259111007390514\n",
      "Training Loader: Epoch 55/100: 100%|██████████| 7/7 [00:00<00:00, 2056.46it/s]\n",
      "2025-02-04 11:35:58,434 - decision_learning.modeling.train - INFO - epoch: 55, train_loss: 0.022871130013040135, val_metric: 0.05985018637163081, test_regret: 0.02681456022653493\n",
      "Training Loader: Epoch 56/100: 100%|██████████| 7/7 [00:00<00:00, 2099.70it/s]\n",
      "2025-02-04 11:35:58,443 - decision_learning.modeling.train - INFO - epoch: 56, train_loss: 0.021956765492047583, val_metric: 0.06037715109388448, test_regret: 0.028371632371133156\n",
      "Training Loader: Epoch 57/100: 100%|██████████| 7/7 [00:00<00:00, 1959.69it/s]\n",
      "2025-02-04 11:35:58,452 - decision_learning.modeling.train - INFO - epoch: 57, train_loss: 0.02137654966541699, val_metric: 0.060468405279688525, test_regret: 0.028369934043410795\n",
      "Training Loader: Epoch 58/100: 100%|██████████| 7/7 [00:00<00:00, 2169.20it/s]\n",
      "2025-02-04 11:35:58,462 - decision_learning.modeling.train - INFO - epoch: 58, train_loss: 0.021254202084881917, val_metric: 0.06109346796095201, test_regret: 0.027207581131482254\n",
      "Training Loader: Epoch 59/100: 100%|██████████| 7/7 [00:00<00:00, 2051.58it/s]\n",
      "2025-02-04 11:35:58,472 - decision_learning.modeling.train - INFO - epoch: 59, train_loss: 0.022395711126072065, val_metric: 0.06184707379791641, test_regret: 0.026457131045024496\n",
      "Training Loader: Epoch 60/100: 100%|██████████| 7/7 [00:00<00:00, 1876.17it/s]\n",
      "2025-02-04 11:35:58,481 - decision_learning.modeling.train - INFO - epoch: 60, train_loss: 0.020956572677407945, val_metric: 0.06467138328958379, test_regret: 0.026338507643772817\n",
      "Training Loader: Epoch 61/100: 100%|██████████| 7/7 [00:00<00:00, 1600.36it/s]\n",
      "2025-02-04 11:35:58,492 - decision_learning.modeling.train - INFO - epoch: 61, train_loss: 0.021158820550356592, val_metric: 0.05999230659652073, test_regret: 0.027827162438341477\n",
      "Training Loader: Epoch 62/100: 100%|██████████| 7/7 [00:00<00:00, 2079.03it/s]\n",
      "2025-02-04 11:35:58,501 - decision_learning.modeling.train - INFO - epoch: 62, train_loss: 0.021811012976935933, val_metric: 0.0679431778637855, test_regret: 0.028877507588699898\n",
      "Training Loader: Epoch 63/100: 100%|██████████| 7/7 [00:00<00:00, 1918.84it/s]\n",
      "2025-02-04 11:35:58,511 - decision_learning.modeling.train - INFO - epoch: 63, train_loss: 0.021213918924331665, val_metric: 0.06190854565036451, test_regret: 0.02929132125059266\n",
      "Training Loader: Epoch 64/100: 100%|██████████| 7/7 [00:00<00:00, 1982.99it/s]\n",
      "2025-02-04 11:35:58,521 - decision_learning.modeling.train - INFO - epoch: 64, train_loss: 0.02154777411903654, val_metric: 0.06073470190269437, test_regret: 0.026354938746752318\n",
      "Training Loader: Epoch 65/100: 100%|██████████| 7/7 [00:00<00:00, 1771.14it/s]\n",
      "2025-02-04 11:35:58,532 - decision_learning.modeling.train - INFO - epoch: 65, train_loss: 0.02087042959673064, val_metric: 0.05978613284093007, test_regret: 0.026490749224554272\n",
      "Training Loader: Epoch 66/100: 100%|██████████| 7/7 [00:00<00:00, 1818.19it/s]\n",
      "2025-02-04 11:35:58,544 - decision_learning.modeling.train - INFO - epoch: 66, train_loss: 0.021315511582153186, val_metric: 0.060618654302319885, test_regret: 0.02674149032948339\n",
      "Training Loader: Epoch 67/100: 100%|██████████| 7/7 [00:00<00:00, 1935.28it/s]\n",
      "2025-02-04 11:35:58,555 - decision_learning.modeling.train - INFO - epoch: 67, train_loss: 0.02088187954255513, val_metric: 0.06009506785723916, test_regret: 0.027886912220425955\n",
      "Training Loader: Epoch 68/100: 100%|██████████| 7/7 [00:00<00:00, 1701.84it/s]\n",
      "2025-02-04 11:35:58,567 - decision_learning.modeling.train - INFO - epoch: 68, train_loss: 0.021703343306268965, val_metric: 0.06131659124831398, test_regret: 0.027546565602990573\n",
      "Training Loader: Epoch 69/100: 100%|██████████| 7/7 [00:00<00:00, 1965.86it/s]\n",
      "2025-02-04 11:35:58,580 - decision_learning.modeling.train - INFO - epoch: 69, train_loss: 0.022445058716194972, val_metric: 0.06263661961974476, test_regret: 0.026822764455839865\n",
      "Training Loader: Epoch 70/100: 100%|██████████| 7/7 [00:00<00:00, 1667.81it/s]\n",
      "2025-02-04 11:35:58,591 - decision_learning.modeling.train - INFO - epoch: 70, train_loss: 0.02086790943784373, val_metric: 0.06045077544081011, test_regret: 0.027383832711571403\n",
      "Training Loader: Epoch 71/100: 100%|██████████| 7/7 [00:00<00:00, 2159.15it/s]\n",
      "2025-02-04 11:35:58,600 - decision_learning.modeling.train - INFO - epoch: 71, train_loss: 0.020909106891070093, val_metric: 0.059811810073280694, test_regret: 0.026880479728406688\n",
      "Training Loader: Epoch 72/100: 100%|██████████| 7/7 [00:00<00:00, 1578.67it/s]\n",
      "2025-02-04 11:35:58,611 - decision_learning.modeling.train - INFO - epoch: 72, train_loss: 0.020885920152068138, val_metric: 0.060992148718717054, test_regret: 0.02730489269716156\n",
      "Training Loader: Epoch 73/100: 100%|██████████| 7/7 [00:00<00:00, 2102.26it/s]\n",
      "2025-02-04 11:35:58,621 - decision_learning.modeling.train - INFO - epoch: 73, train_loss: 0.02104269473680428, val_metric: 0.06159838957001701, test_regret: 0.027576208824712737\n",
      "Training Loader: Epoch 74/100: 100%|██████████| 7/7 [00:00<00:00, 1826.33it/s]\n",
      "2025-02-04 11:35:58,631 - decision_learning.modeling.train - INFO - epoch: 74, train_loss: 0.021391512293900763, val_metric: 0.06143488328068288, test_regret: 0.02715991821715721\n",
      "Training Loader: Epoch 75/100: 100%|██████████| 7/7 [00:00<00:00, 1755.78it/s]\n",
      "2025-02-04 11:35:58,641 - decision_learning.modeling.train - INFO - epoch: 75, train_loss: 0.021093012765049934, val_metric: 0.05934738127356573, test_regret: 0.027620154578669048\n",
      "Training Loader: Epoch 76/100: 100%|██████████| 7/7 [00:00<00:00, 1452.03it/s]\n",
      "2025-02-04 11:35:58,651 - decision_learning.modeling.train - INFO - epoch: 76, train_loss: 0.021230544362749373, val_metric: 0.059868851207646546, test_regret: 0.026742049471225833\n",
      "Training Loader: Epoch 77/100: 100%|██████████| 7/7 [00:00<00:00, 2068.49it/s]\n",
      "2025-02-04 11:35:58,661 - decision_learning.modeling.train - INFO - epoch: 77, train_loss: 0.021369634994438717, val_metric: 0.061038101428695986, test_regret: 0.02734860155617403\n",
      "Training Loader: Epoch 78/100: 100%|██████████| 7/7 [00:00<00:00, 2091.48it/s]\n",
      "2025-02-04 11:35:58,671 - decision_learning.modeling.train - INFO - epoch: 78, train_loss: 0.02140031648533685, val_metric: 0.05982883519473056, test_regret: 0.026590464577163686\n",
      "Training Loader: Epoch 79/100: 100%|██████████| 7/7 [00:00<00:00, 1762.31it/s]\n",
      "2025-02-04 11:35:58,680 - decision_learning.modeling.train - INFO - epoch: 79, train_loss: 0.020908926480582783, val_metric: 0.06178767193974296, test_regret: 0.026813833864832135\n",
      "Training Loader: Epoch 80/100: 100%|██████████| 7/7 [00:00<00:00, 1703.62it/s]\n",
      "2025-02-04 11:35:58,690 - decision_learning.modeling.train - INFO - epoch: 80, train_loss: 0.021325131079980304, val_metric: 0.05756994812793619, test_regret: 0.026364992846868687\n",
      "Training Loader: Epoch 81/100: 100%|██████████| 7/7 [00:00<00:00, 2011.38it/s]\n",
      "2025-02-04 11:35:58,700 - decision_learning.modeling.train - INFO - epoch: 81, train_loss: 0.02092399527984006, val_metric: 0.0598317308608788, test_regret: 0.026511775392694375\n",
      "Training Loader: Epoch 82/100: 100%|██████████| 7/7 [00:00<00:00, 1888.48it/s]\n",
      "2025-02-04 11:35:58,710 - decision_learning.modeling.train - INFO - epoch: 82, train_loss: 0.02197346995983805, val_metric: 0.06311992839532984, test_regret: 0.02883794264932998\n",
      "Training Loader: Epoch 83/100: 100%|██████████| 7/7 [00:00<00:00, 1927.28it/s]\n",
      "2025-02-04 11:35:58,720 - decision_learning.modeling.train - INFO - epoch: 83, train_loss: 0.021467525511980057, val_metric: 0.06246098990888639, test_regret: 0.03038852108546774\n",
      "Training Loader: Epoch 84/100: 100%|██████████| 7/7 [00:00<00:00, 1800.35it/s]\n",
      "2025-02-04 11:35:58,730 - decision_learning.modeling.train - INFO - epoch: 84, train_loss: 0.021687783034784452, val_metric: 0.061307107650948975, test_regret: 0.027298308411222567\n",
      "Training Loader: Epoch 85/100: 100%|██████████| 7/7 [00:00<00:00, 2066.16it/s]\n",
      "2025-02-04 11:35:58,741 - decision_learning.modeling.train - INFO - epoch: 85, train_loss: 0.02102221761431013, val_metric: 0.06113661803823326, test_regret: 0.027089492485728794\n",
      "Training Loader: Epoch 86/100: 100%|██████████| 7/7 [00:00<00:00, 1986.34it/s]\n",
      "2025-02-04 11:35:58,751 - decision_learning.modeling.train - INFO - epoch: 86, train_loss: 0.021534012098397528, val_metric: 0.06039598455302933, test_regret: 0.025510848966244726\n",
      "Training Loader: Epoch 87/100: 100%|██████████| 7/7 [00:00<00:00, 2038.47it/s]\n",
      "2025-02-04 11:35:58,760 - decision_learning.modeling.train - INFO - epoch: 87, train_loss: 0.02105824649333954, val_metric: 0.061424166990098145, test_regret: 0.027426437222095694\n",
      "Training Loader: Epoch 88/100: 100%|██████████| 7/7 [00:00<00:00, 1720.39it/s]\n",
      "2025-02-04 11:35:58,769 - decision_learning.modeling.train - INFO - epoch: 88, train_loss: 0.020950197907430784, val_metric: 0.062018377453324405, test_regret: 0.028220378433242547\n",
      "Training Loader: Epoch 89/100: 100%|██████████| 7/7 [00:00<00:00, 2060.07it/s]\n",
      "2025-02-04 11:35:58,784 - decision_learning.modeling.train - INFO - epoch: 89, train_loss: 0.02090587413736752, val_metric: 0.0616919114461781, test_regret: 0.027618888235844272\n",
      "Training Loader: Epoch 90/100: 100%|██████████| 7/7 [00:00<00:00, 1554.52it/s]\n",
      "2025-02-04 11:35:58,798 - decision_learning.modeling.train - INFO - epoch: 90, train_loss: 0.02225418309015887, val_metric: 0.06070814085256356, test_regret: 0.027048729136642996\n",
      "Training Loader: Epoch 91/100: 100%|██████████| 7/7 [00:00<00:00, 1968.50it/s]\n",
      "2025-02-04 11:35:58,808 - decision_learning.modeling.train - INFO - epoch: 91, train_loss: 0.021001453644462993, val_metric: 0.06234206408613564, test_regret: 0.02757754135877182\n",
      "Training Loader: Epoch 92/100: 100%|██████████| 7/7 [00:00<00:00, 1919.97it/s]\n",
      "2025-02-04 11:35:58,819 - decision_learning.modeling.train - INFO - epoch: 92, train_loss: 0.021524865446346148, val_metric: 0.06185968564505602, test_regret: 0.02551660934550405\n",
      "Training Loader: Epoch 93/100: 100%|██████████| 7/7 [00:00<00:00, 1955.39it/s]\n",
      "2025-02-04 11:35:58,829 - decision_learning.modeling.train - INFO - epoch: 93, train_loss: 0.021659630749906813, val_metric: 0.06179255619589661, test_regret: 0.028215650985613332\n",
      "Training Loader: Epoch 94/100: 100%|██████████| 7/7 [00:00<00:00, 1781.56it/s]\n",
      "2025-02-04 11:35:58,838 - decision_learning.modeling.train - INFO - epoch: 94, train_loss: 0.020595377577202662, val_metric: 0.07044785930692168, test_regret: 0.031072067510912064\n",
      "Training Loader: Epoch 95/100: 100%|██████████| 7/7 [00:00<00:00, 2105.88it/s]\n",
      "2025-02-04 11:35:58,848 - decision_learning.modeling.train - INFO - epoch: 95, train_loss: 0.021399979612657, val_metric: 0.0636150582337249, test_regret: 0.03101041211802937\n",
      "Training Loader: Epoch 96/100: 100%|██████████| 7/7 [00:00<00:00, 1971.80it/s]\n",
      "2025-02-04 11:35:58,856 - decision_learning.modeling.train - INFO - epoch: 96, train_loss: 0.020694306918552945, val_metric: 0.06216977195031561, test_regret: 0.02827410829675423\n",
      "Training Loader: Epoch 97/100: 100%|██████████| 7/7 [00:00<00:00, 1897.51it/s]\n",
      "2025-02-04 11:35:58,876 - decision_learning.modeling.train - INFO - epoch: 97, train_loss: 0.02097428696496146, val_metric: 0.06072738714764521, test_regret: 0.026666692492841516\n",
      "Training Loader: Epoch 98/100: 100%|██████████| 7/7 [00:00<00:00, 990.52it/s]\n",
      "2025-02-04 11:35:58,910 - decision_learning.modeling.train - INFO - epoch: 98, train_loss: 0.02089583155299936, val_metric: 0.06024925335774673, test_regret: 0.02616950922946408\n",
      "Training Loader: Epoch 99/100: 100%|██████████| 7/7 [00:00<00:00, 1091.86it/s]\n",
      "2025-02-04 11:35:58,936 - decision_learning.modeling.train - INFO - epoch: 99, train_loss: 0.021858039711202894, val_metric: 0.06042959870162964, test_regret: 0.028334215163129055\n",
      "Training Loader: Epoch 100/100: 100%|██████████| 7/7 [00:00<00:00, 913.59it/s]\n",
      "2025-02-04 11:35:58,952 - decision_learning.modeling.train - INFO - epoch: 100, train_loss: 0.02071602429662432, val_metric: 0.06401147376650027, test_regret: 0.029448368663359748\n",
      "2025-02-04 11:35:58,953 - decision_learning.utils - INFO - Function 'train' took 1.2228021621704102 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "custom_results, custom_models = lossfn_experiment_pipeline(X_train=generated_data['feat'],\n",
    "                true_cost_train=generated_data['cost'],\n",
    "                X_test=generated_data_test['feat'],\n",
    "                true_cost_test=generated_data_test['cost_true'], \n",
    "                predmodel=pred_model,\n",
    "                optmodel=optmodel,\n",
    "                train_solver_kwargs=train_solver_kwargs,\n",
    "                test_solver_kwargs=test_solver_kwargs,\n",
    "                val_split_params={'test_size':200, 'random_state':42},                                \n",
    "                custom_loss_inputs=custom_loss_inputs,\n",
    "                training_configs={'num_epochs':100,\n",
    "                                 'dataloader_params': {'batch_size':32, 'shuffle':True}},\n",
    "                save_models=False,\n",
    "                training_loop_verbose=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a7a1c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_regret</th>\n",
       "      <th>loss_name</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>0.020716</td>\n",
       "      <td>0.064011</td>\n",
       "      <td>0.029448</td>\n",
       "      <td>cosine</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  train_loss  val_metric  test_regret loss_name hyperparameters\n",
       "99     99    0.020716    0.064011     0.029448    cosine            None"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_results[custom_results.epoch == 99]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7abce12-eb9e-4570-9789-960e9348dc1c",
   "metadata": {},
   "source": [
    "# Combine all existing examples so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "522ca681-6abb-4b5a-aeb4-300e9fb3f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results = pd.concat([preimplement_loss_results, custom_results, PG_init_results], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7652951c-6bf6-4dd9-8ea1-3dd0eda04716",
   "metadata": {},
   "source": [
    "## Find test regret using validation regret for hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b6ee69c-2642-4dc3-8d8f-346fa02dd7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_regret</th>\n",
       "      <th>loss_name</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>97</td>\n",
       "      <td>14.291678</td>\n",
       "      <td>0.042557</td>\n",
       "      <td>0.009365</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.5156692688606229, 'finite_diff_type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>76</td>\n",
       "      <td>0.021567</td>\n",
       "      <td>0.058663</td>\n",
       "      <td>0.025965</td>\n",
       "      <td>Cosine</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>79</td>\n",
       "      <td>0.021325</td>\n",
       "      <td>0.057570</td>\n",
       "      <td>0.026365</td>\n",
       "      <td>cosine</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>4.137239</td>\n",
       "      <td>0.063587</td>\n",
       "      <td>0.031507</td>\n",
       "      <td>SPO+</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>89</td>\n",
       "      <td>0.772511</td>\n",
       "      <td>0.104221</td>\n",
       "      <td>0.064851</td>\n",
       "      <td>MSE</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  train_loss  val_metric  test_regret loss_name  \\\n",
       "497     97   14.291678    0.042557     0.009365        PG   \n",
       "276     76    0.021567    0.058663     0.025965    Cosine   \n",
       "379     79    0.021325    0.057570     0.026365    cosine   \n",
       "70      70    4.137239    0.063587     0.031507      SPO+   \n",
       "189     89    0.772511    0.104221     0.064851       MSE   \n",
       "\n",
       "                                       hyperparameters  \n",
       "497  {'h': 0.5156692688606229, 'finite_diff_type': ...  \n",
       "276                                                 {}  \n",
       "379                                               None  \n",
       "70                                                  {}  \n",
       "189                                                 {}  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_results.loc[combined_results.groupby('loss_name')['val_metric'].idxmin()].sort_values(by='test_regret')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decision_learning",
   "language": "python",
   "name": "decision_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
