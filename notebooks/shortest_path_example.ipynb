{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c7ce37e-e3da-4734-8bc7-c87bd478bc44",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51f68c4c-2c49-4d5c-aaaf-8a86c324b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from functools import partial\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from decision_learning.data.shortest_path_grid import genData\n",
    "from decision_learning.modeling.loss import SPOPlus\n",
    "from decision_learning.modeling.models import LinearRegression\n",
    "from decision_learning.modeling.val_metrics import decision_regret\n",
    "from decision_learning.modeling.train import train, calc_test_regret, init_loss_data_pretraining, filter_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e292d-c0a3-4b68-8beb-d45fb76fdb61",
   "metadata": {},
   "source": [
    "# Training Loop Overview\n",
    "While user can run entire experiment pipeline using `decision_learning.modeling.pipeline`, sometimes they may want more detailed control over training data setup (compared to the default behavior in `decision_learning.modeling.pipeline.lossfn_experiment_data_pipeline`, or hyperparmeter searching (compared to the default grid search in the pipeline function `decision_learning.modeling.pipeline.lossfn_experiment_pipeline`). This may happen to a highly customized loss function or experiment process. In that case, we may still want a function to handle Pytorch training loop behavior and we can use the appropriate customized components without needing to delve into boilerplate Pytorch code. In that case, we can use the `decision_learning.modeling.train.train` function to handle pytorch training functionality\n",
    "\n",
    "At a high level, the train function needs the following components:\n",
    "- prediction model: predicting true costs\n",
    "- optimization model: linear optimization model parameterized by cost/coefficient vector for objective function, and returns the corresponding solution, objective value.\n",
    "- loss function: a callable nn.Module loss function that can be used for PyTorch autograd functionality\n",
    "- structured training data inputs: dictionary mapping from keys to specific data (features, true costs, true obj, true sol), for training, validation, and test sets\n",
    "- val metric: a callable function used during each epoch to evaluate model on the validation set\n",
    "- misc training loop parameters: device, number epochs, optimizer, learning rate, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a8b45b-6515-4c96-99aa-b22e3c83ddad",
   "metadata": {},
   "source": [
    "## Optimization Model (linear)\n",
    "\n",
    "Decision-aware/focused problems require an optimization model to actually solve the decision problem. Since each decision problem is unique in terms of the modeling and solving, the user is expected to provide the optimization model function/object, which is treated like a black-box by the `pipeline`,`train`, and loss/regret functions in the code base. It could be Gurobi, Pyomo, or any user custom solver. However, to play nicely with the rest of the package, it must do the following:\n",
    "\n",
    "- Input Argument when called:\n",
    "    - costs: vector of objective function coefficients. Expected to be numpy np.ndarray or torch.tensor\n",
    "- Returns 2 objects:\n",
    "    - sols: solution to optimization model given the input costs. Expected to be numpy np.ndarray or torch.tensor\n",
    "    - obj: objective value to optimization model given the input costs. Expected to be numpy np.ndarray or torch.tensor\n",
    " \n",
    "The return objects of optimal solution and objective are generally returned as any solver, and any linear program needs its objective function to be parameterized by a vector of cost/coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba50d038-23db-4a42-bd73-41546dff57b5",
   "metadata": {},
   "source": [
    "### Example Solver/Optimization Model\n",
    "- Below, `shortest_path_solver` is a custom user optimization model specified in the form of a callable function, and its first input argument is the vector of costs. The rest of the input arguments size, sens, need to be pre-set before being passed to `pipeline`, `train`, or any loss function. This can be accomplished using the `partial` python function (see example below). The exact implementation is not important but mainly that it:\n",
    "    - accepts a costs vector input\n",
    "    - returns solution (sol) and objective value (obj) for the input cost vector\n",
    "- Note that `shortest_path_solver` also has two returns: sol, obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96846d76-e16b-498a-9d35-90454258f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path_solver(costs, size, sens = 1e-4):\n",
    "    # Forward Pass\n",
    "    starting_ind = 0\n",
    "    starting_ind_c = 0\n",
    "    samples = costs.shape[0]\n",
    "    V_arr = torch.zeros(samples, size ** 2)\n",
    "    for i in range(0, 2 * (size - 1)):\n",
    "        num_nodes = min(i + 1, 9 - i)\n",
    "        num_nodes_next = min(i + 2, 9 - i - 1)\n",
    "        num_arcs = 2 * (max(num_nodes, num_nodes_next) - 1)\n",
    "        V_1 = V_arr[:, starting_ind:starting_ind + num_nodes]\n",
    "        layer_costs = costs[:, starting_ind_c:starting_ind_c + num_arcs]\n",
    "        l_costs = layer_costs[:, 0::2]\n",
    "        r_costs = layer_costs[:, 1::2]\n",
    "        next_V_val_l = torch.ones(samples, num_nodes_next) * float('inf')\n",
    "        next_V_val_r = torch.ones(samples, num_nodes_next) * float('inf')\n",
    "        if num_nodes_next > num_nodes:\n",
    "            next_V_val_l[:, :num_nodes_next - 1] = V_1 + l_costs\n",
    "            next_V_val_r[:, 1:num_nodes_next] = V_1 + r_costs\n",
    "        else:\n",
    "            next_V_val_l = V_1[:, :num_nodes_next] + l_costs\n",
    "            next_V_val_r = V_1[:, 1:num_nodes_next + 1] + r_costs\n",
    "        next_V_val = torch.minimum(next_V_val_l, next_V_val_r)\n",
    "        V_arr[:, starting_ind + num_nodes:starting_ind + num_nodes + num_nodes_next] = next_V_val\n",
    "\n",
    "        starting_ind += num_nodes\n",
    "        starting_ind_c += num_arcs\n",
    "\n",
    "    # Backward Pass\n",
    "    starting_ind = size ** 2\n",
    "    starting_ind_c = costs.shape[1]\n",
    "    prev_act = torch.ones(samples, 1)\n",
    "    sol = torch.zeros(costs.shape)\n",
    "    for i in range(2 * (size - 1), 0, -1):\n",
    "        num_nodes = min(i + 1, 9 - i)\n",
    "        num_nodes_next = min(i, 9 - i + 1)\n",
    "        V_1 = V_arr[:, starting_ind - num_nodes:starting_ind]\n",
    "        V_2 = V_arr[:, starting_ind - num_nodes - num_nodes_next:starting_ind - num_nodes]\n",
    "\n",
    "        num_arcs = 2 * (max(num_nodes, num_nodes_next) - 1)\n",
    "        layer_costs = costs[:, starting_ind_c - num_arcs: starting_ind_c]\n",
    "\n",
    "        if num_nodes < num_nodes_next:\n",
    "            l_cs_res = ((V_2[:, :num_nodes_next - 1] - V_1 + layer_costs[:, ::2]) < sens) * prev_act\n",
    "            r_cs_res = ((V_2[:, 1:num_nodes_next] - V_1 + layer_costs[:, 1::2]) < sens) * prev_act\n",
    "            prev_act = torch.zeros(V_2.shape)\n",
    "            prev_act[:, :num_nodes_next - 1] += l_cs_res\n",
    "            prev_act[:, 1:num_nodes_next] += r_cs_res\n",
    "        else:\n",
    "            l_cs_res = ((V_2 - V_1[:, :num_nodes - 1] + layer_costs[:, ::2]) < sens) * prev_act[:, :num_nodes - 1]\n",
    "            r_cs_res = ((V_2 - V_1[:, 1:num_nodes] + layer_costs[:, 1::2]) < sens) * prev_act[:, 1:num_nodes]\n",
    "            prev_act = torch.zeros(V_2.shape)\n",
    "            prev_act += l_cs_res\n",
    "            prev_act += r_cs_res\n",
    "        cs = torch.zeros(layer_costs.shape)\n",
    "        cs[:, ::2] = l_cs_res\n",
    "        cs[:, 1::2] = r_cs_res\n",
    "        sol[:, starting_ind_c - num_arcs: starting_ind_c] = cs\n",
    "\n",
    "        starting_ind = starting_ind - num_nodes\n",
    "        starting_ind_c = starting_ind_c - num_arcs\n",
    "    # Dimension (samples, num edges)\n",
    "    obj = torch.sum(sol * costs, axis=1)\n",
    "    # Dimension (samples, 1)\n",
    "    return sol.to(torch.float32), obj.reshape(-1,1).to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10aaf87-47b5-4352-af7d-d617ef4cd2b0",
   "metadata": {},
   "source": [
    "### Presetting non-cost input arguments of `shortest_path_solver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97eb42d0-1b1d-4d6a-8bae-90decd46745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------optimization model------------\n",
    "optmodel = partial(shortest_path_solver,size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7a5c1-079e-418a-8f9d-13cb5fa0d172",
   "metadata": {},
   "source": [
    "## Data Generation Setup\n",
    "Any decision-aware/focused problem will of course need data inputs. The example below uses a pre-implemented synthetic data generator provided in the package found within \n",
    "`decision_learning.data.shortest_path_grid` to generate shortest path problem and can be generated by calling the `genData` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09062476-7703-40a0-9ffa-56f6953499fe",
   "metadata": {},
   "source": [
    "### Specific parameters to set up data generation\n",
    "This data setup, and the synthetic data generation is in line with the paper https://arxiv.org/pdf/2402.03256 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca23dd97-13d8-4ef7-a83b-b5029867d6f8",
   "metadata": {},
   "source": [
    "### Create Experiments Grid\n",
    "This shortest path experiment has two important settings:\n",
    "- number of samples: less samples means higher error/more noise, more samples means lower error/less noise\n",
    "- epsilon: noise level on edge costs, can be uniformly distributed multiplicative noise, or normally distributed additive noise\n",
    "- This example below creates 100 trials for 8 different settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63a075bc-c47a-43fc-b8e6-168d1250e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control the randomization seeding for pytorch\n",
    "torch.manual_seed(105)\n",
    "indices_arr = torch.randperm(100000)\n",
    "indices_arr_test = torch.randperm(100000)\n",
    "\n",
    "n_arr = [200, 400, 800, 1600] # array of number of samples for an experiment\n",
    "ep_arr = ['unif', 'normal'] # noise type\n",
    "trials = 100 # number of trials per setting\n",
    "\n",
    "# create an array where each item is [number of samples, noise type, trial number] representing an experiment run\n",
    "exp_arr = []\n",
    "for n in n_arr:\n",
    "    for ep in ep_arr:\n",
    "        for t in range(trials):\n",
    "            exp_arr.append([n, ep, t]) # add current [number of samples, noise type, trial number] experiment run setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f671e307-62fa-4c16-bbda-cc222de460f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current experiment setting: number of data points 200, epsilon type unif, trial number 0\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "sim = 0 # simulation trial number, only show one experiment run for demonstration purposes\n",
    "exp = exp_arr[sim] # current experiment\n",
    "num_data = exp[0]  # number of training data\n",
    "ep_type = exp[1] # noise type of current experiment\n",
    "trial = exp[2] # trial number of current experiment\n",
    "\n",
    "# shortest path problem data generation parameters - https://arxiv.org/pdf/2402.03256\n",
    "grid = (5, 5)  # grid size\n",
    "num_feat = 5  # size of feature\n",
    "deg = 6  # polynomial degree in edge cost function\n",
    "e = .3  # noise width/amount of noise\n",
    "\n",
    "# path planting for shortest path example - see page 9, subsection \"Harder Example with Planted Arcs\" in section 4.2 of paper https://arxiv.org/pdf/2402.03256\n",
    "planted_good_pwl_params = {'slope0':0, # slope of first segment of piecewise linear cost function for \"good\" edge cost planted\n",
    "                    'int0':2, # intercept of first segment of piecewise linear cost function for \"good\" edge cost planted\n",
    "                    'slope1':0, # slope of second segment of piecewise linear cost function for \"good\" edge cost planted\n",
    "                    'int1':2} # intercept of second segment of piecewise linear cost function for \"good\" edge cost planted\n",
    "planted_bad_pwl_params = {'slope0':4, # slope of first segment of piecewise linear cost function for \"bad\" edge cost planted\n",
    "                    'int0':0, # intercept of first segment of piecewise linear cost function for \"bad\" edge cost planted\n",
    "                    'slope1':0, # slope of second segment of piecewise linear cost function for \"bad\" edge cost planted\n",
    "                    'int1':2.2} # intercept of second segment of piecewise linear cost function for \"bad\" edge cost planted\n",
    "plant_edge = True # to plant edges in shortest path experiment or not\n",
    "\n",
    "print(f'current experiment setting: number of data points {num_data}, epsilon type {ep_type}, trial number {trial}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818a50ab-6542-44a6-a578-8385d581091a",
   "metadata": {},
   "source": [
    "### Calling `genData` from `decision_learning.data.shortest_path_grid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed917ba3-a8b3-4ce0-a93c-d0ecd9399282",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------DATA------------\n",
    "# training data\n",
    "generated_data = genData(num_data=num_data+200, # number of data points to generate for training set\n",
    "        num_features=num_feat, # number of features \n",
    "        grid=grid, # grid shape\n",
    "        deg=deg, # polynomial degree\n",
    "        noise_type=ep_type, # epsilon noise type\n",
    "        noise_width=e, # amount of noise\n",
    "        seed=indices_arr[trial], # seed the randomness\n",
    "        plant_edges=plant_edge, # to plant edges or not\n",
    "        planted_good_pwl_params=planted_good_pwl_params, # cost function for good edges\n",
    "        planted_bad_pwl_params=planted_bad_pwl_params) # cost function for bad edges\n",
    "\n",
    "# testing data\n",
    "generated_data_test = genData(num_data=10000, # number of data points to generate for test set\n",
    "        num_features=num_feat, # number of features \n",
    "        grid=grid,  # grid shape\n",
    "        deg=deg,  # polynomial degree\n",
    "        noise_type=ep_type,  # epsilon noise type\n",
    "        noise_width=e, # amount of noise\n",
    "        seed=indices_arr_test[trial],      # seed the randomness\n",
    "        plant_edges=plant_edge, # to plant edges or not\n",
    "        planted_good_pwl_params=planted_good_pwl_params, # cost function for good edges\n",
    "        planted_bad_pwl_params=planted_bad_pwl_params) # cost function for bad edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d78968-cdab-4087-94d3-71201f96d2f8",
   "metadata": {},
   "source": [
    "### Split Data into train, val and create structured data inputs\n",
    "For many decision aware loss function experiment processes, we need the following four:\n",
    "- X: features\n",
    "- true_cost: true cost associated with each X\n",
    "- true_sol: true solution of LP given true_cost\n",
    "- true_obj: true objective of LP given true_cost\n",
    "\n",
    "In this case, we will need to get the `true_sol` and `true_obj` for each data sample/`true_cost` vector by plugging into our optimization solver. The code block below shows this.\n",
    "\n",
    "Furthermore, this package's training loop, in order to flexibly handle different loss function signatures and behavior, requires data to be organized as dictionaries mapping key->data where the keys correspond to named arguments in the loss function. This way, the training loop can flexibly inject or remove the correct named arguments to each loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d1ef73d-f20b-4432-bac1-b17413158f10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training data - get true_sol, true_obj\n",
    "sol, obj = shortest_path_solver(costs=generated_data['cost'], size=5) # plug into solver\n",
    "# get structured data in the form of dictionary\n",
    "final_data = {'X':generated_data['feat'],\n",
    "              'true_cost':generated_data['cost'],\n",
    "              'true_sol':sol,\n",
    "              'true_obj':obj}\n",
    "\n",
    "# ------------------TRAIN/VAL SPLIT--------------------\n",
    "train_dict = {}\n",
    "val_dict = {}\n",
    "\n",
    "# For each (key,value) tuple in final_data, we split into train val split\n",
    "# using sklearn train_test_split. Because the test_size, random_state seed are\n",
    "# always the same, ensures each (key,value) are split the same way across indices\n",
    "# (this behavior has been checked/tested)\n",
    "for key, value in final_data.items():\n",
    "    train_data, val_data = train_test_split(value, test_size=200, random_state=42)\n",
    "    train_dict[key] = train_data\n",
    "    val_dict[key] = val_data\n",
    "    \n",
    "# test data - get true_sol, true_obj and structured data form\n",
    "sol_test, obj_test = shortest_path_solver(costs=generated_data_test['cost_true'], size=5)\n",
    "final_data_test = {'X':generated_data_test['feat'],\n",
    "              'true_cost':generated_data_test['cost_true'],\n",
    "              'true_sol':sol_test,\n",
    "              'true_obj':obj_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56224c51-da29-4f65-98c0-94f4aa5eedd2",
   "metadata": {},
   "source": [
    "## Prediction Model\n",
    "- Any decision-aware/focused problem will of course need prediction model to predict the cost/coefficient vector given contextual input/features. This example uses a simple `LinearRegression` object implemented within `decision_learning.modeling.models`. \n",
    "- The package expects the prediction model to be a PyTorch model since PyTorch offers convenient autograd functionality/allows user to specify custom losses/backwards passes that are found within many decision-aware/focused works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa766de7-a36e-4ef4-a69a-806c604627bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------prediction model------------\n",
    "pred_model = LinearRegression(input_dim=generated_data['feat'].shape[1],\n",
    "                 output_dim=generated_data['cost'].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "7aee636a-fa74-4451-b14d-468f18490c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3895,  0.1444, -0.3851, -0.3101, -0.0109,  0.3007],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Num of cores: 1\n",
      "Optimizing for optDataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 1052.83it/s]\n"
     ]
    }
   ],
   "source": [
    "import pyepo\n",
    "\n",
    "feat, cost_true, cost, eps = sp_grid_mike_neurips.genData(num_data + 200, \n",
    "                                                     num_feat, \n",
    "                                                     grid, \n",
    "                                                     ep_type, \n",
    "                                                     deg, \n",
    "                                                     e, \n",
    "                                                     seed=indices_arr[trial])\n",
    "x_train, x_val, c_train, c_val = train_test_split(feat, cost, test_size=200, random_state=42)\n",
    "x_test, c_test, c_hat_test, eps_test = sp_grid_mike_neurips.genData(10000, \n",
    "                                                          num_feat, \n",
    "                                                          grid, \n",
    "                                                          ep_type, \n",
    "                                                          deg, \n",
    "                                                          e, \n",
    "                                                          seed=indices_arr_test[trial])\n",
    "\n",
    "old_model = LinearRegression(input_dim=train_dict['X'].shape[1],\n",
    "                 output_dim=train_dict['true_cost'].shape[1])\n",
    "old_model.linear = copy.deepcopy(predmodel.linear)\n",
    "print(old_model.linear.weight[0])\n",
    "\n",
    "optmodel_old = sp_grid_mike_neurips.shortestPathModel()\n",
    "old_loss_fn = SPOPlus2(optmodel_old)\n",
    "\n",
    "optimizer = torch.optim.Adam(old_model.parameters(), lr=0.01)\n",
    "# build dataset\n",
    "dataset = pyepo.data.dataset.optDataset(optmodel_old, x_train[:,:-1], c_train)\n",
    "loader_train = DataLoader(dataset, batch_size=200, shuffle=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f6b099-29f3-4b36-bc0f-babe6912b8e6",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "Below, we use the preimplemented `SPOPlus` loss from `decision_learning.modeling.loss`, which requires an optimization model input for solving for sol,obj under current predicted costs for loss, backpropogation each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e929b7b8-c808-4c61-8f36-d9fd1c0e0bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_fn = SPOPlus(optmodel=optmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f18f9d-5b81-4fac-a215-1b0ec5082514",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcd4986-6c9c-4e3f-b0bf-9eefc2f996e5",
   "metadata": {},
   "source": [
    "Initialize Inputs to Trainining Loop: the below code block was previously already instantiated, here it is copy and pasting the separate components for ease of reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e274229f-11c0-4bb7-bc94-1b1b89434653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Model\n",
    "pred_model = LinearRegression(input_dim=train_dict['X'].shape[1],\n",
    "                 output_dim=train_dict['true_cost'].shape[1])\n",
    "\n",
    "# optimization solver\n",
    "optmodel = partial(shortest_path_solver,size=5)\n",
    "\n",
    "# training, validation data\n",
    "train_data_dict = train_dict\n",
    "val_data_dict = val_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3bb225b-680d-41b1-a2c4-f35a98a6bd7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/.conda/envs/pyepo_dsl/lib/python3.11/site-packages/torch/autograd/__init__.py:266: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "metrics, trained_model = train(pred_model=pred_model,\n",
    "                optmodel=optmodel,\n",
    "                loss_fn=loss_fn,\n",
    "                train_data_dict=train_data_dict,\n",
    "                val_data_dict=val_data_dict,\n",
    "                test_data_dict=final_data_test,\n",
    "                dataloader_params={'batch_size':200, 'shuffle':True},\n",
    "                num_epochs=100,\n",
    "                lr=0.01,\n",
    "                scheduler_params=None, #{'step_size': 10, 'gamma': 0.1},\n",
    "                minimization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a87ef66d-e675-4b1a-9510-aa7a0ba395a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_regret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13.636871</td>\n",
       "      <td>0.352304</td>\n",
       "      <td>0.321175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.056755</td>\n",
       "      <td>0.351924</td>\n",
       "      <td>0.305195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12.505816</td>\n",
       "      <td>0.347754</td>\n",
       "      <td>0.288633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11.984661</td>\n",
       "      <td>0.324476</td>\n",
       "      <td>0.272795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11.486254</td>\n",
       "      <td>0.332619</td>\n",
       "      <td>0.256549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>4.242315</td>\n",
       "      <td>0.220304</td>\n",
       "      <td>0.046371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>4.210351</td>\n",
       "      <td>0.213829</td>\n",
       "      <td>0.046720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>4.248733</td>\n",
       "      <td>0.211432</td>\n",
       "      <td>0.046008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>4.131743</td>\n",
       "      <td>0.227005</td>\n",
       "      <td>0.046019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>4.126982</td>\n",
       "      <td>0.226158</td>\n",
       "      <td>0.045630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  train_loss  val_metric  test_regret\n",
       "0       0   13.636871    0.352304     0.321175\n",
       "1       1   13.056755    0.351924     0.305195\n",
       "2       2   12.505816    0.347754     0.288633\n",
       "3       3   11.984661    0.324476     0.272795\n",
       "4       4   11.486254    0.332619     0.256549\n",
       "..    ...         ...         ...          ...\n",
       "95     95    4.242315    0.220304     0.046371\n",
       "96     96    4.210351    0.213829     0.046720\n",
       "97     97    4.248733    0.211432     0.046008\n",
       "98     98    4.131743    0.227005     0.046019\n",
       "99     99    4.126982    0.226158     0.045630\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd2aa83-3dd4-411b-8f4a-9ef34bbb3154",
   "metadata": {},
   "source": [
    "# Evaluation Regret\n",
    "After training model, we may want to evaluate the model on a separate performance metric like normalized regret on a test dataset. While this is automatically done as part of the `train` function, here we explicitly show it for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f74df8dc-db4c-45fe-88ea-e691f03e4dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.045630453251899586\n"
     ]
    }
   ],
   "source": [
    "test_regret = calc_test_regret(pred_model=trained_model,\n",
    "                               test_data_dict=final_data_test, #final_data_te,\n",
    "                               optmodel=optmodel)\n",
    "print(test_regret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0732580-2130-4130-894a-6b88c0a7d0f4",
   "metadata": {},
   "source": [
    "# Further Examples\n",
    "Since we may want to try different loss functions for decision aware problems, below are two more examples of preimplemented loss functions. The key is to ensure the training data dictionary named keys match the named arguments of the specific loss function. (Note we don't need to do this for val, test data dictionaries since those are not passed to loss function, but rather the validation metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7059cb-9e27-421c-a5e3-e05dd0be540c",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1951dc79-e2b4-4777-8253-78f2259f1c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Model\n",
    "pred_model = LinearRegression(input_dim=train_dict['X'].shape[1],\n",
    "                 output_dim=train_dict['true_cost'].shape[1])\n",
    "\n",
    "# optimization solver\n",
    "optmodel = partial(shortest_path_solver,size=5)\n",
    "\n",
    "# loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# training, validation data - need to modify training data dict to match inputs to loss function\n",
    "train_data_dict = train_dict\n",
    "train_data_dict.update({'target':train_dict['true_cost']}) # extra input key needed for loss function\n",
    "\n",
    "val_data_dict = val_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "969caf1e-f62a-4296-8430-49e7ea3220fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics, trained_model = train(pred_model=pred_model,\n",
    "                optmodel=optmodel,\n",
    "                loss_fn=loss_fn,\n",
    "                train_data_dict=train_data_dict,\n",
    "                val_data_dict=val_data_dict,\n",
    "                num_epochs=100,\n",
    "                lr=0.1,\n",
    "                scheduler_params={'step_size': 10, 'gamma': 0.1},\n",
    "                minimization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "882ab81e-b685-49b1-a268-122ebac53c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05776541068493522\n"
     ]
    }
   ],
   "source": [
    "test_regret = calc_test_regret(pred_model=trained_model,\n",
    "                               test_data_dict=final_data_test,\n",
    "                               optmodel=optmodel)\n",
    "print(test_regret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f634575-13cd-4657-96b8-ee878cfb5998",
   "metadata": {},
   "source": [
    "## Cosine Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "676498b9-0111-48c7-b90f-aece245b635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Model\n",
    "pred_model = LinearRegression(input_dim=train_dict['X'].shape[1],\n",
    "                 output_dim=train_dict['true_cost'].shape[1])\n",
    "\n",
    "# optimization solver\n",
    "optmodel = partial(shortest_path_solver,size=5)\n",
    "\n",
    "# loss function\n",
    "loss_fn = nn.CosineEmbeddingLoss()\n",
    "\n",
    "# training, validation data\n",
    "train_data_dict = train_dict\n",
    "train_data_dict.update({'input2':train_dict['true_cost'], \n",
    "                       'target':torch.ones(train_dict['true_cost'].shape[0])}) # extra input key needed for loss function\n",
    "\n",
    "val_data_dict = val_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a7238d8-86dd-462a-94c8-edc9a4d95658",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics, trained_model = train(pred_model=pred_model,\n",
    "                optmodel=optmodel,\n",
    "                loss_fn=loss_fn,\n",
    "                train_data_dict=train_data_dict,\n",
    "                val_data_dict=val_data_dict,\n",
    "                num_epochs=100,\n",
    "                lr=0.1,\n",
    "                scheduler_params={'step_size': 10, 'gamma': 0.1},\n",
    "                minimization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28317d84-cda8-4cb5-a5dd-106cf75adaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030530295741855704\n"
     ]
    }
   ],
   "source": [
    "test_regret = calc_test_regret(pred_model=trained_model,\n",
    "                               test_data_dict=final_data_test,\n",
    "                               optmodel=optmodel)\n",
    "print(test_regret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyepo_dsl)",
   "language": "python",
   "name": "pyepo_dsl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
