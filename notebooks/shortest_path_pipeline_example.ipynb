{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ade29e-a77d-44e2-96f1-fc0f6a975780",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4baf8671-4258-44c4-be4f-161728a53c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from functools import partial\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import decision_learning.modeling.pipeline\n",
    "import decision_learning.data.shortest_path_grid\n",
    "\n",
    "from decision_learning.modeling.models import LinearRegression\n",
    "from decision_learning.modeling.pipeline import lossfn_experiment_pipeline, lossfn_hyperparam_grid\n",
    "from decision_learning.data.shortest_path_grid import genData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf793762-b0e9-4994-a0e4-b8a68dbedb1f",
   "metadata": {},
   "source": [
    "# Pipeline Setup Overview\n",
    "To run pipeline function in `decision_learning.modeling.pipeline`, we need these components:\n",
    "- data (features, true costs), and appropriate train-test splits\n",
    "- prediction model: predicting true costs\n",
    "- optimization model: linear optimization model parameterized by cost/coefficient vector for objective function, and returns the corresponding solution, objective value.\n",
    "- existing loss functions (hyperparameter configs):loss functions to train the prediction model against true costs as labels and already implemented within `decision_learning.modeling.loss` specified with the loss name as string, and hyperparameters to search over as a dictionary.\n",
    "- custom loss functions: user provided loss function as a callable, and data dictionary with appropriate features, and labels required by loss function\n",
    "- misc params: other parameters to set for pipeline experiment function\n",
    "    - val_split_params={'test_size':200, 'random_state':42},\n",
    "    - training configuration: ex: {'num_epochs':100, 'dataloader_params': {'batch_size':200, 'shuffle':True}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bb933f-d853-419a-affe-33eb5ca30c39",
   "metadata": {},
   "source": [
    "## Optimization Model (linear)\n",
    "\n",
    "Decision-aware/focused problems require an optimization model to actually solve the decision problem. Since each decision problem is unique in terms of the modeling and solving, the user is expected to provide the optimization model function/object, which is treated like a black-box by the `pipeline`,`train`, and loss/regret functions in the code base. It could be Gurobi, Pyomo, or any user custom solver. However, to play nicely with the rest of the package, it must do the following:\n",
    "\n",
    "- Input Argument when called:\n",
    "    - costs: vector of objective function coefficients. Expected to be numpy np.ndarray or torch.tensor\n",
    "- Returns 2 objects:\n",
    "    - sols: solution to optimization model given the input costs. Expected to be numpy np.ndarray or torch.tensor\n",
    "    - obj: objective value to optimization model given the input costs. Expected to be numpy np.ndarray or torch.tensor\n",
    " \n",
    "The return objects of optimal solution and objective are generally returned as any solver, and any linear program needs its objective function to be parameterized by a vector of cost/coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a075f24e-0ba4-46a4-a761-78e4ce2fb0ed",
   "metadata": {},
   "source": [
    "### Example Solver/Optimization Model\n",
    "- Below, `shortest_path_solver` is a custom user optimization model specified in the form of a callable function, and its first input argument is the vector of costs. The rest of the input arguments size, sens, need to be pre-set before being passed to `pipeline`, `train`, or any loss function. This can be accomplished using the `partial` python function (see example below). The exact implementation is not important but mainly that it:\n",
    "    - accepts a costs vector input\n",
    "    - returns solution (sol) and objective value (obj) for the input cost vector\n",
    "- Note that `shortest_path_solver` also has two returns: sol, obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0cc057-2b90-488e-b434-88d16ca8cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path_solver(costs, size, sens = 1e-4):\n",
    "    # Forward Pass\n",
    "    starting_ind = 0\n",
    "    starting_ind_c = 0\n",
    "    samples = costs.shape[0]\n",
    "    V_arr = torch.zeros(samples, size ** 2)\n",
    "    for i in range(0, 2 * (size - 1)):\n",
    "        num_nodes = min(i + 1, 9 - i)\n",
    "        num_nodes_next = min(i + 2, 9 - i - 1)\n",
    "        num_arcs = 2 * (max(num_nodes, num_nodes_next) - 1)\n",
    "        V_1 = V_arr[:, starting_ind:starting_ind + num_nodes]\n",
    "        layer_costs = costs[:, starting_ind_c:starting_ind_c + num_arcs]\n",
    "        l_costs = layer_costs[:, 0::2]\n",
    "        r_costs = layer_costs[:, 1::2]\n",
    "        next_V_val_l = torch.ones(samples, num_nodes_next) * float('inf')\n",
    "        next_V_val_r = torch.ones(samples, num_nodes_next) * float('inf')\n",
    "        if num_nodes_next > num_nodes:\n",
    "            next_V_val_l[:, :num_nodes_next - 1] = V_1 + l_costs\n",
    "            next_V_val_r[:, 1:num_nodes_next] = V_1 + r_costs\n",
    "        else:\n",
    "            next_V_val_l = V_1[:, :num_nodes_next] + l_costs\n",
    "            next_V_val_r = V_1[:, 1:num_nodes_next + 1] + r_costs\n",
    "        next_V_val = torch.minimum(next_V_val_l, next_V_val_r)\n",
    "        V_arr[:, starting_ind + num_nodes:starting_ind + num_nodes + num_nodes_next] = next_V_val\n",
    "\n",
    "        starting_ind += num_nodes\n",
    "        starting_ind_c += num_arcs\n",
    "\n",
    "    # Backward Pass\n",
    "    starting_ind = size ** 2\n",
    "    starting_ind_c = costs.shape[1]\n",
    "    prev_act = torch.ones(samples, 1)\n",
    "    sol = torch.zeros(costs.shape)\n",
    "    for i in range(2 * (size - 1), 0, -1):\n",
    "        num_nodes = min(i + 1, 9 - i)\n",
    "        num_nodes_next = min(i, 9 - i + 1)\n",
    "        V_1 = V_arr[:, starting_ind - num_nodes:starting_ind]\n",
    "        V_2 = V_arr[:, starting_ind - num_nodes - num_nodes_next:starting_ind - num_nodes]\n",
    "\n",
    "        num_arcs = 2 * (max(num_nodes, num_nodes_next) - 1)\n",
    "        layer_costs = costs[:, starting_ind_c - num_arcs: starting_ind_c]\n",
    "\n",
    "        if num_nodes < num_nodes_next:\n",
    "            l_cs_res = ((V_2[:, :num_nodes_next - 1] - V_1 + layer_costs[:, ::2]) < sens) * prev_act\n",
    "            r_cs_res = ((V_2[:, 1:num_nodes_next] - V_1 + layer_costs[:, 1::2]) < sens) * prev_act\n",
    "            prev_act = torch.zeros(V_2.shape)\n",
    "            prev_act[:, :num_nodes_next - 1] += l_cs_res\n",
    "            prev_act[:, 1:num_nodes_next] += r_cs_res\n",
    "        else:\n",
    "            l_cs_res = ((V_2 - V_1[:, :num_nodes - 1] + layer_costs[:, ::2]) < sens) * prev_act[:, :num_nodes - 1]\n",
    "            r_cs_res = ((V_2 - V_1[:, 1:num_nodes] + layer_costs[:, 1::2]) < sens) * prev_act[:, 1:num_nodes]\n",
    "            prev_act = torch.zeros(V_2.shape)\n",
    "            prev_act += l_cs_res\n",
    "            prev_act += r_cs_res\n",
    "        cs = torch.zeros(layer_costs.shape)\n",
    "        cs[:, ::2] = l_cs_res\n",
    "        cs[:, 1::2] = r_cs_res\n",
    "        sol[:, starting_ind_c - num_arcs: starting_ind_c] = cs\n",
    "\n",
    "        starting_ind = starting_ind - num_nodes\n",
    "        starting_ind_c = starting_ind_c - num_arcs\n",
    "    # Dimension (samples, num edges)\n",
    "    obj = torch.sum(sol * costs, axis=1)\n",
    "    # Dimension (samples, 1)\n",
    "    sol = sol.to(torch.float32)\n",
    "    obj = obj.reshape(-1,1).to(torch.float32)\n",
    "    return sol, obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e59d0-469f-4d64-b6f4-1b51a624d1bd",
   "metadata": {},
   "source": [
    "### Presetting non-cost input arguments of `shortest_path_solver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae92ee9-ce23-424c-8fa6-f47d06892522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------optimization model------------\n",
    "optmodel = partial(shortest_path_solver,size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571bb3c5-d810-4bd4-981a-6972f4d2d475",
   "metadata": {},
   "source": [
    "## Data Generation Setup\n",
    "Any decision-aware/focused problem will of course need data inputs. The example below uses a pre-implemented synthetic data generator provided in the package found within \n",
    "`decision_learning.data.shortest_path_grid` to generate shortest path problem and can be generated by calling the `genData` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb52ecaf-e5ef-4125-9431-79d1893b9a08",
   "metadata": {},
   "source": [
    "### Specific parameters to set up data generation\n",
    "This data setup, and the synthetic data generation is in line with the paper https://arxiv.org/pdf/2402.03256 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a53af7-bf10-486f-ad90-159897baba3b",
   "metadata": {},
   "source": [
    "### Create Experiments Grid\n",
    "This shortest path experiment has two important settings:\n",
    "- number of samples: less samples means higher error/more noise, more samples means lower error/less noise\n",
    "- epsilon: noise level on edge costs, can be uniformly distributed multiplicative noise, or normally distributed additive noise\n",
    "- This example below creates 100 trials for 8 different settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c950eb8-0d00-48c8-b211-f0afec1d0007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control the randomization seeding for pytorch\n",
    "torch.manual_seed(105)\n",
    "indices_arr = torch.randperm(100000)\n",
    "indices_arr_test = torch.randperm(100000)\n",
    "\n",
    "n_arr = [200, 400, 800, 1600] # array of number of samples for an experiment\n",
    "ep_arr = ['unif', 'normal'] # noise type\n",
    "trials = 100 # number of trials per setting\n",
    "\n",
    "# create an array where each item is [number of samples, noise type, trial number] representing an experiment run\n",
    "exp_arr = []\n",
    "for n in n_arr:\n",
    "    for ep in ep_arr:\n",
    "        for t in range(trials):\n",
    "            exp_arr.append([n, ep, t]) # add current [number of samples, noise type, trial number] experiment run setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21a3dd33-3f0a-4966-8649-63c76ff46e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current experiment setting: number of data points 200, epsilon type unif, trial number 0\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "sim = 0 # simulation trial number, only show one experiment run for demonstration purposes\n",
    "exp = exp_arr[sim] # current experiment\n",
    "num_data = exp[0]  # number of training data\n",
    "ep_type = exp[1] # noise type of current experiment\n",
    "trial = exp[2] # trial number of current experiment\n",
    "\n",
    "# shortest path problem data generation parameters - https://arxiv.org/pdf/2402.03256\n",
    "grid = (5, 5)  # grid size\n",
    "num_feat = 5  # size of feature\n",
    "deg = 6  # polynomial degree in edge cost function\n",
    "e = .3  # noise width/amount of noise\n",
    "\n",
    "# path planting for shortest path example - see page 9, subsection \"Harder Example with Planted Arcs\" in section 4.2 of paper https://arxiv.org/pdf/2402.03256\n",
    "planted_good_pwl_params = {'slope0':0, # slope of first segment of piecewise linear cost function for \"good\" edge cost planted\n",
    "                    'int0':2, # intercept of first segment of piecewise linear cost function for \"good\" edge cost planted\n",
    "                    'slope1':0, # slope of second segment of piecewise linear cost function for \"good\" edge cost planted\n",
    "                    'int1':2} # intercept of second segment of piecewise linear cost function for \"good\" edge cost planted\n",
    "planted_bad_pwl_params = {'slope0':4, # slope of first segment of piecewise linear cost function for \"bad\" edge cost planted\n",
    "                    'int0':0, # intercept of first segment of piecewise linear cost function for \"bad\" edge cost planted\n",
    "                    'slope1':0, # slope of second segment of piecewise linear cost function for \"bad\" edge cost planted\n",
    "                    'int1':2.2} # intercept of second segment of piecewise linear cost function for \"bad\" edge cost planted\n",
    "plant_edge = True # to plant edges in shortest path experiment or not\n",
    "\n",
    "print(f'current experiment setting: number of data points {num_data}, epsilon type {ep_type}, trial number {trial}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59553dc-1f26-4848-97f1-5de3006afd70",
   "metadata": {},
   "source": [
    "### Calling `genData` from `decision_learning.data.shortest_path_grid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6208b7ef-6b2e-4a04-b68f-323ca537e207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------DATA------------\n",
    "# training data\n",
    "generated_data = genData(num_data=num_data+200, # number of data points to generate for training set\n",
    "        num_features=num_feat, # number of features \n",
    "        grid=grid, # grid shape\n",
    "        deg=deg, # polynomial degree\n",
    "        noise_type=ep_type, # epsilon noise type\n",
    "        noise_width=e, # amount of noise\n",
    "        seed=indices_arr[trial], # seed the randomness\n",
    "        plant_edges=plant_edge, # to plant edges or not\n",
    "        planted_good_pwl_params=planted_good_pwl_params, # cost function for good edges\n",
    "        planted_bad_pwl_params=planted_bad_pwl_params) # cost function for bad edges\n",
    "\n",
    "# testing data\n",
    "generated_data_test = genData(num_data=10000, # number of data points to generate for test set\n",
    "        num_features=num_feat, # number of features \n",
    "        grid=grid,  # grid shape\n",
    "        deg=deg,  # polynomial degree\n",
    "        noise_type=ep_type,  # epsilon noise type\n",
    "        noise_width=e, # amount of noise\n",
    "        seed=indices_arr_test[trial],      # seed the randomness\n",
    "        plant_edges=plant_edge, # to plant edges or not\n",
    "        planted_good_pwl_params=planted_good_pwl_params, # cost function for good edges\n",
    "        planted_bad_pwl_params=planted_bad_pwl_params) # cost function for bad edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b2dd8-1cbb-48a2-b648-d50e2255ada6",
   "metadata": {},
   "source": [
    "## Prediction Model\n",
    "- Any decision-aware/focused problem will of course need prediction model to predict the cost/coefficient vector given contextual input/features. This example uses a simple `LinearRegression` object implemented within `decision_learning.modeling.models`. \n",
    "- The package expects the prediction model to be a PyTorch model since PyTorch offers convenient autograd functionality/allows user to specify custom losses/backwards passes that are found within many decision-aware/focused works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b32a03bb-f163-4f38-b81b-8574a61253a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------prediction model------------\n",
    "pred_model = LinearRegression(input_dim=generated_data['feat'].shape[1],\n",
    "                 output_dim=generated_data['cost'].shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972c6cc7-c367-4294-960c-2149d0e5ba08",
   "metadata": {},
   "source": [
    "# Pipeline Function Overview\n",
    "Pipeline function `lossfn_experiment_pipeline` from `decision_learning.modeling.pipeline` takes in the following arguments:\n",
    "- X_train: training set features\n",
    "- true_cost_train: training set true costs\n",
    "- X_test: test set features\n",
    "- true_cost_test: test set true costs\n",
    "- predmodel: pytorch prediction model\n",
    "- optmodel: optimization model\n",
    "- val_split_params: how to split training data into train/val splits. Defaults to {'test_size':0.2, 'random_state':42}.\n",
    "- loss_names: list of loss functions to run experiment pipeline on that are implemented already in the codebase in decision_learning.modeling.loss\n",
    "- loss_configs: dictionary mapping from loss_name (key) to a dictionary of different hyperparameters that are then grid searched over.\n",
    "- custom_loss_inputs:list of custom loss function configurations to run through the train function as part of experient pipeline\n",
    "- minimize: minimization problem?\n",
    "- training_configs: parameters to be passed into train function for pytorch training loop. \n",
    "- save_models: flag to save models or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb45d5-9dcc-4351-bad3-1dcaf469cefa",
   "metadata": {},
   "source": [
    "# Example: Off-the-Shelf Preimplemented Loss Functions\n",
    "Here since we only use off-the-shelf preimplemented loss functions, without any hyperparameter searching, we only need to specify the individual loss names `['SPO+', 'MSE']` to `loss_name` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad0dd576-08a8-4205-bd5f-96603ce39242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "preimplement_loss_results, preimplement_loss_models = lossfn_experiment_pipeline(X_train=generated_data['feat'],\n",
    "                true_cost_train=generated_data['cost'],\n",
    "                X_test=generated_data_test['feat'],\n",
    "                true_cost_test=generated_data_test['cost_true'], \n",
    "                predmodel=pred_model,\n",
    "                optmodel=optmodel,\n",
    "                val_split_params={'test_size':200, 'random_state':42},\n",
    "                loss_names=['SPO+', 'MSE'],                            \n",
    "                training_configs={'num_epochs':100,\n",
    "                                 'dataloader_params': {'batch_size':200, 'shuffle':True}},\n",
    "                save_models=True                                                                                 \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97631bdd-60fc-481c-8089-7b3a229917ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_regret</th>\n",
       "      <th>loss_name</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>4.306511</td>\n",
       "      <td>0.189203</td>\n",
       "      <td>0.044372</td>\n",
       "      <td>SPO+</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>99</td>\n",
       "      <td>2.063001</td>\n",
       "      <td>0.439980</td>\n",
       "      <td>0.236546</td>\n",
       "      <td>MSE</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  train_loss  val_metric  test_regret loss_name hyperparameters\n",
       "99      99    4.306511    0.189203     0.044372      SPO+              {}\n",
       "199     99    2.063001    0.439980     0.236546       MSE              {}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preimplement_loss_results[preimplement_loss_results.epoch == 99]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92815220-580f-4857-acf5-bbea917bdd06",
   "metadata": {},
   "source": [
    "### Saved Down Models\n",
    "models saved as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e77adca-8885-49ea-938f-02d2cd196bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SPO+_{}': LinearRegression(\n",
       "   (linear): Linear(in_features=6, out_features=40, bias=True)\n",
       " ),\n",
       " 'MSE_{}': LinearRegression(\n",
       "   (linear): Linear(in_features=6, out_features=40, bias=True)\n",
       " )}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preimplement_loss_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1b038b-0739-4224-9335-130c5722ed13",
   "metadata": {},
   "source": [
    "# Providing Hyperparameter Search Example\n",
    "Here since we we are still using off-the-shelf preimplemented loss functions, but now since PG loss accepts two arguments ('h': width size, and 'finite_diff_type': finite different scheme}, we can search over the hyperparameters by inputting them into the `loss_configs` argument in the exaxmple below as: `{'PG': {'h':[num_data**-.125, num_data**-.25, num_data**-.5, num_data**-1], 'finite_diff_type': ['B', 'C', 'F']}}`. The pipeline function will use a helper function `lossfn_hyperparam_grid` to take the cartesian product of the `h` and `finite_diff_type` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c32ab754-a6e5-4b09-a55b-2d74054656f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "PG_results, PG_models = lossfn_experiment_pipeline(X_train=generated_data['feat'],\n",
    "                true_cost_train=generated_data['cost'],\n",
    "                X_test=generated_data_test['feat'],\n",
    "                true_cost_test=generated_data_test['cost_true'], \n",
    "                predmodel=pred_model,\n",
    "                optmodel=optmodel,\n",
    "                val_split_params={'test_size':200, 'random_state':42},\n",
    "                loss_names=['PG'],\n",
    "                loss_configs={'PG': {'h':[num_data**-.125, num_data**-.25, num_data**-.5, num_data**-1], 'finite_diff_type': ['B', 'C', 'F']}},\n",
    "                training_configs={'num_epochs':100,\n",
    "                                 'dataloader_params': {'batch_size':200, 'shuffle':True}},\n",
    "                save_models=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a4ad715-d010-4c06-af95-64e568ea1094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_regret</th>\n",
       "      <th>loss_name</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>15.553047</td>\n",
       "      <td>0.153668</td>\n",
       "      <td>0.110702</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.5156692688606229, 'finite_diff_type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>99</td>\n",
       "      <td>15.179111</td>\n",
       "      <td>0.174463</td>\n",
       "      <td>0.104197</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.5156692688606229, 'finite_diff_type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>99</td>\n",
       "      <td>14.057665</td>\n",
       "      <td>0.221629</td>\n",
       "      <td>0.042703</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.5156692688606229, 'finite_diff_type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>99</td>\n",
       "      <td>15.498624</td>\n",
       "      <td>0.192540</td>\n",
       "      <td>0.111207</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.26591479484724945, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>99</td>\n",
       "      <td>15.176310</td>\n",
       "      <td>0.159259</td>\n",
       "      <td>0.101614</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.26591479484724945, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>99</td>\n",
       "      <td>14.663220</td>\n",
       "      <td>0.192385</td>\n",
       "      <td>0.075795</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.26591479484724945, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>99</td>\n",
       "      <td>15.297663</td>\n",
       "      <td>0.195223</td>\n",
       "      <td>0.109455</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.07071067811865475, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>99</td>\n",
       "      <td>15.304697</td>\n",
       "      <td>0.184279</td>\n",
       "      <td>0.109064</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.07071067811865475, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>99</td>\n",
       "      <td>15.325191</td>\n",
       "      <td>0.194410</td>\n",
       "      <td>0.114124</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.07071067811865475, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>99</td>\n",
       "      <td>15.940586</td>\n",
       "      <td>0.226152</td>\n",
       "      <td>0.153410</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.005, 'finite_diff_type': 'B'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>99</td>\n",
       "      <td>16.016407</td>\n",
       "      <td>0.231691</td>\n",
       "      <td>0.153148</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.005, 'finite_diff_type': 'C'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>99</td>\n",
       "      <td>16.335396</td>\n",
       "      <td>0.235089</td>\n",
       "      <td>0.182718</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.005, 'finite_diff_type': 'F'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      epoch  train_loss  val_metric  test_regret loss_name  \\\n",
       "99       99   15.553047    0.153668     0.110702        PG   \n",
       "199      99   15.179111    0.174463     0.104197        PG   \n",
       "299      99   14.057665    0.221629     0.042703        PG   \n",
       "399      99   15.498624    0.192540     0.111207        PG   \n",
       "499      99   15.176310    0.159259     0.101614        PG   \n",
       "599      99   14.663220    0.192385     0.075795        PG   \n",
       "699      99   15.297663    0.195223     0.109455        PG   \n",
       "799      99   15.304697    0.184279     0.109064        PG   \n",
       "899      99   15.325191    0.194410     0.114124        PG   \n",
       "999      99   15.940586    0.226152     0.153410        PG   \n",
       "1099     99   16.016407    0.231691     0.153148        PG   \n",
       "1199     99   16.335396    0.235089     0.182718        PG   \n",
       "\n",
       "                                        hyperparameters  \n",
       "99    {'h': 0.5156692688606229, 'finite_diff_type': ...  \n",
       "199   {'h': 0.5156692688606229, 'finite_diff_type': ...  \n",
       "299   {'h': 0.5156692688606229, 'finite_diff_type': ...  \n",
       "399   {'h': 0.26591479484724945, 'finite_diff_type':...  \n",
       "499   {'h': 0.26591479484724945, 'finite_diff_type':...  \n",
       "599   {'h': 0.26591479484724945, 'finite_diff_type':...  \n",
       "699   {'h': 0.07071067811865475, 'finite_diff_type':...  \n",
       "799   {'h': 0.07071067811865475, 'finite_diff_type':...  \n",
       "899   {'h': 0.07071067811865475, 'finite_diff_type':...  \n",
       "999               {'h': 0.005, 'finite_diff_type': 'B'}  \n",
       "1099              {'h': 0.005, 'finite_diff_type': 'C'}  \n",
       "1199              {'h': 0.005, 'finite_diff_type': 'F'}  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PG_results[PG_results.epoch == 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f1246a9-41ef-4867-866b-707b61c9ad07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PG_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc086f-aef8-461b-b555-9e95143f6dc3",
   "metadata": {},
   "source": [
    "# Specific Model Initialization Example\n",
    "Here we will use the pre-trained `SPO+` model as initialization point for PG loss example from above and observe the improvement in test_regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2440fac9-24fd-47e2-bf20-97599580c78c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "PG_init_results, PG_init_models = lossfn_experiment_pipeline(X_train=generated_data['feat'],\n",
    "                true_cost_train=generated_data['cost'],\n",
    "                X_test=generated_data_test['feat'],\n",
    "                true_cost_test=generated_data_test['cost_true'], \n",
    "                predmodel=preimplement_loss_models['SPO+_{}'],\n",
    "                optmodel=optmodel,\n",
    "                val_split_params={'test_size':200, 'random_state':42},\n",
    "                loss_names=['PG'],\n",
    "                loss_configs={'PG': {'h':[num_data**-.125, num_data**-.25, num_data**-.5, num_data**-1], 'finite_diff_type': ['B', 'C', 'F']}},\n",
    "                training_configs={'num_epochs':100,\n",
    "                                 'dataloader_params': {'batch_size':200, 'shuffle':True}},\n",
    "                save_models=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2619437-1ce8-49c0-bc00-f884f32cd81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_regret</th>\n",
       "      <th>loss_name</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>14.572641</td>\n",
       "      <td>0.186284</td>\n",
       "      <td>0.030941</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.5156692688606229, 'finite_diff_type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>99</td>\n",
       "      <td>14.241334</td>\n",
       "      <td>0.192212</td>\n",
       "      <td>0.011308</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.5156692688606229, 'finite_diff_type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>99</td>\n",
       "      <td>13.625747</td>\n",
       "      <td>0.217974</td>\n",
       "      <td>0.088459</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.5156692688606229, 'finite_diff_type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>99</td>\n",
       "      <td>14.227107</td>\n",
       "      <td>0.167983</td>\n",
       "      <td>0.013983</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.26591479484724945, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>99</td>\n",
       "      <td>14.104239</td>\n",
       "      <td>0.185358</td>\n",
       "      <td>0.007053</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.26591479484724945, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>99</td>\n",
       "      <td>13.905497</td>\n",
       "      <td>0.227982</td>\n",
       "      <td>0.052451</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.26591479484724945, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>99</td>\n",
       "      <td>14.259411</td>\n",
       "      <td>0.198385</td>\n",
       "      <td>0.022180</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.07071067811865475, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>99</td>\n",
       "      <td>14.149658</td>\n",
       "      <td>0.196140</td>\n",
       "      <td>0.018171</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.07071067811865475, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>99</td>\n",
       "      <td>14.182386</td>\n",
       "      <td>0.186763</td>\n",
       "      <td>0.024017</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.07071067811865475, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>99</td>\n",
       "      <td>14.507373</td>\n",
       "      <td>0.235413</td>\n",
       "      <td>0.039757</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.005, 'finite_diff_type': 'B'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>99</td>\n",
       "      <td>14.300651</td>\n",
       "      <td>0.210332</td>\n",
       "      <td>0.030548</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.005, 'finite_diff_type': 'C'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>99</td>\n",
       "      <td>14.314169</td>\n",
       "      <td>0.203718</td>\n",
       "      <td>0.028768</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.005, 'finite_diff_type': 'F'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      epoch  train_loss  val_metric  test_regret loss_name  \\\n",
       "99       99   14.572641    0.186284     0.030941        PG   \n",
       "199      99   14.241334    0.192212     0.011308        PG   \n",
       "299      99   13.625747    0.217974     0.088459        PG   \n",
       "399      99   14.227107    0.167983     0.013983        PG   \n",
       "499      99   14.104239    0.185358     0.007053        PG   \n",
       "599      99   13.905497    0.227982     0.052451        PG   \n",
       "699      99   14.259411    0.198385     0.022180        PG   \n",
       "799      99   14.149658    0.196140     0.018171        PG   \n",
       "899      99   14.182386    0.186763     0.024017        PG   \n",
       "999      99   14.507373    0.235413     0.039757        PG   \n",
       "1099     99   14.300651    0.210332     0.030548        PG   \n",
       "1199     99   14.314169    0.203718     0.028768        PG   \n",
       "\n",
       "                                        hyperparameters  \n",
       "99    {'h': 0.5156692688606229, 'finite_diff_type': ...  \n",
       "199   {'h': 0.5156692688606229, 'finite_diff_type': ...  \n",
       "299   {'h': 0.5156692688606229, 'finite_diff_type': ...  \n",
       "399   {'h': 0.26591479484724945, 'finite_diff_type':...  \n",
       "499   {'h': 0.26591479484724945, 'finite_diff_type':...  \n",
       "599   {'h': 0.26591479484724945, 'finite_diff_type':...  \n",
       "699   {'h': 0.07071067811865475, 'finite_diff_type':...  \n",
       "799   {'h': 0.07071067811865475, 'finite_diff_type':...  \n",
       "899   {'h': 0.07071067811865475, 'finite_diff_type':...  \n",
       "999               {'h': 0.005, 'finite_diff_type': 'B'}  \n",
       "1099              {'h': 0.005, 'finite_diff_type': 'C'}  \n",
       "1199              {'h': 0.005, 'finite_diff_type': 'F'}  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PG_init_results[PG_init_results.epoch == 99]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d75eb3-683d-4d11-b355-40333a48cd5d",
   "metadata": {},
   "source": [
    "# Custom Loss Function Example\n",
    "Simple example using `nn.CosineEmbeddingLoss`, which takes different input arguments then our existing decision-aware loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c95d9777-39f3-405d-970c-a4bb116cbd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------custom loss function inputs------------\n",
    "# every dictionary key is necessary for custom loss inputs \n",
    "custom_loss_inputs = [{'loss_name':'cosine', # name of loss function - used just for final metric logging purposes\n",
    "                      'loss':nn.CosineEmbeddingLoss, # callable function \n",
    "                      'data': {'X': generated_data['feat'], # data input for loss function, X is the feature name, it must be X for pipeline function\n",
    "                               'input2':generated_data['cost'], # remaining is whatever labels, arguments, the loss function needs. input2, target are arguments used by nn.CosineEmbeddingLoss\n",
    "                               'target':torch.ones(generated_data['cost'].shape[0])}\n",
    "                      }\n",
    "                     ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "976df299-bfe3-46fe-ad60-a5112d269c4e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/train.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = {key: torch.tensor(value, dtype=torch.float32) for key, value in kwargs.items()}\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_hat = torch.tensor(w_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.tensor(z_hat, dtype=torch.float32)\n",
      "/home1/yongpeng/decision-focused-learning/src/decision_learning/modeling/val_metrics.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_obj = torch.tensor(true_obj, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "custom_results, custom_models = lossfn_experiment_pipeline(X_train=generated_data['feat'],\n",
    "                true_cost_train=generated_data['cost'],\n",
    "                X_test=generated_data_test['feat'],\n",
    "                true_cost_test=generated_data_test['cost_true'], \n",
    "                predmodel=pred_model,\n",
    "                optmodel=optmodel,\n",
    "                val_split_params={'test_size':200, 'random_state':42},                                \n",
    "                custom_loss_inputs=custom_loss_inputs,\n",
    "                training_configs={'num_epochs':100,\n",
    "                                 'dataloader_params': {'batch_size':200, 'shuffle':True}},\n",
    "                save_models=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52a19293-ea06-49a4-b59b-1c97ea577acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_regret</th>\n",
       "      <th>loss_name</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>0.022789</td>\n",
       "      <td>0.181629</td>\n",
       "      <td>0.029064</td>\n",
       "      <td>cosine</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  train_loss  val_metric  test_regret loss_name hyperparameters\n",
       "99     99    0.022789    0.181629     0.029064    cosine            None"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_results[custom_results.epoch == 99]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7abce12-eb9e-4570-9789-960e9348dc1c",
   "metadata": {},
   "source": [
    "# Combine all existing examples so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "522ca681-6abb-4b5a-aeb4-300e9fb3f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results = pd.concat([preimplement_loss_results, custom_results, PG_init_results], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7652951c-6bf6-4dd9-8ea1-3dd0eda04716",
   "metadata": {},
   "source": [
    "## Find test regret using validation regret for hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b6ee69c-2642-4dc3-8d8f-346fa02dd7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_regret</th>\n",
       "      <th>loss_name</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>35</td>\n",
       "      <td>14.333408</td>\n",
       "      <td>0.151630</td>\n",
       "      <td>0.015145</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.26591479484724945, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>90</td>\n",
       "      <td>0.023462</td>\n",
       "      <td>0.131474</td>\n",
       "      <td>0.029915</td>\n",
       "      <td>cosine</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>4.306511</td>\n",
       "      <td>0.189203</td>\n",
       "      <td>0.044372</td>\n",
       "      <td>SPO+</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>99</td>\n",
       "      <td>2.063001</td>\n",
       "      <td>0.439980</td>\n",
       "      <td>0.236546</td>\n",
       "      <td>MSE</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  train_loss  val_metric  test_regret loss_name  \\\n",
       "635     35   14.333408    0.151630     0.015145        PG   \n",
       "290     90    0.023462    0.131474     0.029915    cosine   \n",
       "99      99    4.306511    0.189203     0.044372      SPO+   \n",
       "199     99    2.063001    0.439980     0.236546       MSE   \n",
       "\n",
       "                                       hyperparameters  \n",
       "635  {'h': 0.26591479484724945, 'finite_diff_type':...  \n",
       "290                                               None  \n",
       "99                                                  {}  \n",
       "199                                                 {}  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_results.loc[combined_results.groupby('loss_name')['val_metric'].idxmin()].sort_values(by='test_regret')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyepo_dsl)",
   "language": "python",
   "name": "pyepo_dsl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
