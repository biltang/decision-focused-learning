{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ade29e-a77d-44e2-96f1-fc0f6a975780",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4baf8671-4258-44c4-be4f-161728a53c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from functools import partial\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import decision_learning.modeling.pipeline\n",
    "import decision_learning.data.shortest_path_grid\n",
    "\n",
    "from decision_learning.modeling.models import LinearRegression\n",
    "from decision_learning.modeling.pipeline import lossfn_experiment_pipeline, lossfn_hyperparam_grid\n",
    "from decision_learning.data.shortest_path_grid import genData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf793762-b0e9-4994-a0e4-b8a68dbedb1f",
   "metadata": {},
   "source": [
    "# Pipeline Setup Overview\n",
    "To run pipeline function in `decision_learning.modeling.pipeline`, we need these components:\n",
    "- data (features, true costs), and appropriate train-test splits\n",
    "- prediction model: predicting true costs\n",
    "- optimization model: linear optimization model parameterized by cost/coefficient vector for objective function, and returns the corresponding solution, objective value.\n",
    "- existing loss functions (hyperparameter configs):loss functions to train the prediction model against true costs as labels and already implemented within `decision_learning.modeling.loss` specified with the loss name as string, and hyperparameters to search over as a dictionary.\n",
    "- custom loss functions: user provided loss function as a callable, and data dictionary with appropriate features, and labels required by loss function\n",
    "- misc params: other parameters to set for pipeline experiment function\n",
    "    - val_split_params={'test_size':200, 'random_state':42},\n",
    "    - training configuration: ex: {'num_epochs':100, 'dataloader_params': {'batch_size':200, 'shuffle':True}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bb933f-d853-419a-affe-33eb5ca30c39",
   "metadata": {},
   "source": [
    "## Optimization Model (linear)\n",
    "\n",
    "Decision-aware/focused problems require an optimization model to actually solve the decision problem. Since each decision problem is unique in terms of the modeling and solving, the user is expected to provide the optimization model function/object, which is treated like a black-box by the `pipeline`,`train`, and loss/regret functions in the code base. It could be Gurobi, Pyomo, or any user custom solver. However, to play nicely with the rest of the package, it must do the following:\n",
    "\n",
    "- Input Argument when called:\n",
    "    - costs: vector of objective function coefficients. Expected to be numpy np.ndarray or torch.tensor\n",
    "- Returns 2 objects:\n",
    "    - sols: solution to optimization model given the input costs. Expected to be numpy np.ndarray or torch.tensor\n",
    "    - obj: objective value to optimization model given the input costs. Expected to be numpy np.ndarray or torch.tensor\n",
    " \n",
    "The return objects of optimal solution and objective are generally returned as any solver, and any linear program needs its objective function to be parameterized by a vector of cost/coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a075f24e-0ba4-46a4-a761-78e4ce2fb0ed",
   "metadata": {},
   "source": [
    "### Example Solver/Optimization Model\n",
    "- Below, `shortest_path_solver` is a custom user optimization model specified in the form of a callable function, and its first input argument is the vector of costs. The rest of the input arguments size, sens, need to be pre-set before being passed to `pipeline`, `train`, or any loss function. This can be accomplished using the `partial` python function (see example below). The exact implementation is not important but mainly that it:\n",
    "    - accepts a costs vector input\n",
    "    - returns solution (sol) and objective value (obj) for the input cost vector\n",
    "- Note that `shortest_path_solver` also has two returns: sol, obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0cc057-2b90-488e-b434-88d16ca8cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path_solver(costs, size, sens = 1e-4):\n",
    "    # Forward Pass\n",
    "    starting_ind = 0\n",
    "    starting_ind_c = 0\n",
    "    samples = costs.shape[0]\n",
    "    V_arr = torch.zeros(samples, size ** 2)\n",
    "    for i in range(0, 2 * (size - 1)):\n",
    "        num_nodes = min(i + 1, 9 - i)\n",
    "        num_nodes_next = min(i + 2, 9 - i - 1)\n",
    "        num_arcs = 2 * (max(num_nodes, num_nodes_next) - 1)\n",
    "        V_1 = V_arr[:, starting_ind:starting_ind + num_nodes]\n",
    "        layer_costs = costs[:, starting_ind_c:starting_ind_c + num_arcs]\n",
    "        l_costs = layer_costs[:, 0::2]\n",
    "        r_costs = layer_costs[:, 1::2]\n",
    "        next_V_val_l = torch.ones(samples, num_nodes_next) * float('inf')\n",
    "        next_V_val_r = torch.ones(samples, num_nodes_next) * float('inf')\n",
    "        if num_nodes_next > num_nodes:\n",
    "            next_V_val_l[:, :num_nodes_next - 1] = V_1 + l_costs\n",
    "            next_V_val_r[:, 1:num_nodes_next] = V_1 + r_costs\n",
    "        else:\n",
    "            next_V_val_l = V_1[:, :num_nodes_next] + l_costs\n",
    "            next_V_val_r = V_1[:, 1:num_nodes_next + 1] + r_costs\n",
    "        next_V_val = torch.minimum(next_V_val_l, next_V_val_r)\n",
    "        V_arr[:, starting_ind + num_nodes:starting_ind + num_nodes + num_nodes_next] = next_V_val\n",
    "\n",
    "        starting_ind += num_nodes\n",
    "        starting_ind_c += num_arcs\n",
    "\n",
    "    # Backward Pass\n",
    "    starting_ind = size ** 2\n",
    "    starting_ind_c = costs.shape[1]\n",
    "    prev_act = torch.ones(samples, 1)\n",
    "    sol = torch.zeros(costs.shape)\n",
    "    for i in range(2 * (size - 1), 0, -1):\n",
    "        num_nodes = min(i + 1, 9 - i)\n",
    "        num_nodes_next = min(i, 9 - i + 1)\n",
    "        V_1 = V_arr[:, starting_ind - num_nodes:starting_ind]\n",
    "        V_2 = V_arr[:, starting_ind - num_nodes - num_nodes_next:starting_ind - num_nodes]\n",
    "\n",
    "        num_arcs = 2 * (max(num_nodes, num_nodes_next) - 1)\n",
    "        layer_costs = costs[:, starting_ind_c - num_arcs: starting_ind_c]\n",
    "\n",
    "        if num_nodes < num_nodes_next:\n",
    "            l_cs_res = ((V_2[:, :num_nodes_next - 1] - V_1 + layer_costs[:, ::2]) < sens) * prev_act\n",
    "            r_cs_res = ((V_2[:, 1:num_nodes_next] - V_1 + layer_costs[:, 1::2]) < sens) * prev_act\n",
    "            prev_act = torch.zeros(V_2.shape)\n",
    "            prev_act[:, :num_nodes_next - 1] += l_cs_res\n",
    "            prev_act[:, 1:num_nodes_next] += r_cs_res\n",
    "        else:\n",
    "            l_cs_res = ((V_2 - V_1[:, :num_nodes - 1] + layer_costs[:, ::2]) < sens) * prev_act[:, :num_nodes - 1]\n",
    "            r_cs_res = ((V_2 - V_1[:, 1:num_nodes] + layer_costs[:, 1::2]) < sens) * prev_act[:, 1:num_nodes]\n",
    "            prev_act = torch.zeros(V_2.shape)\n",
    "            prev_act += l_cs_res\n",
    "            prev_act += r_cs_res\n",
    "        cs = torch.zeros(layer_costs.shape)\n",
    "        cs[:, ::2] = l_cs_res\n",
    "        cs[:, 1::2] = r_cs_res\n",
    "        sol[:, starting_ind_c - num_arcs: starting_ind_c] = cs\n",
    "\n",
    "        starting_ind = starting_ind - num_nodes\n",
    "        starting_ind_c = starting_ind_c - num_arcs\n",
    "    # Dimension (samples, num edges)\n",
    "    obj = torch.sum(sol * costs, axis=1)\n",
    "    # Dimension (samples, 1)\n",
    "    sol = sol.to(torch.float32)\n",
    "    obj = obj.reshape(-1,1).to(torch.float32)\n",
    "    return sol, obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e59d0-469f-4d64-b6f4-1b51a624d1bd",
   "metadata": {},
   "source": [
    "### Presetting non-cost input arguments of `shortest_path_solver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae92ee9-ce23-424c-8fa6-f47d06892522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------optimization model------------\n",
    "optmodel = partial(shortest_path_solver,size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571bb3c5-d810-4bd4-981a-6972f4d2d475",
   "metadata": {},
   "source": [
    "## Data Generation Setup\n",
    "Any decision-aware/focused problem will of course need data inputs. The example below uses a pre-implemented synthetic data generator provided in the package found within \n",
    "`decision_learning.data.shortest_path_grid` to generate shortest path problem and can be generated by calling the `genData` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb52ecaf-e5ef-4125-9431-79d1893b9a08",
   "metadata": {},
   "source": [
    "### Specific parameters to set up data generation\n",
    "This data setup, and the synthetic data generation is in line with the paper https://arxiv.org/pdf/2402.03256 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a53af7-bf10-486f-ad90-159897baba3b",
   "metadata": {},
   "source": [
    "### Create Experiments Grid\n",
    "This shortest path experiment has two important settings:\n",
    "- number of samples: less samples means higher error/more noise, more samples means lower error/less noise\n",
    "- epsilon: noise level on edge costs, can be uniformly distributed multiplicative noise, or normally distributed additive noise\n",
    "- This example below creates 100 trials for 8 different settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c950eb8-0d00-48c8-b211-f0afec1d0007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control the randomization seeding for pytorch\n",
    "torch.manual_seed(105)\n",
    "indices_arr = torch.randperm(100000)\n",
    "indices_arr_test = torch.randperm(100000)\n",
    "\n",
    "n_arr = [200, 400, 800, 1600] # array of number of samples for an experiment\n",
    "ep_arr = ['unif', 'normal'] # noise type\n",
    "trials = 100 # number of trials per setting\n",
    "\n",
    "# create an array where each item is [number of samples, noise type, trial number] representing an experiment run\n",
    "exp_arr = []\n",
    "for n in n_arr:\n",
    "    for ep in ep_arr:\n",
    "        for t in range(trials):\n",
    "            exp_arr.append([n, ep, t]) # add current [number of samples, noise type, trial number] experiment run setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21a3dd33-3f0a-4966-8649-63c76ff46e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current experiment setting: number of data points 200, epsilon type unif, trial number 0\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "sim = 0 # simulation trial number, only show one experiment run for demonstration purposes\n",
    "exp = exp_arr[sim] # current experiment\n",
    "num_data = exp[0]  # number of training data\n",
    "ep_type = exp[1] # noise type of current experiment\n",
    "trial = exp[2] # trial number of current experiment\n",
    "\n",
    "# shortest path problem data generation parameters - https://arxiv.org/pdf/2402.03256\n",
    "grid = (5, 5)  # grid size\n",
    "num_feat = 5  # size of feature\n",
    "deg = 6  # polynomial degree in edge cost function\n",
    "e = .3  # noise width/amount of noise\n",
    "\n",
    "# path planting for shortest path example - see page 9, subsection \"Harder Example with Planted Arcs\" in section 4.2 of paper https://arxiv.org/pdf/2402.03256\n",
    "planted_good_pwl_params = {'slope0':0, # slope of first segment of piecewise linear cost function for \"good\" edge cost planted\n",
    "                    'int0':2, # intercept of first segment of piecewise linear cost function for \"good\" edge cost planted\n",
    "                    'slope1':0, # slope of second segment of piecewise linear cost function for \"good\" edge cost planted\n",
    "                    'int1':2} # intercept of second segment of piecewise linear cost function for \"good\" edge cost planted\n",
    "planted_bad_pwl_params = {'slope0':4, # slope of first segment of piecewise linear cost function for \"bad\" edge cost planted\n",
    "                    'int0':0, # intercept of first segment of piecewise linear cost function for \"bad\" edge cost planted\n",
    "                    'slope1':0, # slope of second segment of piecewise linear cost function for \"bad\" edge cost planted\n",
    "                    'int1':2.2} # intercept of second segment of piecewise linear cost function for \"bad\" edge cost planted\n",
    "plant_edge = True # to plant edges in shortest path experiment or not\n",
    "\n",
    "print(f'current experiment setting: number of data points {num_data}, epsilon type {ep_type}, trial number {trial}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59553dc-1f26-4848-97f1-5de3006afd70",
   "metadata": {},
   "source": [
    "### Calling `genData` from `decision_learning.data.shortest_path_grid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6208b7ef-6b2e-4a04-b68f-323ca537e207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------DATA------------\n",
    "# training data\n",
    "generated_data = genData(num_data=num_data+200, # number of data points to generate for training set\n",
    "        num_features=num_feat, # number of features \n",
    "        grid=grid, # grid shape\n",
    "        deg=deg, # polynomial degree\n",
    "        noise_type=ep_type, # epsilon noise type\n",
    "        noise_width=e, # amount of noise\n",
    "        seed=indices_arr[trial], # seed the randomness\n",
    "        plant_edges=plant_edge, # to plant edges or not\n",
    "        planted_good_pwl_params=planted_good_pwl_params, # cost function for good edges\n",
    "        planted_bad_pwl_params=planted_bad_pwl_params) # cost function for bad edges\n",
    "\n",
    "# testing data\n",
    "generated_data_test = genData(num_data=10000, # number of data points to generate for test set\n",
    "        num_features=num_feat, # number of features \n",
    "        grid=grid,  # grid shape\n",
    "        deg=deg,  # polynomial degree\n",
    "        noise_type=ep_type,  # epsilon noise type\n",
    "        noise_width=e, # amount of noise\n",
    "        seed=indices_arr_test[trial],      # seed the randomness\n",
    "        plant_edges=plant_edge, # to plant edges or not\n",
    "        planted_good_pwl_params=planted_good_pwl_params, # cost function for good edges\n",
    "        planted_bad_pwl_params=planted_bad_pwl_params) # cost function for bad edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b2dd8-1cbb-48a2-b648-d50e2255ada6",
   "metadata": {},
   "source": [
    "## Prediction Model\n",
    "- Any decision-aware/focused problem will of course need prediction model to predict the cost/coefficient vector given contextual input/features. This example uses a simple `LinearRegression` object implemented within `decision_learning.modeling.models`. \n",
    "- The package expects the prediction model to be a PyTorch model since PyTorch offers convenient autograd functionality/allows user to specify custom losses/backwards passes that are found within many decision-aware/focused works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b32a03bb-f163-4f38-b81b-8574a61253a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------prediction model------------\n",
    "pred_model = LinearRegression(input_dim=generated_data['feat'].shape[1],\n",
    "                 output_dim=generated_data['cost'].shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972c6cc7-c367-4294-960c-2149d0e5ba08",
   "metadata": {},
   "source": [
    "# Pipeline Function Overview\n",
    "Pipeline function `lossfn_experiment_pipeline` from `decision_learning.modeling.pipeline` takes in the following arguments:\n",
    "- X_train: training set features\n",
    "- true_cost_train: training set true costs\n",
    "- X_test: test set features\n",
    "- true_cost_test: test set true costs\n",
    "- predmodel: pytorch prediction model\n",
    "- optmodel: optimization model\n",
    "- val_split_params: how to split training data into train/val splits. Defaults to {'test_size':0.2, 'random_state':42}.\n",
    "- loss_names: list of loss functions to run experiment pipeline on that are implemented already in the codebase in decision_learning.modeling.loss\n",
    "- loss_configs: dictionary mapping from loss_name (key) to a dictionary of different hyperparameters that are then grid searched over.\n",
    "- custom_loss_inputs:list of custom loss function configurations to run through the train function as part of experient pipeline\n",
    "- minimize: minimization problem?\n",
    "- training_configs: parameters to be passed into train function for pytorch training loop. \n",
    "- save_models: flag to save models or not.\n",
    "\n",
    "Note when running pipeline function, we turn off training loop logging/console output for each experiment setting, however, this can still be turned on by setting `training_loop_verbose=True` flag on when calling pipeline function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb45d5-9dcc-4351-bad3-1dcaf469cefa",
   "metadata": {},
   "source": [
    "# Example: Off-the-Shelf Preimplemented Loss Functions\n",
    "Here since we only use off-the-shelf preimplemented loss functions, without any hyperparameter searching, we only need to specify the individual loss names `['SPO+', 'MSE']` to `loss_name` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad0dd576-08a8-4205-bd5f-96603ce39242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 09:21:47,382 - decision_learning.modeling.pipeline - INFO - Loss number 1/2, on loss function SPO+\n",
      "2024-12-03 09:21:47,383 - decision_learning.modeling.pipeline - INFO - Trial 1/1 for running loss function SPO+, current hyperparameters: {}\n",
      "/home1/yongpeng/.conda/envs/pyepo_dsl/lib/python3.11/site-packages/torch/autograd/__init__.py:266: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "2024-12-03 09:21:56,089 - decision_learning.modeling.pipeline - INFO - Loss number 2/2, on loss function MSE\n",
      "2024-12-03 09:21:56,091 - decision_learning.modeling.pipeline - INFO - Trial 1/1 for running loss function MSE, current hyperparameters: {}\n"
     ]
    }
   ],
   "source": [
    "preimplement_loss_results, preimplement_loss_models = lossfn_experiment_pipeline(X_train=generated_data['feat'],\n",
    "                true_cost_train=generated_data['cost'],\n",
    "                X_test=generated_data_test['feat'],\n",
    "                true_cost_test=generated_data_test['cost_true'], \n",
    "                predmodel=pred_model,\n",
    "                optmodel=optmodel,\n",
    "                val_split_params={'test_size':200, 'random_state':42},\n",
    "                loss_names=['SPO+', 'MSE'],                            \n",
    "                training_configs={'num_epochs':100,\n",
    "                                 'dataloader_params': {'batch_size':200, 'shuffle':True}},\n",
    "                save_models=True                                                                                 \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97631bdd-60fc-481c-8089-7b3a229917ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_regret</th>\n",
       "      <th>loss_name</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>4.306511</td>\n",
       "      <td>0.211430</td>\n",
       "      <td>0.044372</td>\n",
       "      <td>SPO+</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>99</td>\n",
       "      <td>2.063001</td>\n",
       "      <td>0.465148</td>\n",
       "      <td>0.236546</td>\n",
       "      <td>MSE</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  train_loss  val_metric  test_regret loss_name hyperparameters\n",
       "99      99    4.306511    0.211430     0.044372      SPO+              {}\n",
       "199     99    2.063001    0.465148     0.236546       MSE              {}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preimplement_loss_results[preimplement_loss_results.epoch == 99]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92815220-580f-4857-acf5-bbea917bdd06",
   "metadata": {},
   "source": [
    "### Saved Down Models\n",
    "models saved as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e77adca-8885-49ea-938f-02d2cd196bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SPO+_{}': LinearRegression(\n",
       "   (linear): Linear(in_features=6, out_features=40, bias=True)\n",
       " ),\n",
       " 'MSE_{}': LinearRegression(\n",
       "   (linear): Linear(in_features=6, out_features=40, bias=True)\n",
       " )}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preimplement_loss_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1b038b-0739-4224-9335-130c5722ed13",
   "metadata": {},
   "source": [
    "# Providing Hyperparameter Search Example\n",
    "Here since we we are still using off-the-shelf preimplemented loss functions, but now since PG loss accepts two arguments ('h': width size, and 'finite_diff_type': finite different scheme}, we can search over the hyperparameters by inputting them into the `loss_configs` argument in the exaxmple below as: `{'PG': {'h':[num_data**-.125, num_data**-.25, num_data**-.5, num_data**-1], 'finite_diff_type': ['B', 'C', 'F']}}`. The pipeline function will use a helper function `lossfn_hyperparam_grid` to take the cartesian product of the `h` and `finite_diff_type` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c32ab754-a6e5-4b09-a55b-2d74054656f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 09:22:13,274 - decision_learning.modeling.pipeline - INFO - Loss number 1/1, on loss function PG\n",
      "2024-12-03 09:22:13,275 - decision_learning.modeling.pipeline - INFO - Trial 1/12 for running loss function PG, current hyperparameters: {'h': 0.5156692688606229, 'finite_diff_type': 'B'}\n",
      "2024-12-03 09:22:16,813 - decision_learning.modeling.pipeline - INFO - Trial 2/12 for running loss function PG, current hyperparameters: {'h': 0.5156692688606229, 'finite_diff_type': 'C'}\n",
      "2024-12-03 09:22:20,009 - decision_learning.modeling.pipeline - INFO - Trial 3/12 for running loss function PG, current hyperparameters: {'h': 0.5156692688606229, 'finite_diff_type': 'F'}\n",
      "2024-12-03 09:22:25,879 - decision_learning.modeling.pipeline - INFO - Trial 4/12 for running loss function PG, current hyperparameters: {'h': 0.26591479484724945, 'finite_diff_type': 'B'}\n",
      "2024-12-03 09:22:29,162 - decision_learning.modeling.pipeline - INFO - Trial 5/12 for running loss function PG, current hyperparameters: {'h': 0.26591479484724945, 'finite_diff_type': 'C'}\n",
      "2024-12-03 09:22:32,534 - decision_learning.modeling.pipeline - INFO - Trial 6/12 for running loss function PG, current hyperparameters: {'h': 0.26591479484724945, 'finite_diff_type': 'F'}\n",
      "2024-12-03 09:22:35,705 - decision_learning.modeling.pipeline - INFO - Trial 7/12 for running loss function PG, current hyperparameters: {'h': 0.07071067811865475, 'finite_diff_type': 'B'}\n",
      "2024-12-03 09:22:39,098 - decision_learning.modeling.pipeline - INFO - Trial 8/12 for running loss function PG, current hyperparameters: {'h': 0.07071067811865475, 'finite_diff_type': 'C'}\n",
      "2024-12-03 09:22:42,397 - decision_learning.modeling.pipeline - INFO - Trial 9/12 for running loss function PG, current hyperparameters: {'h': 0.07071067811865475, 'finite_diff_type': 'F'}\n",
      "2024-12-03 09:22:45,558 - decision_learning.modeling.pipeline - INFO - Trial 10/12 for running loss function PG, current hyperparameters: {'h': 0.005, 'finite_diff_type': 'B'}\n",
      "2024-12-03 09:22:48,727 - decision_learning.modeling.pipeline - INFO - Trial 11/12 for running loss function PG, current hyperparameters: {'h': 0.005, 'finite_diff_type': 'C'}\n",
      "2024-12-03 09:22:54,287 - decision_learning.modeling.pipeline - INFO - Trial 12/12 for running loss function PG, current hyperparameters: {'h': 0.005, 'finite_diff_type': 'F'}\n"
     ]
    }
   ],
   "source": [
    "PG_results, PG_models = lossfn_experiment_pipeline(X_train=generated_data['feat'],\n",
    "                true_cost_train=generated_data['cost'],\n",
    "                X_test=generated_data_test['feat'],\n",
    "                true_cost_test=generated_data_test['cost_true'], \n",
    "                predmodel=pred_model,\n",
    "                optmodel=optmodel,\n",
    "                val_split_params={'test_size':200, 'random_state':42},\n",
    "                loss_names=['PG'],\n",
    "                loss_configs={'PG': {'h':[num_data**-.125, num_data**-.25, num_data**-.5, num_data**-1], 'finite_diff_type': ['B', 'C', 'F']}},\n",
    "                training_configs={'num_epochs':100,\n",
    "                                 'dataloader_params': {'batch_size':200, 'shuffle':True}},\n",
    "                save_models=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a4ad715-d010-4c06-af95-64e568ea1094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_regret</th>\n",
       "      <th>loss_name</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>15.553047</td>\n",
       "      <td>0.153668</td>\n",
       "      <td>0.110702</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.5156692688606229, 'finite_diff_type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>99</td>\n",
       "      <td>15.179111</td>\n",
       "      <td>0.174463</td>\n",
       "      <td>0.104197</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.5156692688606229, 'finite_diff_type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>99</td>\n",
       "      <td>14.057665</td>\n",
       "      <td>0.221629</td>\n",
       "      <td>0.042703</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.5156692688606229, 'finite_diff_type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>99</td>\n",
       "      <td>15.498624</td>\n",
       "      <td>0.192540</td>\n",
       "      <td>0.111207</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.26591479484724945, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>99</td>\n",
       "      <td>15.176310</td>\n",
       "      <td>0.159259</td>\n",
       "      <td>0.101614</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.26591479484724945, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>99</td>\n",
       "      <td>14.663220</td>\n",
       "      <td>0.192385</td>\n",
       "      <td>0.075795</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.26591479484724945, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>99</td>\n",
       "      <td>15.297663</td>\n",
       "      <td>0.195223</td>\n",
       "      <td>0.109455</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.07071067811865475, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>99</td>\n",
       "      <td>15.304697</td>\n",
       "      <td>0.184279</td>\n",
       "      <td>0.109064</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.07071067811865475, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>99</td>\n",
       "      <td>15.325191</td>\n",
       "      <td>0.194410</td>\n",
       "      <td>0.114124</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.07071067811865475, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>99</td>\n",
       "      <td>15.940586</td>\n",
       "      <td>0.226152</td>\n",
       "      <td>0.153410</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.005, 'finite_diff_type': 'B'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>99</td>\n",
       "      <td>16.016407</td>\n",
       "      <td>0.231691</td>\n",
       "      <td>0.153148</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.005, 'finite_diff_type': 'C'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>99</td>\n",
       "      <td>16.335396</td>\n",
       "      <td>0.235089</td>\n",
       "      <td>0.182718</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.005, 'finite_diff_type': 'F'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      epoch  train_loss  val_metric  test_regret loss_name  \\\n",
       "99       99   15.553047    0.153668     0.110702        PG   \n",
       "199      99   15.179111    0.174463     0.104197        PG   \n",
       "299      99   14.057665    0.221629     0.042703        PG   \n",
       "399      99   15.498624    0.192540     0.111207        PG   \n",
       "499      99   15.176310    0.159259     0.101614        PG   \n",
       "599      99   14.663220    0.192385     0.075795        PG   \n",
       "699      99   15.297663    0.195223     0.109455        PG   \n",
       "799      99   15.304697    0.184279     0.109064        PG   \n",
       "899      99   15.325191    0.194410     0.114124        PG   \n",
       "999      99   15.940586    0.226152     0.153410        PG   \n",
       "1099     99   16.016407    0.231691     0.153148        PG   \n",
       "1199     99   16.335396    0.235089     0.182718        PG   \n",
       "\n",
       "                                        hyperparameters  \n",
       "99    {'h': 0.5156692688606229, 'finite_diff_type': ...  \n",
       "199   {'h': 0.5156692688606229, 'finite_diff_type': ...  \n",
       "299   {'h': 0.5156692688606229, 'finite_diff_type': ...  \n",
       "399   {'h': 0.26591479484724945, 'finite_diff_type':...  \n",
       "499   {'h': 0.26591479484724945, 'finite_diff_type':...  \n",
       "599   {'h': 0.26591479484724945, 'finite_diff_type':...  \n",
       "699   {'h': 0.07071067811865475, 'finite_diff_type':...  \n",
       "799   {'h': 0.07071067811865475, 'finite_diff_type':...  \n",
       "899   {'h': 0.07071067811865475, 'finite_diff_type':...  \n",
       "999               {'h': 0.005, 'finite_diff_type': 'B'}  \n",
       "1099              {'h': 0.005, 'finite_diff_type': 'C'}  \n",
       "1199              {'h': 0.005, 'finite_diff_type': 'F'}  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PG_results[PG_results.epoch == 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f1246a9-41ef-4867-866b-707b61c9ad07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PG_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28068f",
   "metadata": {},
   "source": [
    "### Additional Parameter Tuning Example - CosineSurrogateDotProdVecMag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e535be13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_55084/3555705404.py:18: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_l[:, :num_nodes_next - 1] = V_1 + l_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_55084/3555705404.py:19: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_r[:, 1:num_nodes_next] = V_1 + r_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_55084/3555705404.py:21: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_l = V_1[:, :num_nodes_next] + l_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_55084/3555705404.py:22: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  next_V_val_r = V_1[:, 1:num_nodes_next + 1] + r_costs\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_55084/3555705404.py:44: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  l_cs_res = ((V_2[:, :num_nodes_next - 1] - V_1 + layer_costs[:, ::2]) < sens) * prev_act\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_55084/3555705404.py:45: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  r_cs_res = ((V_2[:, 1:num_nodes_next] - V_1 + layer_costs[:, 1::2]) < sens) * prev_act\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_55084/3555705404.py:50: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  l_cs_res = ((V_2 - V_1[:, :num_nodes - 1] + layer_costs[:, ::2]) < sens) * prev_act[:, :num_nodes - 1]\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_55084/3555705404.py:51: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  r_cs_res = ((V_2 - V_1[:, 1:num_nodes] + layer_costs[:, 1::2]) < sens) * prev_act[:, 1:num_nodes]\n",
      "/var/folders/k9/q1b_kr4s353_ckw475n0fxw40000gn/T/ipykernel_55084/3555705404.py:63: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  obj = torch.sum(sol * costs, axis=1)\n",
      "2025-01-22 08:56:23,717 - decision_learning.modeling.pipeline - INFO - Loss number 1/1, on loss function CosineSurrogateDotProdVecMag\n",
      "2025-01-22 08:56:23,718 - decision_learning.modeling.pipeline - INFO - Trial 1/7 for running loss function CosineSurrogateDotProdVecMag, current hyperparameters: {'alpha': 0.01}\n",
      "2025-01-22 08:56:23,719 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-01-22 08:56:24,278 - decision_learning.utils - INFO - Function 'train' took 0.5596330165863037 seconds to run.\n",
      "2025-01-22 08:56:24,279 - decision_learning.modeling.pipeline - INFO - Trial 2/7 for running loss function CosineSurrogateDotProdVecMag, current hyperparameters: {'alpha': 0.1}\n",
      "2025-01-22 08:56:24,279 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-01-22 08:56:24,873 - decision_learning.utils - INFO - Function 'train' took 0.593665599822998 seconds to run.\n",
      "2025-01-22 08:56:24,873 - decision_learning.modeling.pipeline - INFO - Trial 3/7 for running loss function CosineSurrogateDotProdVecMag, current hyperparameters: {'alpha': 1}\n",
      "2025-01-22 08:56:24,874 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-01-22 08:56:25,416 - decision_learning.utils - INFO - Function 'train' took 0.5420498847961426 seconds to run.\n",
      "2025-01-22 08:56:25,417 - decision_learning.modeling.pipeline - INFO - Trial 4/7 for running loss function CosineSurrogateDotProdVecMag, current hyperparameters: {'alpha': 2.5}\n",
      "2025-01-22 08:56:25,417 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-01-22 08:56:25,979 - decision_learning.utils - INFO - Function 'train' took 0.5616858005523682 seconds to run.\n",
      "2025-01-22 08:56:25,979 - decision_learning.modeling.pipeline - INFO - Trial 5/7 for running loss function CosineSurrogateDotProdVecMag, current hyperparameters: {'alpha': 5}\n",
      "2025-01-22 08:56:25,980 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-01-22 08:56:26,507 - decision_learning.utils - INFO - Function 'train' took 0.5274238586425781 seconds to run.\n",
      "2025-01-22 08:56:26,508 - decision_learning.modeling.pipeline - INFO - Trial 6/7 for running loss function CosineSurrogateDotProdVecMag, current hyperparameters: {'alpha': 7.5}\n",
      "2025-01-22 08:56:26,508 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-01-22 08:56:27,021 - decision_learning.utils - INFO - Function 'train' took 0.5129969120025635 seconds to run.\n",
      "2025-01-22 08:56:27,022 - decision_learning.modeling.pipeline - INFO - Trial 7/7 for running loss function CosineSurrogateDotProdVecMag, current hyperparameters: {'alpha': 10}\n",
      "2025-01-22 08:56:27,022 - decision_learning.modeling.train - INFO - Training on device: cpu\n",
      "2025-01-22 08:56:27,534 - decision_learning.utils - INFO - Function 'train' took 0.5113999843597412 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "cos_surr_results, cos_surr_models = lossfn_experiment_pipeline(X_train=generated_data['feat'],\n",
    "                true_cost_train=generated_data['cost'],\n",
    "                X_test=generated_data_test['feat'],\n",
    "                true_cost_test=generated_data_test['cost_true'], \n",
    "                predmodel=pred_model,\n",
    "                optmodel=optmodel,\n",
    "                val_split_params={'test_size':200, 'random_state':42},\n",
    "                loss_names=['CosineSurrogateDotProdVecMag'],\n",
    "                loss_configs={'CosineSurrogateDotProdVecMag': {'alpha':[0.01, 0.1, 1, 2.5, 5, 7.5, 10]}},\n",
    "                training_configs={'num_epochs':100,\n",
    "                                 'dataloader_params': {'batch_size':200, 'shuffle':True}},\n",
    "                save_models=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d44eee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_regret</th>\n",
       "      <th>loss_name</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>-262.287231</td>\n",
       "      <td>0.391855</td>\n",
       "      <td>0.250060</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>99</td>\n",
       "      <td>-227.810394</td>\n",
       "      <td>0.478831</td>\n",
       "      <td>0.267152</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>99</td>\n",
       "      <td>-73.140350</td>\n",
       "      <td>0.299098</td>\n",
       "      <td>0.129629</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>99</td>\n",
       "      <td>-30.240700</td>\n",
       "      <td>0.258759</td>\n",
       "      <td>0.075556</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 2.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>99</td>\n",
       "      <td>-15.257957</td>\n",
       "      <td>0.204101</td>\n",
       "      <td>0.068355</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>99</td>\n",
       "      <td>-10.171493</td>\n",
       "      <td>0.242172</td>\n",
       "      <td>0.106509</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 7.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>99</td>\n",
       "      <td>-7.603749</td>\n",
       "      <td>0.249935</td>\n",
       "      <td>0.125792</td>\n",
       "      <td>CosineSurrogateDotProdVecMag</td>\n",
       "      <td>{'alpha': 10}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  train_loss  val_metric  test_regret                     loss_name  \\\n",
       "99      99 -262.287231    0.391855     0.250060  CosineSurrogateDotProdVecMag   \n",
       "199     99 -227.810394    0.478831     0.267152  CosineSurrogateDotProdVecMag   \n",
       "299     99  -73.140350    0.299098     0.129629  CosineSurrogateDotProdVecMag   \n",
       "399     99  -30.240700    0.258759     0.075556  CosineSurrogateDotProdVecMag   \n",
       "499     99  -15.257957    0.204101     0.068355  CosineSurrogateDotProdVecMag   \n",
       "599     99  -10.171493    0.242172     0.106509  CosineSurrogateDotProdVecMag   \n",
       "699     99   -7.603749    0.249935     0.125792  CosineSurrogateDotProdVecMag   \n",
       "\n",
       "     hyperparameters  \n",
       "99   {'alpha': 0.01}  \n",
       "199   {'alpha': 0.1}  \n",
       "299     {'alpha': 1}  \n",
       "399   {'alpha': 2.5}  \n",
       "499     {'alpha': 5}  \n",
       "599   {'alpha': 7.5}  \n",
       "699    {'alpha': 10}  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_surr_results[cos_surr_results.epoch == 99]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc086f-aef8-461b-b555-9e95143f6dc3",
   "metadata": {},
   "source": [
    "# Specific Model Initialization Example\n",
    "Here we will use the pre-trained `SPO+` model as initialization point for PG loss example from above and observe the improvement in test_regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2440fac9-24fd-47e2-bf20-97599580c78c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 09:24:00,783 - decision_learning.modeling.pipeline - INFO - Loss number 1/1, on loss function PG\n",
      "2024-12-03 09:24:00,785 - decision_learning.modeling.pipeline - INFO - Trial 1/12 for running loss function PG, current hyperparameters: {'h': 0.5156692688606229, 'finite_diff_type': 'B'}\n",
      "2024-12-03 09:24:04,144 - decision_learning.modeling.pipeline - INFO - Trial 2/12 for running loss function PG, current hyperparameters: {'h': 0.5156692688606229, 'finite_diff_type': 'C'}\n",
      "2024-12-03 09:24:07,373 - decision_learning.modeling.pipeline - INFO - Trial 3/12 for running loss function PG, current hyperparameters: {'h': 0.5156692688606229, 'finite_diff_type': 'F'}\n",
      "2024-12-03 09:24:10,686 - decision_learning.modeling.pipeline - INFO - Trial 4/12 for running loss function PG, current hyperparameters: {'h': 0.26591479484724945, 'finite_diff_type': 'B'}\n",
      "2024-12-03 09:24:13,931 - decision_learning.modeling.pipeline - INFO - Trial 5/12 for running loss function PG, current hyperparameters: {'h': 0.26591479484724945, 'finite_diff_type': 'C'}\n",
      "2024-12-03 09:24:17,309 - decision_learning.modeling.pipeline - INFO - Trial 6/12 for running loss function PG, current hyperparameters: {'h': 0.26591479484724945, 'finite_diff_type': 'F'}\n",
      "2024-12-03 09:24:20,725 - decision_learning.modeling.pipeline - INFO - Trial 7/12 for running loss function PG, current hyperparameters: {'h': 0.07071067811865475, 'finite_diff_type': 'B'}\n",
      "2024-12-03 09:24:23,896 - decision_learning.modeling.pipeline - INFO - Trial 8/12 for running loss function PG, current hyperparameters: {'h': 0.07071067811865475, 'finite_diff_type': 'C'}\n",
      "2024-12-03 09:24:27,385 - decision_learning.modeling.pipeline - INFO - Trial 9/12 for running loss function PG, current hyperparameters: {'h': 0.07071067811865475, 'finite_diff_type': 'F'}\n",
      "2024-12-03 09:24:30,615 - decision_learning.modeling.pipeline - INFO - Trial 10/12 for running loss function PG, current hyperparameters: {'h': 0.005, 'finite_diff_type': 'B'}\n",
      "2024-12-03 09:24:35,829 - decision_learning.modeling.pipeline - INFO - Trial 11/12 for running loss function PG, current hyperparameters: {'h': 0.005, 'finite_diff_type': 'C'}\n",
      "2024-12-03 09:24:39,166 - decision_learning.modeling.pipeline - INFO - Trial 12/12 for running loss function PG, current hyperparameters: {'h': 0.005, 'finite_diff_type': 'F'}\n"
     ]
    }
   ],
   "source": [
    "PG_init_results, PG_init_models = lossfn_experiment_pipeline(X_train=generated_data['feat'],\n",
    "                true_cost_train=generated_data['cost'],\n",
    "                X_test=generated_data_test['feat'],\n",
    "                true_cost_test=generated_data_test['cost_true'], \n",
    "                predmodel=preimplement_loss_models['SPO+_{}'],\n",
    "                optmodel=optmodel,\n",
    "                val_split_params={'test_size':200, 'random_state':42},\n",
    "                loss_names=['PG'],\n",
    "                loss_configs={'PG': {'h':[num_data**-.125, num_data**-.25, num_data**-.5, num_data**-1], 'finite_diff_type': ['B', 'C', 'F']}},\n",
    "                training_configs={'num_epochs':100,\n",
    "                                 'dataloader_params': {'batch_size':200, 'shuffle':True}},\n",
    "                save_models=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2619437-1ce8-49c0-bc00-f884f32cd81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_regret</th>\n",
       "      <th>loss_name</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>14.572644</td>\n",
       "      <td>0.217139</td>\n",
       "      <td>0.030941</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.5156692688606229, 'finite_diff_type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>99</td>\n",
       "      <td>14.229800</td>\n",
       "      <td>0.175496</td>\n",
       "      <td>0.015016</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.5156692688606229, 'finite_diff_type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>99</td>\n",
       "      <td>13.712555</td>\n",
       "      <td>0.232014</td>\n",
       "      <td>0.083302</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.5156692688606229, 'finite_diff_type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>99</td>\n",
       "      <td>14.227107</td>\n",
       "      <td>0.192693</td>\n",
       "      <td>0.013983</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.26591479484724945, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>99</td>\n",
       "      <td>14.104239</td>\n",
       "      <td>0.209925</td>\n",
       "      <td>0.007053</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.26591479484724945, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>99</td>\n",
       "      <td>14.000607</td>\n",
       "      <td>0.212039</td>\n",
       "      <td>0.052660</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.26591479484724945, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>99</td>\n",
       "      <td>14.259412</td>\n",
       "      <td>0.223006</td>\n",
       "      <td>0.022180</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.07071067811865475, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>99</td>\n",
       "      <td>14.149658</td>\n",
       "      <td>0.220004</td>\n",
       "      <td>0.018171</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.07071067811865475, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>99</td>\n",
       "      <td>14.182384</td>\n",
       "      <td>0.188789</td>\n",
       "      <td>0.024017</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.07071067811865475, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>99</td>\n",
       "      <td>14.507380</td>\n",
       "      <td>0.227987</td>\n",
       "      <td>0.039757</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.005, 'finite_diff_type': 'B'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>99</td>\n",
       "      <td>14.300655</td>\n",
       "      <td>0.197865</td>\n",
       "      <td>0.030548</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.005, 'finite_diff_type': 'C'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>99</td>\n",
       "      <td>14.314173</td>\n",
       "      <td>0.215485</td>\n",
       "      <td>0.028654</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.005, 'finite_diff_type': 'F'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      epoch  train_loss  val_metric  test_regret loss_name  \\\n",
       "99       99   14.572644    0.217139     0.030941        PG   \n",
       "199      99   14.229800    0.175496     0.015016        PG   \n",
       "299      99   13.712555    0.232014     0.083302        PG   \n",
       "399      99   14.227107    0.192693     0.013983        PG   \n",
       "499      99   14.104239    0.209925     0.007053        PG   \n",
       "599      99   14.000607    0.212039     0.052660        PG   \n",
       "699      99   14.259412    0.223006     0.022180        PG   \n",
       "799      99   14.149658    0.220004     0.018171        PG   \n",
       "899      99   14.182384    0.188789     0.024017        PG   \n",
       "999      99   14.507380    0.227987     0.039757        PG   \n",
       "1099     99   14.300655    0.197865     0.030548        PG   \n",
       "1199     99   14.314173    0.215485     0.028654        PG   \n",
       "\n",
       "                                        hyperparameters  \n",
       "99    {'h': 0.5156692688606229, 'finite_diff_type': ...  \n",
       "199   {'h': 0.5156692688606229, 'finite_diff_type': ...  \n",
       "299   {'h': 0.5156692688606229, 'finite_diff_type': ...  \n",
       "399   {'h': 0.26591479484724945, 'finite_diff_type':...  \n",
       "499   {'h': 0.26591479484724945, 'finite_diff_type':...  \n",
       "599   {'h': 0.26591479484724945, 'finite_diff_type':...  \n",
       "699   {'h': 0.07071067811865475, 'finite_diff_type':...  \n",
       "799   {'h': 0.07071067811865475, 'finite_diff_type':...  \n",
       "899   {'h': 0.07071067811865475, 'finite_diff_type':...  \n",
       "999               {'h': 0.005, 'finite_diff_type': 'B'}  \n",
       "1099              {'h': 0.005, 'finite_diff_type': 'C'}  \n",
       "1199              {'h': 0.005, 'finite_diff_type': 'F'}  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PG_init_results[PG_init_results.epoch == 99]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d75eb3-683d-4d11-b355-40333a48cd5d",
   "metadata": {},
   "source": [
    "# Custom Loss Function Example\n",
    "Simple example using `nn.CosineEmbeddingLoss`, which takes different input arguments then our existing decision-aware loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c95d9777-39f3-405d-970c-a4bb116cbd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------custom loss function inputs------------\n",
    "# every dictionary key is necessary for custom loss inputs \n",
    "custom_loss_inputs = [{'loss_name':'cosine', # name of loss function - used just for final metric logging purposes\n",
    "                      'loss':nn.CosineEmbeddingLoss, # callable function \n",
    "                      'data': {'X': generated_data['feat'], # data input for loss function, X is the feature name, it must be X for pipeline function\n",
    "                               'input2':generated_data['cost'], # remaining is whatever labels, arguments, the loss function needs. input2, target are arguments used by nn.CosineEmbeddingLoss\n",
    "                               'target':torch.ones(generated_data['cost'].shape[0])}\n",
    "                      }\n",
    "                     ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e248af6-acf6-4673-a3e1-abdffe4c4d1d",
   "metadata": {},
   "source": [
    "Here we also set `training_loop_verbose=True` to showcase console/logging output when we allow for training loop outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "976df299-bfe3-46fe-ad60-a5112d269c4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 09:25:35,779 - decision_learning.modeling.pipeline - INFO - Trial 1/1 for custom loss functions, current loss function: cosine\n",
      "Training Loader: Epoch 1/100: 100%|██████████| 2/2 [00:00<00:00, 235.32it/s]\n",
      "Validation Loader: Epoch 1/100: 100%|██████████| 1/1 [00:00<00:00, 300.56it/s]\n",
      "2024-12-03 09:25:35,817 - decision_learning.modeling.train - INFO - epoch: 1, train_loss: 0.8279918730258942, val_metric: 0.532266186916295, test_regret: 0.4757916220789828\n",
      "Training Loader: Epoch 2/100: 100%|██████████| 2/2 [00:00<00:00, 273.27it/s]\n",
      "Validation Loader: Epoch 2/100: 100%|██████████| 1/1 [00:00<00:00, 324.64it/s]\n",
      "2024-12-03 09:25:35,852 - decision_learning.modeling.train - INFO - epoch: 2, train_loss: 0.7577745318412781, val_metric: 0.4951736106335352, test_regret: 0.4733026644490666\n",
      "Training Loader: Epoch 3/100: 100%|██████████| 2/2 [00:00<00:00, 275.06it/s]\n",
      "Validation Loader: Epoch 3/100: 100%|██████████| 1/1 [00:00<00:00, 321.60it/s]\n",
      "2024-12-03 09:25:35,887 - decision_learning.modeling.train - INFO - epoch: 3, train_loss: 0.6903280019760132, val_metric: 0.5115892234321541, test_regret: 0.4717104517265496\n",
      "Training Loader: Epoch 4/100: 100%|██████████| 2/2 [00:00<00:00, 290.21it/s]\n",
      "Validation Loader: Epoch 4/100: 100%|██████████| 1/1 [00:00<00:00, 324.84it/s]\n",
      "2024-12-03 09:25:35,922 - decision_learning.modeling.train - INFO - epoch: 4, train_loss: 0.6265562474727631, val_metric: 0.5413382042960864, test_regret: 0.4699671836398448\n",
      "Training Loader: Epoch 5/100: 100%|██████████| 2/2 [00:00<00:00, 277.41it/s]\n",
      "Validation Loader: Epoch 5/100: 100%|██████████| 1/1 [00:00<00:00, 324.64it/s]\n",
      "2024-12-03 09:25:35,957 - decision_learning.modeling.train - INFO - epoch: 5, train_loss: 0.5660980045795441, val_metric: 0.5260329457297309, test_regret: 0.4673274493456749\n",
      "Training Loader: Epoch 6/100: 100%|██████████| 2/2 [00:00<00:00, 275.27it/s]\n",
      "Validation Loader: Epoch 6/100: 100%|██████████| 1/1 [00:00<00:00, 333.15it/s]\n",
      "2024-12-03 09:25:35,992 - decision_learning.modeling.train - INFO - epoch: 6, train_loss: 0.5095335245132446, val_metric: 0.5422049039213704, test_regret: 0.46499679327822846\n",
      "Training Loader: Epoch 7/100: 100%|██████████| 2/2 [00:00<00:00, 279.30it/s]\n",
      "Validation Loader: Epoch 7/100: 100%|██████████| 1/1 [00:00<00:00, 299.87it/s]\n",
      "2024-12-03 09:25:36,027 - decision_learning.modeling.train - INFO - epoch: 7, train_loss: 0.4566628038883209, val_metric: 0.5339501388878981, test_regret: 0.4629526128728642\n",
      "Training Loader: Epoch 8/100: 100%|██████████| 2/2 [00:00<00:00, 274.25it/s]\n",
      "Validation Loader: Epoch 8/100: 100%|██████████| 1/1 [00:00<00:00, 328.66it/s]\n",
      "2024-12-03 09:25:36,063 - decision_learning.modeling.train - INFO - epoch: 8, train_loss: 0.40820594131946564, val_metric: 0.5374729900429881, test_regret: 0.4619363335638045\n",
      "Training Loader: Epoch 9/100: 100%|██████████| 2/2 [00:00<00:00, 277.72it/s]\n",
      "Validation Loader: Epoch 9/100: 100%|██████████| 1/1 [00:00<00:00, 309.25it/s]\n",
      "2024-12-03 09:25:36,098 - decision_learning.modeling.train - INFO - epoch: 9, train_loss: 0.36332476139068604, val_metric: 0.5544754578476495, test_regret: 0.45913439590808985\n",
      "Training Loader: Epoch 10/100: 100%|██████████| 2/2 [00:00<00:00, 274.63it/s]\n",
      "Validation Loader: Epoch 10/100: 100%|██████████| 1/1 [00:00<00:00, 319.42it/s]\n",
      "2024-12-03 09:25:36,133 - decision_learning.modeling.train - INFO - epoch: 10, train_loss: 0.32265712320804596, val_metric: 0.5378876866487957, test_regret: 0.4538973175797001\n",
      "Training Loader: Epoch 11/100: 100%|██████████| 2/2 [00:00<00:00, 278.69it/s]\n",
      "Validation Loader: Epoch 11/100: 100%|██████████| 1/1 [00:00<00:00, 332.04it/s]\n",
      "2024-12-03 09:25:36,168 - decision_learning.modeling.train - INFO - epoch: 11, train_loss: 0.28566884994506836, val_metric: 0.5425275439278631, test_regret: 0.4482736267029688\n",
      "Training Loader: Epoch 12/100: 100%|██████████| 2/2 [00:00<00:00, 276.25it/s]\n",
      "Validation Loader: Epoch 12/100: 100%|██████████| 1/1 [00:00<00:00, 326.76it/s]\n",
      "2024-12-03 09:25:36,205 - decision_learning.modeling.train - INFO - epoch: 12, train_loss: 0.25217023491859436, val_metric: 0.5327756380914834, test_regret: 0.43965318691670874\n",
      "Training Loader: Epoch 13/100: 100%|██████████| 2/2 [00:00<00:00, 276.17it/s]\n",
      "Validation Loader: Epoch 13/100: 100%|██████████| 1/1 [00:00<00:00, 320.25it/s]\n",
      "2024-12-03 09:25:36,241 - decision_learning.modeling.train - INFO - epoch: 13, train_loss: 0.22286875545978546, val_metric: 0.5583282056183747, test_regret: 0.43053228565909857\n",
      "Training Loader: Epoch 14/100: 100%|██████████| 2/2 [00:00<00:00, 281.26it/s]\n",
      "Validation Loader: Epoch 14/100: 100%|██████████| 1/1 [00:00<00:00, 321.13it/s]\n",
      "2024-12-03 09:25:36,277 - decision_learning.modeling.train - INFO - epoch: 14, train_loss: 0.1963414028286934, val_metric: 0.549526357378968, test_regret: 0.4195957100719497\n",
      "Training Loader: Epoch 15/100: 100%|██████████| 2/2 [00:00<00:00, 277.54it/s]\n",
      "Validation Loader: Epoch 15/100: 100%|██████████| 1/1 [00:00<00:00, 344.98it/s]\n",
      "2024-12-03 09:25:36,313 - decision_learning.modeling.train - INFO - epoch: 15, train_loss: 0.17342796176671982, val_metric: 0.5259401448628115, test_regret: 0.40789644424562\n",
      "Training Loader: Epoch 16/100: 100%|██████████| 2/2 [00:00<00:00, 278.90it/s]\n",
      "Validation Loader: Epoch 16/100: 100%|██████████| 1/1 [00:00<00:00, 324.03it/s]\n",
      "2024-12-03 09:25:36,348 - decision_learning.modeling.train - INFO - epoch: 16, train_loss: 0.15347671508789062, val_metric: 0.5222628116630903, test_regret: 0.39295818352719347\n",
      "Training Loader: Epoch 17/100: 100%|██████████| 2/2 [00:00<00:00, 274.60it/s]\n",
      "Validation Loader: Epoch 17/100: 100%|██████████| 1/1 [00:00<00:00, 329.53it/s]\n",
      "2024-12-03 09:25:36,384 - decision_learning.modeling.train - INFO - epoch: 17, train_loss: 0.13635633140802383, val_metric: 0.5075023104664053, test_regret: 0.376262580375399\n",
      "Training Loader: Epoch 18/100: 100%|██████████| 2/2 [00:00<00:00, 279.60it/s]\n",
      "Validation Loader: Epoch 18/100: 100%|██████████| 1/1 [00:00<00:00, 327.88it/s]\n",
      "2024-12-03 09:25:36,419 - decision_learning.modeling.train - INFO - epoch: 18, train_loss: 0.12156262621283531, val_metric: 0.5196053576649767, test_regret: 0.3487252989639573\n",
      "Training Loader: Epoch 19/100: 100%|██████████| 2/2 [00:00<00:00, 279.22it/s]\n",
      "Validation Loader: Epoch 19/100: 100%|██████████| 1/1 [00:00<00:00, 325.59it/s]\n",
      "2024-12-03 09:25:36,455 - decision_learning.modeling.train - INFO - epoch: 19, train_loss: 0.10917041450738907, val_metric: 0.4939435223504195, test_regret: 0.3187998099487136\n",
      "Training Loader: Epoch 20/100: 100%|██████████| 2/2 [00:00<00:00, 278.34it/s]\n",
      "Validation Loader: Epoch 20/100: 100%|██████████| 1/1 [00:00<00:00, 321.70it/s]\n",
      "2024-12-03 09:25:36,490 - decision_learning.modeling.train - INFO - epoch: 20, train_loss: 0.09882546216249466, val_metric: 0.44141339393462814, test_regret: 0.29083346998899345\n",
      "Training Loader: Epoch 21/100: 100%|██████████| 2/2 [00:00<00:00, 265.42it/s]\n",
      "Validation Loader: Epoch 21/100: 100%|██████████| 1/1 [00:00<00:00, 346.81it/s]\n",
      "2024-12-03 09:25:36,525 - decision_learning.modeling.train - INFO - epoch: 21, train_loss: 0.08974417671561241, val_metric: 0.3889127338642971, test_regret: 0.26624218622911255\n",
      "Training Loader: Epoch 22/100: 100%|██████████| 2/2 [00:00<00:00, 287.16it/s]\n",
      "Validation Loader: Epoch 22/100: 100%|██████████| 1/1 [00:00<00:00, 347.30it/s]\n",
      "2024-12-03 09:25:36,559 - decision_learning.modeling.train - INFO - epoch: 22, train_loss: 0.08236010372638702, val_metric: 0.3761623366584658, test_regret: 0.2458761606875044\n",
      "Training Loader: Epoch 23/100: 100%|██████████| 2/2 [00:00<00:00, 285.83it/s]\n",
      "Validation Loader: Epoch 23/100: 100%|██████████| 1/1 [00:00<00:00, 351.93it/s]\n",
      "2024-12-03 09:25:36,593 - decision_learning.modeling.train - INFO - epoch: 23, train_loss: 0.07605084404349327, val_metric: 0.36351783477771826, test_regret: 0.22710488403781207\n",
      "Training Loader: Epoch 24/100: 100%|██████████| 2/2 [00:00<00:00, 286.34it/s]\n",
      "Validation Loader: Epoch 24/100: 100%|██████████| 1/1 [00:00<00:00, 348.28it/s]\n",
      "2024-12-03 09:25:36,628 - decision_learning.modeling.train - INFO - epoch: 24, train_loss: 0.07067165151238441, val_metric: 0.3217355352863986, test_regret: 0.20949659434239795\n",
      "Training Loader: Epoch 25/100: 100%|██████████| 2/2 [00:00<00:00, 281.17it/s]\n",
      "Validation Loader: Epoch 25/100: 100%|██████████| 1/1 [00:00<00:00, 345.78it/s]\n",
      "2024-12-03 09:25:36,668 - decision_learning.modeling.train - INFO - epoch: 25, train_loss: 0.06612608581781387, val_metric: 0.3045029259834497, test_regret: 0.1940276053397969\n",
      "Training Loader: Epoch 26/100: 100%|██████████| 2/2 [00:00<00:00, 209.34it/s]\n",
      "Validation Loader: Epoch 26/100: 100%|██████████| 1/1 [00:00<00:00, 322.29it/s]\n",
      "2024-12-03 09:25:36,706 - decision_learning.modeling.train - INFO - epoch: 26, train_loss: 0.06222320720553398, val_metric: 0.2706244926085065, test_regret: 0.18169863094775893\n",
      "Training Loader: Epoch 27/100: 100%|██████████| 2/2 [00:00<00:00, 280.82it/s]\n",
      "Validation Loader: Epoch 27/100: 100%|██████████| 1/1 [00:00<00:00, 347.07it/s]\n",
      "2024-12-03 09:25:36,741 - decision_learning.modeling.train - INFO - epoch: 27, train_loss: 0.05888638086616993, val_metric: 0.28351674256805737, test_regret: 0.1688483761993065\n",
      "Training Loader: Epoch 28/100: 100%|██████████| 2/2 [00:00<00:00, 287.05it/s]\n",
      "Validation Loader: Epoch 28/100: 100%|██████████| 1/1 [00:00<00:00, 345.01it/s]\n",
      "2024-12-03 09:25:36,776 - decision_learning.modeling.train - INFO - epoch: 28, train_loss: 0.05600985512137413, val_metric: 0.25038243724730425, test_regret: 0.15682552581469245\n",
      "Training Loader: Epoch 29/100: 100%|██████████| 2/2 [00:00<00:00, 283.65it/s]\n",
      "Validation Loader: Epoch 29/100: 100%|██████████| 1/1 [00:00<00:00, 348.08it/s]\n",
      "2024-12-03 09:25:36,810 - decision_learning.modeling.train - INFO - epoch: 29, train_loss: 0.05346906930208206, val_metric: 0.22034854144107563, test_regret: 0.1474573759107001\n",
      "Training Loader: Epoch 30/100: 100%|██████████| 2/2 [00:00<00:00, 285.76it/s]\n",
      "Validation Loader: Epoch 30/100: 100%|██████████| 1/1 [00:00<00:00, 350.66it/s]\n",
      "2024-12-03 09:25:36,845 - decision_learning.modeling.train - INFO - epoch: 30, train_loss: 0.051154669374227524, val_metric: 0.23585956674167569, test_regret: 0.13921304168527276\n",
      "Training Loader: Epoch 31/100: 100%|██████████| 2/2 [00:00<00:00, 288.76it/s]\n",
      "Validation Loader: Epoch 31/100: 100%|██████████| 1/1 [00:00<00:00, 348.02it/s]\n",
      "2024-12-03 09:25:36,879 - decision_learning.modeling.train - INFO - epoch: 31, train_loss: 0.049151744693517685, val_metric: 0.20091317912955542, test_regret: 0.1322670450370346\n",
      "Training Loader: Epoch 32/100: 100%|██████████| 2/2 [00:00<00:00, 281.82it/s]\n",
      "Validation Loader: Epoch 32/100: 100%|██████████| 1/1 [00:00<00:00, 350.46it/s]\n",
      "2024-12-03 09:25:36,913 - decision_learning.modeling.train - INFO - epoch: 32, train_loss: 0.0473259799182415, val_metric: 0.19416749806820405, test_regret: 0.12622191805521762\n",
      "Training Loader: Epoch 33/100: 100%|██████████| 2/2 [00:00<00:00, 279.86it/s]\n",
      "Validation Loader: Epoch 33/100: 100%|██████████| 1/1 [00:00<00:00, 346.49it/s]\n",
      "2024-12-03 09:25:36,947 - decision_learning.modeling.train - INFO - epoch: 33, train_loss: 0.045656414702534676, val_metric: 0.1804743463912676, test_regret: 0.11965622140181766\n",
      "Training Loader: Epoch 34/100: 100%|██████████| 2/2 [00:00<00:00, 279.54it/s]\n",
      "Validation Loader: Epoch 34/100: 100%|██████████| 1/1 [00:00<00:00, 346.29it/s]\n",
      "2024-12-03 09:25:36,982 - decision_learning.modeling.train - INFO - epoch: 34, train_loss: 0.044137293472886086, val_metric: 0.18597333269858088, test_regret: 0.11449054599649967\n",
      "Training Loader: Epoch 35/100: 100%|██████████| 2/2 [00:00<00:00, 278.14it/s]\n",
      "Validation Loader: Epoch 35/100: 100%|██████████| 1/1 [00:00<00:00, 339.32it/s]\n",
      "2024-12-03 09:25:37,016 - decision_learning.modeling.train - INFO - epoch: 35, train_loss: 0.042747607454657555, val_metric: 0.1852493580156153, test_regret: 0.11101258681355672\n",
      "Training Loader: Epoch 36/100: 100%|██████████| 2/2 [00:00<00:00, 279.79it/s]\n",
      "Validation Loader: Epoch 36/100: 100%|██████████| 1/1 [00:00<00:00, 347.90it/s]\n",
      "2024-12-03 09:25:37,051 - decision_learning.modeling.train - INFO - epoch: 36, train_loss: 0.04150061681866646, val_metric: 0.19030708666699014, test_regret: 0.106540163558882\n",
      "Training Loader: Epoch 37/100: 100%|██████████| 2/2 [00:00<00:00, 277.54it/s]\n",
      "Validation Loader: Epoch 37/100: 100%|██████████| 1/1 [00:00<00:00, 345.49it/s]\n",
      "2024-12-03 09:25:37,086 - decision_learning.modeling.train - INFO - epoch: 37, train_loss: 0.0403840783983469, val_metric: 0.16105464858698304, test_regret: 0.1019722019668576\n",
      "Training Loader: Epoch 38/100: 100%|██████████| 2/2 [00:00<00:00, 281.69it/s]\n",
      "Validation Loader: Epoch 38/100: 100%|██████████| 1/1 [00:00<00:00, 348.91it/s]\n",
      "2024-12-03 09:25:37,119 - decision_learning.modeling.train - INFO - epoch: 38, train_loss: 0.03935914486646652, val_metric: 0.15839719458886942, test_regret: 0.09783617649992947\n",
      "Training Loader: Epoch 39/100: 100%|██████████| 2/2 [00:00<00:00, 279.81it/s]\n",
      "Validation Loader: Epoch 39/100: 100%|██████████| 1/1 [00:00<00:00, 344.73it/s]\n",
      "2024-12-03 09:25:37,154 - decision_learning.modeling.train - INFO - epoch: 39, train_loss: 0.038396263495087624, val_metric: 0.16082307669939339, test_regret: 0.09371655252411286\n",
      "Training Loader: Epoch 40/100: 100%|██████████| 2/2 [00:00<00:00, 281.77it/s]\n",
      "Validation Loader: Epoch 40/100: 100%|██████████| 1/1 [00:00<00:00, 346.98it/s]\n",
      "2024-12-03 09:25:37,190 - decision_learning.modeling.train - INFO - epoch: 40, train_loss: 0.03754737414419651, val_metric: 0.1663800693631219, test_regret: 0.08944711339881453\n",
      "Training Loader: Epoch 41/100: 100%|██████████| 2/2 [00:00<00:00, 280.11it/s]\n",
      "Validation Loader: Epoch 41/100: 100%|██████████| 1/1 [00:00<00:00, 346.67it/s]\n",
      "2024-12-03 09:25:37,224 - decision_learning.modeling.train - INFO - epoch: 41, train_loss: 0.036743342876434326, val_metric: 0.1672909133606899, test_regret: 0.08535134613734197\n",
      "Training Loader: Epoch 42/100: 100%|██████████| 2/2 [00:00<00:00, 286.48it/s]\n",
      "Validation Loader: Epoch 42/100: 100%|██████████| 1/1 [00:00<00:00, 346.01it/s]\n",
      "2024-12-03 09:25:37,259 - decision_learning.modeling.train - INFO - epoch: 42, train_loss: 0.03600270114839077, val_metric: 0.17628477101284157, test_regret: 0.08143016348197875\n",
      "Training Loader: Epoch 43/100: 100%|██████████| 2/2 [00:00<00:00, 281.55it/s]\n",
      "Validation Loader: Epoch 43/100: 100%|██████████| 1/1 [00:00<00:00, 346.75it/s]\n",
      "2024-12-03 09:25:37,293 - decision_learning.modeling.train - INFO - epoch: 43, train_loss: 0.03532657027244568, val_metric: 0.16850468590241616, test_regret: 0.07790186412347047\n",
      "Training Loader: Epoch 44/100: 100%|██████████| 2/2 [00:00<00:00, 283.44it/s]\n",
      "Validation Loader: Epoch 44/100: 100%|██████████| 1/1 [00:00<00:00, 350.08it/s]\n",
      "2024-12-03 09:25:37,328 - decision_learning.modeling.train - INFO - epoch: 44, train_loss: 0.03470464609563351, val_metric: 0.1538884330708023, test_regret: 0.07515012032585586\n",
      "Training Loader: Epoch 45/100: 100%|██████████| 2/2 [00:00<00:00, 282.39it/s]\n",
      "Validation Loader: Epoch 45/100: 100%|██████████| 1/1 [00:00<00:00, 349.15it/s]\n",
      "2024-12-03 09:25:37,363 - decision_learning.modeling.train - INFO - epoch: 45, train_loss: 0.034112248569726944, val_metric: 0.17215399277516047, test_regret: 0.0704657875764208\n",
      "Training Loader: Epoch 46/100: 100%|██████████| 2/2 [00:00<00:00, 282.54it/s]\n",
      "Validation Loader: Epoch 46/100: 100%|██████████| 1/1 [00:00<00:00, 345.35it/s]\n",
      "2024-12-03 09:25:37,397 - decision_learning.modeling.train - INFO - epoch: 46, train_loss: 0.03355669975280762, val_metric: 0.16783915948475592, test_regret: 0.06803975342408672\n",
      "Training Loader: Epoch 47/100: 100%|██████████| 2/2 [00:00<00:00, 282.65it/s]\n",
      "Validation Loader: Epoch 47/100: 100%|██████████| 1/1 [00:00<00:00, 348.19it/s]\n",
      "2024-12-03 09:25:37,432 - decision_learning.modeling.train - INFO - epoch: 47, train_loss: 0.0330493189394474, val_metric: 0.17105141846519903, test_regret: 0.0644149542134567\n",
      "Training Loader: Epoch 48/100: 100%|██████████| 2/2 [00:00<00:00, 283.67it/s]\n",
      "Validation Loader: Epoch 48/100: 100%|██████████| 1/1 [00:00<00:00, 349.29it/s]\n",
      "2024-12-03 09:25:37,466 - decision_learning.modeling.train - INFO - epoch: 48, train_loss: 0.032561758533120155, val_metric: 0.15007151480771855, test_regret: 0.06154286081687465\n",
      "Training Loader: Epoch 49/100: 100%|██████████| 2/2 [00:00<00:00, 280.89it/s]\n",
      "Validation Loader: Epoch 49/100: 100%|██████████| 1/1 [00:00<00:00, 346.46it/s]\n",
      "2024-12-03 09:25:37,501 - decision_learning.modeling.train - INFO - epoch: 49, train_loss: 0.032106708735227585, val_metric: 0.16600007423436786, test_regret: 0.0591190911015037\n",
      "Training Loader: Epoch 50/100: 100%|██████████| 2/2 [00:00<00:00, 284.22it/s]\n",
      "Validation Loader: Epoch 50/100: 100%|██████████| 1/1 [00:00<00:00, 347.96it/s]\n",
      "2024-12-03 09:25:37,536 - decision_learning.modeling.train - INFO - epoch: 50, train_loss: 0.03167034778743982, val_metric: 0.1737997545153145, test_regret: 0.05651854201804373\n",
      "Training Loader: Epoch 51/100: 100%|██████████| 2/2 [00:00<00:00, 282.75it/s]\n",
      "Validation Loader: Epoch 51/100: 100%|██████████| 1/1 [00:00<00:00, 347.76it/s]\n",
      "2024-12-03 09:25:37,570 - decision_learning.modeling.train - INFO - epoch: 51, train_loss: 0.03127727657556534, val_metric: 0.15837966941262288, test_regret: 0.05370982498643637\n",
      "Training Loader: Epoch 52/100: 100%|██████████| 2/2 [00:00<00:00, 283.66it/s]\n",
      "Validation Loader: Epoch 52/100: 100%|██████████| 1/1 [00:00<00:00, 348.89it/s]\n",
      "2024-12-03 09:25:37,605 - decision_learning.modeling.train - INFO - epoch: 52, train_loss: 0.030883310362696648, val_metric: 0.17003962154460758, test_regret: 0.05063205452236639\n",
      "Training Loader: Epoch 53/100: 100%|██████████| 2/2 [00:00<00:00, 281.74it/s]\n",
      "Validation Loader: Epoch 53/100: 100%|██████████| 1/1 [00:00<00:00, 348.16it/s]\n",
      "2024-12-03 09:25:37,639 - decision_learning.modeling.train - INFO - epoch: 53, train_loss: 0.030521411448717117, val_metric: 0.1771702750243172, test_regret: 0.047557750388724604\n",
      "Training Loader: Epoch 54/100: 100%|██████████| 2/2 [00:00<00:00, 282.86it/s]\n",
      "Validation Loader: Epoch 54/100: 100%|██████████| 1/1 [00:00<00:00, 348.45it/s]\n",
      "2024-12-03 09:25:37,674 - decision_learning.modeling.train - INFO - epoch: 54, train_loss: 0.03017109353095293, val_metric: 0.16123499393086593, test_regret: 0.04534691090662201\n",
      "Training Loader: Epoch 55/100: 100%|██████████| 2/2 [00:00<00:00, 283.47it/s]\n",
      "Validation Loader: Epoch 55/100: 100%|██████████| 1/1 [00:00<00:00, 347.10it/s]\n",
      "2024-12-03 09:25:37,708 - decision_learning.modeling.train - INFO - epoch: 55, train_loss: 0.029829762876033783, val_metric: 0.17510973527408633, test_regret: 0.0435118320941838\n",
      "Training Loader: Epoch 56/100: 100%|██████████| 2/2 [00:00<00:00, 283.79it/s]\n",
      "Validation Loader: Epoch 56/100: 100%|██████████| 1/1 [00:00<00:00, 348.02it/s]\n",
      "2024-12-03 09:25:37,743 - decision_learning.modeling.train - INFO - epoch: 56, train_loss: 0.02952789980918169, val_metric: 0.18234834244397294, test_regret: 0.04122703958461404\n",
      "Training Loader: Epoch 57/100: 100%|██████████| 2/2 [00:00<00:00, 281.10it/s]\n",
      "Validation Loader: Epoch 57/100: 100%|██████████| 1/1 [00:00<00:00, 350.17it/s]\n",
      "2024-12-03 09:25:37,777 - decision_learning.modeling.train - INFO - epoch: 57, train_loss: 0.02921933401376009, val_metric: 0.19265478579151174, test_regret: 0.04107981286076866\n",
      "Training Loader: Epoch 58/100: 100%|██████████| 2/2 [00:00<00:00, 282.62it/s]\n",
      "Validation Loader: Epoch 58/100: 100%|██████████| 1/1 [00:00<00:00, 349.53it/s]\n",
      "2024-12-03 09:25:37,812 - decision_learning.modeling.train - INFO - epoch: 58, train_loss: 0.028932591900229454, val_metric: 0.15942051602814722, test_regret: 0.0391059937374731\n",
      "Training Loader: Epoch 59/100: 100%|██████████| 2/2 [00:00<00:00, 282.10it/s]\n",
      "Validation Loader: Epoch 59/100: 100%|██████████| 1/1 [00:00<00:00, 348.97it/s]\n",
      "2024-12-03 09:25:37,846 - decision_learning.modeling.train - INFO - epoch: 59, train_loss: 0.02863838244229555, val_metric: 0.16290770048413428, test_regret: 0.03853555419646332\n",
      "Training Loader: Epoch 60/100: 100%|██████████| 2/2 [00:00<00:00, 282.48it/s]\n",
      "Validation Loader: Epoch 60/100: 100%|██████████| 1/1 [00:00<00:00, 348.19it/s]\n",
      "2024-12-03 09:25:37,881 - decision_learning.modeling.train - INFO - epoch: 60, train_loss: 0.028362886048853397, val_metric: 0.1794819702034776, test_regret: 0.0373645859728273\n",
      "Training Loader: Epoch 61/100: 100%|██████████| 2/2 [00:00<00:00, 282.13it/s]\n",
      "Validation Loader: Epoch 61/100: 100%|██████████| 1/1 [00:00<00:00, 346.84it/s]\n",
      "2024-12-03 09:25:37,916 - decision_learning.modeling.train - INFO - epoch: 61, train_loss: 0.02812135685235262, val_metric: 0.18326244261231006, test_regret: 0.03675017106599322\n",
      "Training Loader: Epoch 62/100: 100%|██████████| 2/2 [00:00<00:00, 283.88it/s]\n",
      "Validation Loader: Epoch 62/100: 100%|██████████| 1/1 [00:00<00:00, 350.72it/s]\n",
      "2024-12-03 09:25:37,950 - decision_learning.modeling.train - INFO - epoch: 62, train_loss: 0.027860299684107304, val_metric: 0.1827040209545906, test_regret: 0.03751458227726607\n",
      "Training Loader: Epoch 63/100: 100%|██████████| 2/2 [00:00<00:00, 284.64it/s]\n",
      "Validation Loader: Epoch 63/100: 100%|██████████| 1/1 [00:00<00:00, 350.20it/s]\n",
      "2024-12-03 09:25:37,985 - decision_learning.modeling.train - INFO - epoch: 63, train_loss: 0.02763315849006176, val_metric: 0.1755170008330316, test_regret: 0.03577051989574962\n",
      "Training Loader: Epoch 64/100: 100%|██████████| 2/2 [00:00<00:00, 284.56it/s]\n",
      "Validation Loader: Epoch 64/100: 100%|██████████| 1/1 [00:00<00:00, 347.53it/s]\n",
      "2024-12-03 09:25:38,019 - decision_learning.modeling.train - INFO - epoch: 64, train_loss: 0.027399898506700993, val_metric: 0.18872022673021288, test_regret: 0.03571306938812003\n",
      "Training Loader: Epoch 65/100: 100%|██████████| 2/2 [00:00<00:00, 281.26it/s]\n",
      "Validation Loader: Epoch 65/100: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
      "2024-12-03 09:25:38,347 - decision_learning.modeling.train - INFO - epoch: 65, train_loss: 0.027171429246664047, val_metric: 0.17129407296258492, test_regret: 0.034838512479210604\n",
      "Training Loader: Epoch 66/100: 100%|██████████| 2/2 [00:00<00:00, 266.64it/s]\n",
      "Validation Loader: Epoch 66/100: 100%|██████████| 1/1 [00:00<00:00, 344.95it/s]\n",
      "2024-12-03 09:25:38,382 - decision_learning.modeling.train - INFO - epoch: 66, train_loss: 0.02696438878774643, val_metric: 0.1421710792998309, test_regret: 0.034378110639613005\n",
      "Training Loader: Epoch 67/100: 100%|██████████| 2/2 [00:00<00:00, 288.55it/s]\n",
      "Validation Loader: Epoch 67/100: 100%|██████████| 1/1 [00:00<00:00, 343.40it/s]\n",
      "2024-12-03 09:25:38,415 - decision_learning.modeling.train - INFO - epoch: 67, train_loss: 0.026760689914226532, val_metric: 0.17565621376259197, test_regret: 0.034564930172821695\n",
      "Training Loader: Epoch 68/100: 100%|██████████| 2/2 [00:00<00:00, 287.79it/s]\n",
      "Validation Loader: Epoch 68/100: 100%|██████████| 1/1 [00:00<00:00, 346.84it/s]\n",
      "2024-12-03 09:25:38,448 - decision_learning.modeling.train - INFO - epoch: 68, train_loss: 0.02656019851565361, val_metric: 0.1761555824375775, test_regret: 0.033816923936409546\n",
      "Training Loader: Epoch 69/100: 100%|██████████| 2/2 [00:00<00:00, 289.98it/s]\n",
      "Validation Loader: Epoch 69/100: 100%|██████████| 1/1 [00:00<00:00, 346.21it/s]\n",
      "2024-12-03 09:25:38,482 - decision_learning.modeling.train - INFO - epoch: 69, train_loss: 0.026349973864853382, val_metric: 0.18161043600178814, test_regret: 0.03353132966286844\n",
      "Training Loader: Epoch 70/100: 100%|██████████| 2/2 [00:00<00:00, 288.99it/s]\n",
      "Validation Loader: Epoch 70/100: 100%|██████████| 1/1 [00:00<00:00, 349.67it/s]\n",
      "2024-12-03 09:25:38,515 - decision_learning.modeling.train - INFO - epoch: 70, train_loss: 0.026173442602157593, val_metric: 0.19210002732590753, test_regret: 0.03294329370478043\n",
      "Training Loader: Epoch 71/100: 100%|██████████| 2/2 [00:00<00:00, 288.48it/s]\n",
      "Validation Loader: Epoch 71/100: 100%|██████████| 1/1 [00:00<00:00, 338.96it/s]\n",
      "2024-12-03 09:25:38,548 - decision_learning.modeling.train - INFO - epoch: 71, train_loss: 0.02599699515849352, val_metric: 0.17052201161486852, test_regret: 0.03272357712812099\n",
      "Training Loader: Epoch 72/100: 100%|██████████| 2/2 [00:00<00:00, 287.54it/s]\n",
      "Validation Loader: Epoch 72/100: 100%|██████████| 1/1 [00:00<00:00, 342.73it/s]\n",
      "2024-12-03 09:25:38,582 - decision_learning.modeling.train - INFO - epoch: 72, train_loss: 0.025821486487984657, val_metric: 0.1718281896313493, test_regret: 0.032732937962151716\n",
      "Training Loader: Epoch 73/100: 100%|██████████| 2/2 [00:00<00:00, 287.22it/s]\n",
      "Validation Loader: Epoch 73/100: 100%|██████████| 1/1 [00:00<00:00, 339.78it/s]\n",
      "2024-12-03 09:25:38,615 - decision_learning.modeling.train - INFO - epoch: 73, train_loss: 0.025660322979092598, val_metric: 0.16880956814903583, test_regret: 0.03218054772561278\n",
      "Training Loader: Epoch 74/100: 100%|██████████| 2/2 [00:00<00:00, 288.93it/s]\n",
      "Validation Loader: Epoch 74/100: 100%|██████████| 1/1 [00:00<00:00, 343.88it/s]\n",
      "2024-12-03 09:25:38,649 - decision_learning.modeling.train - INFO - epoch: 74, train_loss: 0.02549960371106863, val_metric: 0.17408451827825366, test_regret: 0.03203315726555622\n",
      "Training Loader: Epoch 75/100: 100%|██████████| 2/2 [00:00<00:00, 283.21it/s]\n",
      "Validation Loader: Epoch 75/100: 100%|██████████| 1/1 [00:00<00:00, 343.26it/s]\n",
      "2024-12-03 09:25:38,682 - decision_learning.modeling.train - INFO - epoch: 75, train_loss: 0.025345321744680405, val_metric: 0.16466100889311916, test_regret: 0.032617284457070926\n",
      "Training Loader: Epoch 76/100: 100%|██████████| 2/2 [00:00<00:00, 289.33it/s]\n",
      "Validation Loader: Epoch 76/100: 100%|██████████| 1/1 [00:00<00:00, 342.76it/s]\n",
      "2024-12-03 09:25:38,716 - decision_learning.modeling.train - INFO - epoch: 76, train_loss: 0.025187160819768906, val_metric: 0.18583888773336096, test_regret: 0.03222299024180048\n",
      "Training Loader: Epoch 77/100: 100%|██████████| 2/2 [00:00<00:00, 287.52it/s]\n",
      "Validation Loader: Epoch 77/100: 100%|██████████| 1/1 [00:00<00:00, 334.63it/s]\n",
      "2024-12-03 09:25:38,749 - decision_learning.modeling.train - INFO - epoch: 77, train_loss: 0.025034569203853607, val_metric: 0.16629650206616922, test_regret: 0.031888773281036877\n",
      "Training Loader: Epoch 78/100: 100%|██████████| 2/2 [00:00<00:00, 286.22it/s]\n",
      "Validation Loader: Epoch 78/100: 100%|██████████| 1/1 [00:00<00:00, 336.30it/s]\n",
      "2024-12-03 09:25:38,783 - decision_learning.modeling.train - INFO - epoch: 78, train_loss: 0.024902651086449623, val_metric: 0.18989876285254492, test_regret: 0.03154542541368804\n",
      "Training Loader: Epoch 79/100: 100%|██████████| 2/2 [00:00<00:00, 286.79it/s]\n",
      "Validation Loader: Epoch 79/100: 100%|██████████| 1/1 [00:00<00:00, 333.78it/s]\n",
      "2024-12-03 09:25:38,817 - decision_learning.modeling.train - INFO - epoch: 79, train_loss: 0.024761839769780636, val_metric: 0.14421201225035923, test_regret: 0.030924182358472784\n",
      "Training Loader: Epoch 80/100: 100%|██████████| 2/2 [00:00<00:00, 286.00it/s]\n",
      "Validation Loader: Epoch 80/100: 100%|██████████| 1/1 [00:00<00:00, 336.51it/s]\n",
      "2024-12-03 09:25:38,851 - decision_learning.modeling.train - INFO - epoch: 80, train_loss: 0.024636317044496536, val_metric: 0.1539185061336912, test_regret: 0.03133802389035894\n",
      "Training Loader: Epoch 81/100: 100%|██████████| 2/2 [00:00<00:00, 282.32it/s]\n",
      "Validation Loader: Epoch 81/100: 100%|██████████| 1/1 [00:00<00:00, 334.26it/s]\n",
      "2024-12-03 09:25:38,885 - decision_learning.modeling.train - INFO - epoch: 81, train_loss: 0.024507937021553516, val_metric: 0.1619301631317063, test_regret: 0.03055167551053697\n",
      "Training Loader: Epoch 82/100: 100%|██████████| 2/2 [00:00<00:00, 284.23it/s]\n",
      "Validation Loader: Epoch 82/100: 100%|██████████| 1/1 [00:00<00:00, 338.41it/s]\n",
      "2024-12-03 09:25:38,918 - decision_learning.modeling.train - INFO - epoch: 82, train_loss: 0.02438683807849884, val_metric: 0.17243156970404497, test_regret: 0.031127006235387153\n",
      "Training Loader: Epoch 83/100: 100%|██████████| 2/2 [00:00<00:00, 283.97it/s]\n",
      "Validation Loader: Epoch 83/100: 100%|██████████| 1/1 [00:00<00:00, 338.71it/s]\n",
      "2024-12-03 09:25:38,952 - decision_learning.modeling.train - INFO - epoch: 83, train_loss: 0.024268019944429398, val_metric: 0.20698310009368653, test_regret: 0.030890532825200438\n",
      "Training Loader: Epoch 84/100: 100%|██████████| 2/2 [00:00<00:00, 283.85it/s]\n",
      "Validation Loader: Epoch 84/100: 100%|██████████| 1/1 [00:00<00:00, 336.00it/s]\n",
      "2024-12-03 09:25:38,986 - decision_learning.modeling.train - INFO - epoch: 84, train_loss: 0.024159076623618603, val_metric: 0.16774327688478724, test_regret: 0.030181454002059353\n",
      "Training Loader: Epoch 85/100: 100%|██████████| 2/2 [00:00<00:00, 281.88it/s]\n",
      "Validation Loader: Epoch 85/100: 100%|██████████| 1/1 [00:00<00:00, 336.97it/s]\n",
      "2024-12-03 09:25:39,020 - decision_learning.modeling.train - INFO - epoch: 85, train_loss: 0.024046888574957848, val_metric: 0.17042373340354827, test_regret: 0.029736474720055358\n",
      "Training Loader: Epoch 86/100: 100%|██████████| 2/2 [00:00<00:00, 283.61it/s]\n",
      "Validation Loader: Epoch 86/100: 100%|██████████| 1/1 [00:00<00:00, 339.92it/s]\n",
      "2024-12-03 09:25:39,054 - decision_learning.modeling.train - INFO - epoch: 86, train_loss: 0.023940145038068295, val_metric: 0.18048844095902528, test_regret: 0.02974973386941187\n",
      "Training Loader: Epoch 87/100: 100%|██████████| 2/2 [00:00<00:00, 283.80it/s]\n",
      "Validation Loader: Epoch 87/100: 100%|██████████| 1/1 [00:00<00:00, 337.14it/s]\n",
      "2024-12-03 09:25:39,088 - decision_learning.modeling.train - INFO - epoch: 87, train_loss: 0.02383894193917513, val_metric: 0.1618984387250702, test_regret: 0.029913909032989103\n",
      "Training Loader: Epoch 88/100: 100%|██████████| 2/2 [00:00<00:00, 283.93it/s]\n",
      "Validation Loader: Epoch 88/100: 100%|██████████| 1/1 [00:00<00:00, 338.71it/s]\n",
      "2024-12-03 09:25:39,122 - decision_learning.modeling.train - INFO - epoch: 88, train_loss: 0.023738105781376362, val_metric: 0.18950795258516492, test_regret: 0.02958452751607911\n",
      "Training Loader: Epoch 89/100: 100%|██████████| 2/2 [00:00<00:00, 283.89it/s]\n",
      "Validation Loader: Epoch 89/100: 100%|██████████| 1/1 [00:00<00:00, 335.28it/s]\n",
      "2024-12-03 09:25:39,157 - decision_learning.modeling.train - INFO - epoch: 89, train_loss: 0.02364855818450451, val_metric: 0.16907469022472854, test_regret: 0.029895549674841752\n",
      "Training Loader: Epoch 90/100: 100%|██████████| 2/2 [00:00<00:00, 284.02it/s]\n",
      "Validation Loader: Epoch 90/100: 100%|██████████| 1/1 [00:00<00:00, 347.24it/s]\n",
      "2024-12-03 09:25:39,190 - decision_learning.modeling.train - INFO - epoch: 90, train_loss: 0.02355367597192526, val_metric: 0.18901541023961319, test_regret: 0.030106316499873005\n",
      "Training Loader: Epoch 91/100: 100%|██████████| 2/2 [00:00<00:00, 284.00it/s]\n",
      "Validation Loader: Epoch 91/100: 100%|██████████| 1/1 [00:00<00:00, 346.81it/s]\n",
      "2024-12-03 09:25:39,225 - decision_learning.modeling.train - INFO - epoch: 91, train_loss: 0.023461095057427883, val_metric: 0.18771675630344528, test_regret: 0.02932792848691379\n",
      "Training Loader: Epoch 92/100: 100%|██████████| 2/2 [00:00<00:00, 283.72it/s]\n",
      "Validation Loader: Epoch 92/100: 100%|██████████| 1/1 [00:00<00:00, 346.78it/s]\n",
      "2024-12-03 09:25:39,260 - decision_learning.modeling.train - INFO - epoch: 92, train_loss: 0.023376813158392906, val_metric: 0.15430911870498878, test_regret: 0.029737310819857133\n",
      "Training Loader: Epoch 93/100: 100%|██████████| 2/2 [00:00<00:00, 280.64it/s]\n",
      "Validation Loader: Epoch 93/100: 100%|██████████| 1/1 [00:00<00:00, 346.41it/s]\n",
      "2024-12-03 09:25:39,295 - decision_learning.modeling.train - INFO - epoch: 93, train_loss: 0.023290046490728855, val_metric: 0.179893480413604, test_regret: 0.029056986862398822\n",
      "Training Loader: Epoch 94/100: 100%|██████████| 2/2 [00:00<00:00, 283.39it/s]\n",
      "Validation Loader: Epoch 94/100: 100%|██████████| 1/1 [00:00<00:00, 347.41it/s]\n",
      "2024-12-03 09:25:39,329 - decision_learning.modeling.train - INFO - epoch: 94, train_loss: 0.023212806321680546, val_metric: 0.1796946795589691, test_regret: 0.029668788957352345\n",
      "Training Loader: Epoch 95/100: 100%|██████████| 2/2 [00:00<00:00, 282.58it/s]\n",
      "Validation Loader: Epoch 95/100: 100%|██████████| 1/1 [00:00<00:00, 345.75it/s]\n",
      "2024-12-03 09:25:39,364 - decision_learning.modeling.train - INFO - epoch: 95, train_loss: 0.02314171753823757, val_metric: 0.1646433557958781, test_regret: 0.029541207095099546\n",
      "Training Loader: Epoch 96/100: 100%|██████████| 2/2 [00:00<00:00, 280.49it/s]\n",
      "Validation Loader: Epoch 96/100: 100%|██████████| 1/1 [00:00<00:00, 344.33it/s]\n",
      "2024-12-03 09:25:39,399 - decision_learning.modeling.train - INFO - epoch: 96, train_loss: 0.02305955160409212, val_metric: 0.1785955242283152, test_regret: 0.029184974581430403\n",
      "Training Loader: Epoch 97/100: 100%|██████████| 2/2 [00:00<00:00, 283.08it/s]\n",
      "Validation Loader: Epoch 97/100: 100%|██████████| 1/1 [00:00<00:00, 342.98it/s]\n",
      "2024-12-03 09:25:39,434 - decision_learning.modeling.train - INFO - epoch: 97, train_loss: 0.02298961766064167, val_metric: 0.18804223383017954, test_regret: 0.02942480110019716\n",
      "Training Loader: Epoch 98/100: 100%|██████████| 2/2 [00:00<00:00, 283.83it/s]\n",
      "Validation Loader: Epoch 98/100: 100%|██████████| 1/1 [00:00<00:00, 345.92it/s]\n",
      "2024-12-03 09:25:39,656 - decision_learning.modeling.train - INFO - epoch: 98, train_loss: 0.022923624143004417, val_metric: 0.17973435832562684, test_regret: 0.029013600250184953\n",
      "Training Loader: Epoch 99/100: 100%|██████████| 2/2 [00:00<00:00, 212.52it/s]\n",
      "Validation Loader: Epoch 99/100: 100%|██████████| 1/1 [00:00<00:00, 308.34it/s]\n",
      "2024-12-03 09:25:39,693 - decision_learning.modeling.train - INFO - epoch: 99, train_loss: 0.022853774949908257, val_metric: 0.1770368534270533, test_regret: 0.029288468060019095\n",
      "Training Loader: Epoch 100/100: 100%|██████████| 2/2 [00:00<00:00, 254.30it/s]\n",
      "Validation Loader: Epoch 100/100: 100%|██████████| 1/1 [00:00<00:00, 308.00it/s]\n",
      "2024-12-03 09:25:39,728 - decision_learning.modeling.train - INFO - epoch: 100, train_loss: 0.022790664806962013, val_metric: 0.16953641523978702, test_regret: 0.029061733470648495\n"
     ]
    }
   ],
   "source": [
    "custom_results, custom_models = lossfn_experiment_pipeline(X_train=generated_data['feat'],\n",
    "                true_cost_train=generated_data['cost'],\n",
    "                X_test=generated_data_test['feat'],\n",
    "                true_cost_test=generated_data_test['cost_true'], \n",
    "                predmodel=pred_model,\n",
    "                optmodel=optmodel,\n",
    "                val_split_params={'test_size':200, 'random_state':42},                                \n",
    "                custom_loss_inputs=custom_loss_inputs,\n",
    "                training_configs={'num_epochs':100,\n",
    "                                 'dataloader_params': {'batch_size':200, 'shuffle':True}},\n",
    "                save_models=False,\n",
    "                training_loop_verbose=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52a19293-ea06-49a4-b59b-1c97ea577acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_regret</th>\n",
       "      <th>loss_name</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>0.022791</td>\n",
       "      <td>0.169536</td>\n",
       "      <td>0.029062</td>\n",
       "      <td>cosine</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  train_loss  val_metric  test_regret loss_name hyperparameters\n",
       "99     99    0.022791    0.169536     0.029062    cosine            None"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_results[custom_results.epoch == 99]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7abce12-eb9e-4570-9789-960e9348dc1c",
   "metadata": {},
   "source": [
    "# Combine all existing examples so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "522ca681-6abb-4b5a-aeb4-300e9fb3f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results = pd.concat([preimplement_loss_results, custom_results, PG_init_results], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7652951c-6bf6-4dd9-8ea1-3dd0eda04716",
   "metadata": {},
   "source": [
    "## Find test regret using validation regret for hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b6ee69c-2642-4dc3-8d8f-346fa02dd7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_regret</th>\n",
       "      <th>loss_name</th>\n",
       "      <th>hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>35</td>\n",
       "      <td>14.333408</td>\n",
       "      <td>0.151630</td>\n",
       "      <td>0.015145</td>\n",
       "      <td>PG</td>\n",
       "      <td>{'h': 0.26591479484724945, 'finite_diff_type':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>90</td>\n",
       "      <td>0.023462</td>\n",
       "      <td>0.131474</td>\n",
       "      <td>0.029915</td>\n",
       "      <td>cosine</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>4.306511</td>\n",
       "      <td>0.189203</td>\n",
       "      <td>0.044372</td>\n",
       "      <td>SPO+</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>99</td>\n",
       "      <td>2.063001</td>\n",
       "      <td>0.439980</td>\n",
       "      <td>0.236546</td>\n",
       "      <td>MSE</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  train_loss  val_metric  test_regret loss_name  \\\n",
       "635     35   14.333408    0.151630     0.015145        PG   \n",
       "290     90    0.023462    0.131474     0.029915    cosine   \n",
       "99      99    4.306511    0.189203     0.044372      SPO+   \n",
       "199     99    2.063001    0.439980     0.236546       MSE   \n",
       "\n",
       "                                       hyperparameters  \n",
       "635  {'h': 0.26591479484724945, 'finite_diff_type':...  \n",
       "290                                               None  \n",
       "99                                                  {}  \n",
       "199                                                 {}  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_results.loc[combined_results.groupby('loss_name')['val_metric'].idxmin()].sort_values(by='test_regret')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decision_learning",
   "language": "python",
   "name": "decision_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
